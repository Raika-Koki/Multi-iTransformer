ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-06 03:21:39,766][0m A new study created in memory with name: no-name-12b1cee1-4218-4b93-b4dc-5df4355b0534[0m
[32m[I 2025-02-06 03:22:38,542][0m Trial 0 finished with value: 0.2337629002025119 and parameters: {'observation_period_num': 139, 'train_rates': 0.7008466155533835, 'learning_rate': 0.000655584848054825, 'batch_size': 81, 'step_size': 2, 'gamma': 0.8140257365675154}. Best is trial 0 with value: 0.2337629002025119.[0m
[32m[I 2025-02-06 03:23:05,120][0m Trial 1 finished with value: 0.3674993469580033 and parameters: {'observation_period_num': 241, 'train_rates': 0.6846325421710757, 'learning_rate': 6.04696034602558e-05, 'batch_size': 173, 'step_size': 13, 'gamma': 0.8402337857539275}. Best is trial 0 with value: 0.2337629002025119.[0m
[32m[I 2025-02-06 03:25:24,480][0m Trial 2 finished with value: 0.1740528377501861 and parameters: {'observation_period_num': 176, 'train_rates': 0.9586330978416737, 'learning_rate': 1.8738845110249833e-05, 'batch_size': 40, 'step_size': 13, 'gamma': 0.9115762733517792}. Best is trial 2 with value: 0.1740528377501861.[0m
[32m[I 2025-02-06 03:26:30,441][0m Trial 3 finished with value: 0.17505804012561665 and parameters: {'observation_period_num': 28, 'train_rates': 0.8614815535958749, 'learning_rate': 5.7465254069507725e-06, 'batch_size': 84, 'step_size': 14, 'gamma': 0.8027449911237806}. Best is trial 2 with value: 0.1740528377501861.[0m
Early stopping at epoch 92
[32m[I 2025-02-06 03:28:32,743][0m Trial 4 finished with value: 0.556802174796356 and parameters: {'observation_period_num': 145, 'train_rates': 0.9420440953746964, 'learning_rate': 1.7184697348733236e-05, 'batch_size': 43, 'step_size': 2, 'gamma': 0.7856864543092996}. Best is trial 2 with value: 0.1740528377501861.[0m
[32m[I 2025-02-06 03:29:07,778][0m Trial 5 finished with value: 0.6952886581420898 and parameters: {'observation_period_num': 152, 'train_rates': 0.9663205908776631, 'learning_rate': 6.3583766007364585e-06, 'batch_size': 174, 'step_size': 13, 'gamma': 0.769055914890455}. Best is trial 2 with value: 0.1740528377501861.[0m
[32m[I 2025-02-06 03:29:40,163][0m Trial 6 finished with value: 0.18107534033474904 and parameters: {'observation_period_num': 194, 'train_rates': 0.9014331979724277, 'learning_rate': 0.0007408922014388152, 'batch_size': 184, 'step_size': 7, 'gamma': 0.8924285120352109}. Best is trial 2 with value: 0.1740528377501861.[0m
[32m[I 2025-02-06 03:30:14,618][0m Trial 7 finished with value: 1.1668336493628366 and parameters: {'observation_period_num': 247, 'train_rates': 0.6121467007385353, 'learning_rate': 1.1696156830402966e-06, 'batch_size': 134, 'step_size': 7, 'gamma': 0.8299664651793742}. Best is trial 2 with value: 0.1740528377501861.[0m
[32m[I 2025-02-06 03:30:40,830][0m Trial 8 finished with value: 0.7853183476994726 and parameters: {'observation_period_num': 191, 'train_rates': 0.8272110039427609, 'learning_rate': 2.235480800683091e-06, 'batch_size': 208, 'step_size': 2, 'gamma': 0.9736872460485133}. Best is trial 2 with value: 0.1740528377501861.[0m
[32m[I 2025-02-06 03:32:23,785][0m Trial 9 finished with value: 0.23375126622448117 and parameters: {'observation_period_num': 107, 'train_rates': 0.7060138475364911, 'learning_rate': 6.922519726785319e-05, 'batch_size': 46, 'step_size': 4, 'gamma': 0.8293464130939073}. Best is trial 2 with value: 0.1740528377501861.[0m
[32m[I 2025-02-06 03:32:47,358][0m Trial 10 finished with value: 0.21728435330288975 and parameters: {'observation_period_num': 65, 'train_rates': 0.7897880458125863, 'learning_rate': 0.00020507755533386983, 'batch_size': 252, 'step_size': 10, 'gamma': 0.9224653641481301}. Best is trial 2 with value: 0.1740528377501861.[0m
[32m[I 2025-02-06 03:33:49,690][0m Trial 11 finished with value: 0.07436162840507944 and parameters: {'observation_period_num': 6, 'train_rates': 0.8682535105148675, 'learning_rate': 9.710315102665618e-06, 'batch_size': 92, 'step_size': 14, 'gamma': 0.9274713105595528}. Best is trial 11 with value: 0.07436162840507944.[0m
[32m[I 2025-02-06 03:39:37,926][0m Trial 12 finished with value: 0.029371784428755442 and parameters: {'observation_period_num': 10, 'train_rates': 0.9747613348407098, 'learning_rate': 1.6984588565304734e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.9420757552336256}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 03:40:37,685][0m Trial 13 finished with value: 0.09784885495901108 and parameters: {'observation_period_num': 14, 'train_rates': 0.9899113379308526, 'learning_rate': 7.70767328426852e-06, 'batch_size': 104, 'step_size': 10, 'gamma': 0.9632767512363376}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 03:45:12,730][0m Trial 14 finished with value: 0.06214868190259868 and parameters: {'observation_period_num': 60, 'train_rates': 0.8995519095454438, 'learning_rate': 3.165085639991628e-05, 'batch_size': 20, 'step_size': 10, 'gamma': 0.9404025618303727}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 03:49:13,839][0m Trial 15 finished with value: 0.07896129142593693 and parameters: {'observation_period_num': 59, 'train_rates': 0.9106548865634924, 'learning_rate': 7.183480971635991e-05, 'batch_size': 23, 'step_size': 10, 'gamma': 0.9480823932655477}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 03:53:12,551][0m Trial 16 finished with value: 0.1290508271791996 and parameters: {'observation_period_num': 86, 'train_rates': 0.7961224767452241, 'learning_rate': 0.00023312062885063952, 'batch_size': 21, 'step_size': 9, 'gamma': 0.874004389817321}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 03:54:00,487][0m Trial 17 finished with value: 0.09714998073501674 and parameters: {'observation_period_num': 43, 'train_rates': 0.924832185280242, 'learning_rate': 2.400795694077571e-05, 'batch_size': 127, 'step_size': 6, 'gamma': 0.9815428611490193}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 03:55:30,647][0m Trial 18 finished with value: 0.07117437074581782 and parameters: {'observation_period_num': 97, 'train_rates': 0.8742009897571905, 'learning_rate': 3.691435941676361e-05, 'batch_size': 60, 'step_size': 12, 'gamma': 0.9462294057254713}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 03:56:48,291][0m Trial 19 finished with value: 0.18805901370008676 and parameters: {'observation_period_num': 44, 'train_rates': 0.7534088915041064, 'learning_rate': 0.0001481866747034936, 'batch_size': 66, 'step_size': 11, 'gamma': 0.8894150008820959}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:01:40,103][0m Trial 20 finished with value: 0.2799751447207892 and parameters: {'observation_period_num': 68, 'train_rates': 0.9770740440485357, 'learning_rate': 2.5889103268328853e-06, 'batch_size': 20, 'step_size': 8, 'gamma': 0.8509660957827867}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:03:08,138][0m Trial 21 finished with value: 0.08795181915370955 and parameters: {'observation_period_num': 108, 'train_rates': 0.8735255920933858, 'learning_rate': 3.0206261198100677e-05, 'batch_size': 61, 'step_size': 12, 'gamma': 0.94234736624388}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:04:44,038][0m Trial 22 finished with value: 0.07975814021550692 and parameters: {'observation_period_num': 87, 'train_rates': 0.8415383173764115, 'learning_rate': 3.787313653988218e-05, 'batch_size': 56, 'step_size': 11, 'gamma': 0.9523208938258548}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:10:29,499][0m Trial 23 finished with value: 0.04973010695699988 and parameters: {'observation_period_num': 33, 'train_rates': 0.9008821102991587, 'learning_rate': 1.2769835965301044e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9868299850775937}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:13:22,287][0m Trial 24 finished with value: 0.05002913695383579 and parameters: {'observation_period_num': 27, 'train_rates': 0.9039855216710352, 'learning_rate': 1.3512527386298578e-05, 'batch_size': 33, 'step_size': 15, 'gamma': 0.9863579772104462}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:14:16,280][0m Trial 25 finished with value: 0.09482261911034584 and parameters: {'observation_period_num': 28, 'train_rates': 0.9365513011797786, 'learning_rate': 1.2349895947394738e-05, 'batch_size': 111, 'step_size': 15, 'gamma': 0.9876389877012532}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:16:50,929][0m Trial 26 finished with value: 0.09175968077033758 and parameters: {'observation_period_num': 24, 'train_rates': 0.9483007291921761, 'learning_rate': 4.3364387453892006e-06, 'batch_size': 38, 'step_size': 15, 'gamma': 0.9883412472273349}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:22:20,104][0m Trial 27 finished with value: 0.0361365626208185 and parameters: {'observation_period_num': 5, 'train_rates': 0.822829914804974, 'learning_rate': 1.2708062649297675e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9663635890256888}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:23:32,560][0m Trial 28 finished with value: 0.15773890787914555 and parameters: {'observation_period_num': 6, 'train_rates': 0.8095257517867191, 'learning_rate': 3.908994897185449e-06, 'batch_size': 74, 'step_size': 5, 'gamma': 0.9662791657429031}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:24:29,128][0m Trial 29 finished with value: 0.2771796534386705 and parameters: {'observation_period_num': 44, 'train_rates': 0.7634313645159468, 'learning_rate': 9.58029582674795e-06, 'batch_size': 89, 'step_size': 14, 'gamma': 0.9068250304297638}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:29:19,961][0m Trial 30 finished with value: 0.11215827754645977 and parameters: {'observation_period_num': 47, 'train_rates': 0.8362553880676973, 'learning_rate': 2.404594773672494e-06, 'batch_size': 18, 'step_size': 12, 'gamma': 0.9604499194117317}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:31:36,877][0m Trial 31 finished with value: 0.054120553036530815 and parameters: {'observation_period_num': 21, 'train_rates': 0.890766817861099, 'learning_rate': 1.4347845466079286e-05, 'batch_size': 41, 'step_size': 15, 'gamma': 0.9706025705241432}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:34:33,039][0m Trial 32 finished with value: 0.045715832088260626 and parameters: {'observation_period_num': 31, 'train_rates': 0.9246118211665397, 'learning_rate': 1.8489924700214995e-05, 'batch_size': 33, 'step_size': 15, 'gamma': 0.9879266267171146}. Best is trial 12 with value: 0.029371784428755442.[0m
[32m[I 2025-02-06 04:37:39,586][0m Trial 33 finished with value: 0.028021975415781955 and parameters: {'observation_period_num': 6, 'train_rates': 0.9321561644365255, 'learning_rate': 4.668291486710994e-05, 'batch_size': 31, 'step_size': 14, 'gamma': 0.9299588678272361}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:39:47,370][0m Trial 34 finished with value: 0.028515183392005997 and parameters: {'observation_period_num': 6, 'train_rates': 0.9624684926247528, 'learning_rate': 0.00011130741958841882, 'batch_size': 47, 'step_size': 14, 'gamma': 0.9292659569459687}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:41:54,042][0m Trial 35 finished with value: 0.03139558226934501 and parameters: {'observation_period_num': 7, 'train_rates': 0.9644866043919539, 'learning_rate': 0.00010472693126696281, 'batch_size': 48, 'step_size': 13, 'gamma': 0.9274699031455976}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:43:12,200][0m Trial 36 finished with value: 0.03559131409193194 and parameters: {'observation_period_num': 17, 'train_rates': 0.9562597747563721, 'learning_rate': 0.0001060545051634934, 'batch_size': 77, 'step_size': 13, 'gamma': 0.9252806274253971}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:43:51,335][0m Trial 37 finished with value: 0.19463038444519043 and parameters: {'observation_period_num': 218, 'train_rates': 0.9848212882397764, 'learning_rate': 0.00028860430469782977, 'batch_size': 151, 'step_size': 13, 'gamma': 0.9032980408715687}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:45:49,322][0m Trial 38 finished with value: 0.03371637873351574 and parameters: {'observation_period_num': 5, 'train_rates': 0.9641291331218451, 'learning_rate': 0.0004139956105112541, 'batch_size': 51, 'step_size': 11, 'gamma': 0.8704874940410882}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:47:10,785][0m Trial 39 finished with value: 0.13648546644777218 and parameters: {'observation_period_num': 134, 'train_rates': 0.933105591175679, 'learning_rate': 5.185910824408995e-05, 'batch_size': 71, 'step_size': 12, 'gamma': 0.9326325407397175}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:48:11,244][0m Trial 40 finished with value: 0.11207100749015808 and parameters: {'observation_period_num': 73, 'train_rates': 0.9673952792154813, 'learning_rate': 0.00010234338588509861, 'batch_size': 103, 'step_size': 13, 'gamma': 0.895414883375889}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:50:12,817][0m Trial 41 finished with value: 0.03287753198518712 and parameters: {'observation_period_num': 13, 'train_rates': 0.9603311004158981, 'learning_rate': 0.000497053290790697, 'batch_size': 50, 'step_size': 11, 'gamma': 0.8748610891758912}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:53:08,771][0m Trial 42 finished with value: 0.05636837737830846 and parameters: {'observation_period_num': 37, 'train_rates': 0.9481077554219366, 'learning_rate': 0.0005215158851308937, 'batch_size': 33, 'step_size': 9, 'gamma': 0.9131835217450772}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:55:06,880][0m Trial 43 finished with value: 0.02847027258722337 and parameters: {'observation_period_num': 14, 'train_rates': 0.9691265915737062, 'learning_rate': 0.00012512564647112603, 'batch_size': 51, 'step_size': 11, 'gamma': 0.9155390488200665}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:57:13,661][0m Trial 44 finished with value: 0.05813700333237648 and parameters: {'observation_period_num': 53, 'train_rates': 0.9876807916015358, 'learning_rate': 0.00010869711474232094, 'batch_size': 48, 'step_size': 13, 'gamma': 0.9163205605600542}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:58:07,822][0m Trial 45 finished with value: 0.13468491658568382 and parameters: {'observation_period_num': 17, 'train_rates': 0.6105456286706074, 'learning_rate': 5.453780844256838e-05, 'batch_size': 84, 'step_size': 14, 'gamma': 0.9341943880956047}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 04:58:37,365][0m Trial 46 finished with value: 0.15290901064872742 and parameters: {'observation_period_num': 151, 'train_rates': 0.9174371349547752, 'learning_rate': 8.833455442401898e-05, 'batch_size': 198, 'step_size': 9, 'gamma': 0.8839914832510161}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 05:01:10,632][0m Trial 47 finished with value: 0.15270004567164597 and parameters: {'observation_period_num': 23, 'train_rates': 0.6555667408597772, 'learning_rate': 0.00018473223308863163, 'batch_size': 30, 'step_size': 12, 'gamma': 0.8546023442821576}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 05:01:38,120][0m Trial 48 finished with value: 0.16753003001213074 and parameters: {'observation_period_num': 172, 'train_rates': 0.9732763493426211, 'learning_rate': 0.00014823801062368612, 'batch_size': 230, 'step_size': 14, 'gamma': 0.7584954864074205}. Best is trial 33 with value: 0.028021975415781955.[0m
[32m[I 2025-02-06 05:03:44,766][0m Trial 49 finished with value: 0.0814368269991043 and parameters: {'observation_period_num': 38, 'train_rates': 0.9410627404445757, 'learning_rate': 0.0003155432995784822, 'batch_size': 46, 'step_size': 11, 'gamma': 0.9555286631570721}. Best is trial 33 with value: 0.028021975415781955.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-06 05:03:44,776][0m A new study created in memory with name: no-name-af15bb78-d9a8-4e9d-b88d-c0eca7e0dc4b[0m
[32m[I 2025-02-06 05:04:50,797][0m Trial 0 finished with value: 0.1714287682139176 and parameters: {'observation_period_num': 33, 'train_rates': 0.6782112540565512, 'learning_rate': 2.7889269149833944e-05, 'batch_size': 71, 'step_size': 6, 'gamma': 0.9814143880004086}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:05:19,280][0m Trial 1 finished with value: 0.6351259736972448 and parameters: {'observation_period_num': 55, 'train_rates': 0.8253728968636055, 'learning_rate': 1.139237767474884e-06, 'batch_size': 215, 'step_size': 12, 'gamma': 0.982752633427704}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:05:52,372][0m Trial 2 finished with value: 0.19845954905933058 and parameters: {'observation_period_num': 91, 'train_rates': 0.8016227623979256, 'learning_rate': 5.652935277630926e-05, 'batch_size': 164, 'step_size': 2, 'gamma': 0.906863523943533}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:11:35,702][0m Trial 3 finished with value: 0.23363435020049414 and parameters: {'observation_period_num': 53, 'train_rates': 0.9775910196931707, 'learning_rate': 2.463820330644286e-06, 'batch_size': 17, 'step_size': 8, 'gamma': 0.7897583976396342}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:14:31,241][0m Trial 4 finished with value: 0.2950333002422537 and parameters: {'observation_period_num': 233, 'train_rates': 0.7427579267504911, 'learning_rate': 4.0213214196443605e-05, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8727277191296743}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:15:18,963][0m Trial 5 finished with value: 0.22040639351551072 and parameters: {'observation_period_num': 162, 'train_rates': 0.6682397836775228, 'learning_rate': 0.0008817920066204496, 'batch_size': 98, 'step_size': 14, 'gamma': 0.8629839482219681}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:15:46,302][0m Trial 6 finished with value: 0.6618180208735995 and parameters: {'observation_period_num': 30, 'train_rates': 0.9201522145448751, 'learning_rate': 5.9922750234865805e-06, 'batch_size': 226, 'step_size': 7, 'gamma': 0.794168941675148}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:16:37,746][0m Trial 7 finished with value: 2.175823207505405 and parameters: {'observation_period_num': 205, 'train_rates': 0.8334072871602556, 'learning_rate': 1.270093784503805e-06, 'batch_size': 104, 'step_size': 1, 'gamma': 0.9073902225312273}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:19:53,135][0m Trial 8 finished with value: 0.2892248283753443 and parameters: {'observation_period_num': 229, 'train_rates': 0.6350499794420881, 'learning_rate': 0.0002040178260812929, 'batch_size': 21, 'step_size': 7, 'gamma': 0.9049665741795375}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:20:24,222][0m Trial 9 finished with value: 0.31432216939624963 and parameters: {'observation_period_num': 83, 'train_rates': 0.6060605800998784, 'learning_rate': 2.5783229932623214e-05, 'batch_size': 153, 'step_size': 9, 'gamma': 0.9578537301744778}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:21:32,079][0m Trial 10 finished with value: 0.4505441947148578 and parameters: {'observation_period_num': 146, 'train_rates': 0.7123241444656189, 'learning_rate': 8.831366996783575e-06, 'batch_size': 70, 'step_size': 3, 'gamma': 0.9513706383164338}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:22:05,030][0m Trial 11 finished with value: 0.2541189607052647 and parameters: {'observation_period_num': 99, 'train_rates': 0.7756517084614274, 'learning_rate': 7.218775272495597e-05, 'batch_size': 168, 'step_size': 4, 'gamma': 0.9251359205217258}. Best is trial 0 with value: 0.1714287682139176.[0m
[32m[I 2025-02-06 05:22:37,024][0m Trial 12 finished with value: 0.07130743653713903 and parameters: {'observation_period_num': 22, 'train_rates': 0.8655038848437647, 'learning_rate': 0.00019130713002903373, 'batch_size': 189, 'step_size': 4, 'gamma': 0.8306428316266837}. Best is trial 12 with value: 0.07130743653713903.[0m
[32m[I 2025-02-06 05:23:08,670][0m Trial 13 finished with value: 0.06775424188201867 and parameters: {'observation_period_num': 17, 'train_rates': 0.894871915551825, 'learning_rate': 0.00027247749464684315, 'batch_size': 201, 'step_size': 4, 'gamma': 0.8250604996211263}. Best is trial 13 with value: 0.06775424188201867.[0m
[32m[I 2025-02-06 05:23:35,657][0m Trial 14 finished with value: 0.0619990249595991 and parameters: {'observation_period_num': 5, 'train_rates': 0.8889314519845791, 'learning_rate': 0.00045003646202930263, 'batch_size': 256, 'step_size': 4, 'gamma': 0.8283072153231493}. Best is trial 14 with value: 0.0619990249595991.[0m
[32m[I 2025-02-06 05:24:03,392][0m Trial 15 finished with value: 0.06454101919725135 and parameters: {'observation_period_num': 22, 'train_rates': 0.8993134224484284, 'learning_rate': 0.0009650492109571128, 'batch_size': 252, 'step_size': 5, 'gamma': 0.7506923095431097}. Best is trial 14 with value: 0.0619990249595991.[0m
[32m[I 2025-02-06 05:24:31,959][0m Trial 16 finished with value: 0.04901083558797836 and parameters: {'observation_period_num': 8, 'train_rates': 0.9565108231012034, 'learning_rate': 0.0008078193906146401, 'batch_size': 251, 'step_size': 10, 'gamma': 0.7597226304164238}. Best is trial 16 with value: 0.04901083558797836.[0m
[32m[I 2025-02-06 05:24:59,573][0m Trial 17 finished with value: 0.08344510942697525 and parameters: {'observation_period_num': 60, 'train_rates': 0.9517562921783457, 'learning_rate': 0.0003954632873529944, 'batch_size': 249, 'step_size': 11, 'gamma': 0.7502341083640769}. Best is trial 16 with value: 0.04901083558797836.[0m
[32m[I 2025-02-06 05:25:27,462][0m Trial 18 finished with value: 0.15337611734867096 and parameters: {'observation_period_num': 125, 'train_rates': 0.9408389966065624, 'learning_rate': 0.0004665735874902455, 'batch_size': 231, 'step_size': 11, 'gamma': 0.7847953579769819}. Best is trial 16 with value: 0.04901083558797836.[0m
[32m[I 2025-02-06 05:26:02,835][0m Trial 19 finished with value: 0.051601532846689224 and parameters: {'observation_period_num': 8, 'train_rates': 0.9776121743096954, 'learning_rate': 0.0001250040082436804, 'batch_size': 188, 'step_size': 15, 'gamma': 0.8247096322947132}. Best is trial 16 with value: 0.04901083558797836.[0m
[32m[I 2025-02-06 05:26:35,942][0m Trial 20 finished with value: 0.1582804173231125 and parameters: {'observation_period_num': 111, 'train_rates': 0.9862721956822018, 'learning_rate': 0.00012170587268309387, 'batch_size': 193, 'step_size': 15, 'gamma': 0.8550174116289531}. Best is trial 16 with value: 0.04901083558797836.[0m
[32m[I 2025-02-06 05:27:01,260][0m Trial 21 finished with value: 0.06907729962367122 and parameters: {'observation_period_num': 6, 'train_rates': 0.8800850710320622, 'learning_rate': 0.00012159989623077579, 'batch_size': 237, 'step_size': 10, 'gamma': 0.8213019659312065}. Best is trial 16 with value: 0.04901083558797836.[0m
[32m[I 2025-02-06 05:27:32,172][0m Trial 22 finished with value: 0.04533158987760544 and parameters: {'observation_period_num': 6, 'train_rates': 0.9476896708071886, 'learning_rate': 0.00042307372454476684, 'batch_size': 213, 'step_size': 13, 'gamma': 0.808424397134813}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:28:03,882][0m Trial 23 finished with value: 0.07682906836271286 and parameters: {'observation_period_num': 71, 'train_rates': 0.9524976938578215, 'learning_rate': 0.0006238645562368877, 'batch_size': 209, 'step_size': 13, 'gamma': 0.7748309257081719}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:28:39,581][0m Trial 24 finished with value: 0.0779995545744896 and parameters: {'observation_period_num': 43, 'train_rates': 0.9792566605233809, 'learning_rate': 0.00011780304276083544, 'batch_size': 185, 'step_size': 15, 'gamma': 0.8071513871145349}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:29:26,375][0m Trial 25 finished with value: 0.05069957219642922 and parameters: {'observation_period_num': 42, 'train_rates': 0.929459600540532, 'learning_rate': 0.0002683873331950989, 'batch_size': 133, 'step_size': 13, 'gamma': 0.7660305289526985}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:30:12,614][0m Trial 26 finished with value: 0.054727086674351735 and parameters: {'observation_period_num': 43, 'train_rates': 0.9274317988395744, 'learning_rate': 0.00027841516886063794, 'batch_size': 129, 'step_size': 13, 'gamma': 0.7616978529982618}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:30:55,856][0m Trial 27 finished with value: 0.0647221222309999 and parameters: {'observation_period_num': 75, 'train_rates': 0.8522487119028712, 'learning_rate': 0.000611164606512605, 'batch_size': 136, 'step_size': 12, 'gamma': 0.770949037542592}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:31:47,403][0m Trial 28 finished with value: 0.19681533201631293 and parameters: {'observation_period_num': 38, 'train_rates': 0.9146439803030045, 'learning_rate': 1.2209007660728207e-05, 'batch_size': 117, 'step_size': 10, 'gamma': 0.8049122263419002}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:33:20,831][0m Trial 29 finished with value: 0.047534998854001365 and parameters: {'observation_period_num': 30, 'train_rates': 0.948944667095858, 'learning_rate': 0.0002981964928621614, 'batch_size': 64, 'step_size': 13, 'gamma': 0.770342937556093}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:34:46,251][0m Trial 30 finished with value: 0.14550194951395193 and parameters: {'observation_period_num': 183, 'train_rates': 0.8530995122227503, 'learning_rate': 2.3926645476889885e-05, 'batch_size': 61, 'step_size': 9, 'gamma': 0.8468782519968173}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:36:45,200][0m Trial 31 finished with value: 0.047901976668665594 and parameters: {'observation_period_num': 25, 'train_rates': 0.9520464864726621, 'learning_rate': 0.00030368524110371977, 'batch_size': 50, 'step_size': 13, 'gamma': 0.7726399493804316}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:38:51,589][0m Trial 32 finished with value: 0.049791653178356314 and parameters: {'observation_period_num': 28, 'train_rates': 0.9541111756981677, 'learning_rate': 0.0005858315888023295, 'batch_size': 47, 'step_size': 12, 'gamma': 0.7787320265689749}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:39:58,716][0m Trial 33 finished with value: 0.08933102171605742 and parameters: {'observation_period_num': 60, 'train_rates': 0.9510863516550286, 'learning_rate': 0.0003448149151696988, 'batch_size': 90, 'step_size': 14, 'gamma': 0.804189434657111}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:42:21,138][0m Trial 34 finished with value: 0.04673653095960617 and parameters: {'observation_period_num': 18, 'train_rates': 0.9888408597489429, 'learning_rate': 7.5439742735882e-05, 'batch_size': 43, 'step_size': 11, 'gamma': 0.7902411431421992}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:44:02,568][0m Trial 35 finished with value: 0.058012298759165165 and parameters: {'observation_period_num': 47, 'train_rates': 0.8055295390931093, 'learning_rate': 5.9855343023923565e-05, 'batch_size': 51, 'step_size': 14, 'gamma': 0.7964746662369517}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:46:57,544][0m Trial 36 finished with value: 0.0890305774176822 and parameters: {'observation_period_num': 64, 'train_rates': 0.9059963939675714, 'learning_rate': 7.689262225451592e-05, 'batch_size': 32, 'step_size': 11, 'gamma': 0.7819143592425162}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:48:12,446][0m Trial 37 finished with value: 0.0845931662999569 and parameters: {'observation_period_num': 29, 'train_rates': 0.9681638423585703, 'learning_rate': 3.971576092644418e-05, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8452427612820912}. Best is trial 22 with value: 0.04533158987760544.[0m
[32m[I 2025-02-06 05:50:22,224][0m Trial 38 finished with value: 0.04474984109401703 and parameters: {'observation_period_num': 53, 'train_rates': 0.9851264067806168, 'learning_rate': 0.00018701567512985115, 'batch_size': 47, 'step_size': 14, 'gamma': 0.8825802598972957}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 05:53:19,989][0m Trial 39 finished with value: 0.10900225887695948 and parameters: {'observation_period_num': 83, 'train_rates': 0.9845994823459256, 'learning_rate': 0.00019049430053185218, 'batch_size': 34, 'step_size': 13, 'gamma': 0.8795716819552338}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 05:54:41,497][0m Trial 40 finished with value: 0.06671500205993652 and parameters: {'observation_period_num': 53, 'train_rates': 0.9897029591850055, 'learning_rate': 0.00015755420689715521, 'batch_size': 74, 'step_size': 14, 'gamma': 0.8134343251902544}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 05:56:54,367][0m Trial 41 finished with value: 0.052587310843011166 and parameters: {'observation_period_num': 19, 'train_rates': 0.9362240602269016, 'learning_rate': 0.0002468030843392085, 'batch_size': 44, 'step_size': 13, 'gamma': 0.8889743761687511}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 05:58:36,315][0m Trial 42 finished with value: 0.05522558465600014 and parameters: {'observation_period_num': 33, 'train_rates': 0.9672501726181173, 'learning_rate': 8.559793174541001e-05, 'batch_size': 60, 'step_size': 14, 'gamma': 0.7915748031593792}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 06:02:55,914][0m Trial 43 finished with value: 0.05205234376763975 and parameters: {'observation_period_num': 22, 'train_rates': 0.9242845591413045, 'learning_rate': 0.000362964447285113, 'batch_size': 22, 'step_size': 12, 'gamma': 0.8874477360179767}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 06:03:42,464][0m Trial 44 finished with value: 0.18450753184868462 and parameters: {'observation_period_num': 35, 'train_rates': 0.7276086162680212, 'learning_rate': 4.2235719027860375e-05, 'batch_size': 108, 'step_size': 11, 'gamma': 0.8668075815520924}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 06:06:04,682][0m Trial 45 finished with value: 0.13290362765071212 and parameters: {'observation_period_num': 98, 'train_rates': 0.9676487758629981, 'learning_rate': 0.00016763108298188263, 'batch_size': 41, 'step_size': 15, 'gamma': 0.9137778726171233}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 06:07:31,891][0m Trial 46 finished with value: 0.16697629830597774 and parameters: {'observation_period_num': 14, 'train_rates': 0.7699917283773332, 'learning_rate': 8.772166876641525e-05, 'batch_size': 59, 'step_size': 14, 'gamma': 0.9899465648819761}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 06:08:38,713][0m Trial 47 finished with value: 0.0777804872886157 and parameters: {'observation_period_num': 47, 'train_rates': 0.917915422216339, 'learning_rate': 0.0005279071175354227, 'batch_size': 87, 'step_size': 12, 'gamma': 0.838845311813009}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 06:09:07,904][0m Trial 48 finished with value: 0.281983702985983 and parameters: {'observation_period_num': 251, 'train_rates': 0.688311493023135, 'learning_rate': 0.0003112512857582532, 'batch_size': 169, 'step_size': 9, 'gamma': 0.7950737576321716}. Best is trial 38 with value: 0.04474984109401703.[0m
[32m[I 2025-02-06 06:09:45,580][0m Trial 49 finished with value: 0.0944263003187476 and parameters: {'observation_period_num': 16, 'train_rates': 0.8689351377636108, 'learning_rate': 1.7579351717571868e-05, 'batch_size': 151, 'step_size': 13, 'gamma': 0.9447269312299951}. Best is trial 38 with value: 0.04474984109401703.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-06 06:09:45,590][0m A new study created in memory with name: no-name-aaaf7cea-c6dc-4c7d-bbd4-a6acfe56d6e0[0m
[32m[I 2025-02-06 06:10:18,307][0m Trial 0 finished with value: 0.20601591039077508 and parameters: {'observation_period_num': 25, 'train_rates': 0.6406027962668096, 'learning_rate': 5.897706882096967e-05, 'batch_size': 148, 'step_size': 7, 'gamma': 0.905259736994849}. Best is trial 0 with value: 0.20601591039077508.[0m
[32m[I 2025-02-06 06:10:40,121][0m Trial 1 finished with value: 0.1658822591284212 and parameters: {'observation_period_num': 25, 'train_rates': 0.7478781298307882, 'learning_rate': 0.0008097868507658561, 'batch_size': 247, 'step_size': 15, 'gamma': 0.8542117509243159}. Best is trial 1 with value: 0.1658822591284212.[0m
[32m[I 2025-02-06 06:11:11,770][0m Trial 2 finished with value: 0.18701047136488816 and parameters: {'observation_period_num': 36, 'train_rates': 0.7479045993832256, 'learning_rate': 0.0001596345247637057, 'batch_size': 180, 'step_size': 6, 'gamma': 0.7978759082724194}. Best is trial 1 with value: 0.1658822591284212.[0m
[32m[I 2025-02-06 06:11:52,196][0m Trial 3 finished with value: 0.6462482230447645 and parameters: {'observation_period_num': 225, 'train_rates': 0.8544528763945494, 'learning_rate': 3.3733765541925467e-06, 'batch_size': 134, 'step_size': 8, 'gamma': 0.7555232773458489}. Best is trial 1 with value: 0.1658822591284212.[0m
[32m[I 2025-02-06 06:14:13,543][0m Trial 4 finished with value: 0.668733422366128 and parameters: {'observation_period_num': 81, 'train_rates': 0.7916597806477679, 'learning_rate': 1.8661201290563413e-06, 'batch_size': 36, 'step_size': 5, 'gamma': 0.7896187752510087}. Best is trial 1 with value: 0.1658822591284212.[0m
Early stopping at epoch 61
[32m[I 2025-02-06 06:14:38,011][0m Trial 5 finished with value: 1.1968661087285961 and parameters: {'observation_period_num': 6, 'train_rates': 0.8655893436913451, 'learning_rate': 3.9082317604576495e-06, 'batch_size': 156, 'step_size': 1, 'gamma': 0.8425708890370119}. Best is trial 1 with value: 0.1658822591284212.[0m
[32m[I 2025-02-06 06:15:53,622][0m Trial 6 finished with value: 1.3964697126587633 and parameters: {'observation_period_num': 134, 'train_rates': 0.8950219554778578, 'learning_rate': 1.0925062739924716e-06, 'batch_size': 73, 'step_size': 3, 'gamma': 0.8584949443427389}. Best is trial 1 with value: 0.1658822591284212.[0m
[32m[I 2025-02-06 06:18:06,600][0m Trial 7 finished with value: 0.17919583540213735 and parameters: {'observation_period_num': 149, 'train_rates': 0.9797689383738777, 'learning_rate': 0.00024501092637133987, 'batch_size': 44, 'step_size': 11, 'gamma': 0.8483501208421654}. Best is trial 1 with value: 0.1658822591284212.[0m
[32m[I 2025-02-06 06:18:30,798][0m Trial 8 finished with value: 0.8086758162047702 and parameters: {'observation_period_num': 218, 'train_rates': 0.77272203927303, 'learning_rate': 3.655055998873679e-06, 'batch_size': 217, 'step_size': 12, 'gamma': 0.7804375109784021}. Best is trial 1 with value: 0.1658822591284212.[0m
[32m[I 2025-02-06 06:19:02,308][0m Trial 9 finished with value: 0.9512049658161981 and parameters: {'observation_period_num': 226, 'train_rates': 0.7248476984845672, 'learning_rate': 1.3872113635386158e-06, 'batch_size': 157, 'step_size': 13, 'gamma': 0.9590521764325449}. Best is trial 1 with value: 0.1658822591284212.[0m
[32m[I 2025-02-06 06:19:21,240][0m Trial 10 finished with value: 0.15233257872627137 and parameters: {'observation_period_num': 75, 'train_rates': 0.6082988904354828, 'learning_rate': 0.0009127792599426367, 'batch_size': 252, 'step_size': 15, 'gamma': 0.915288723291492}. Best is trial 10 with value: 0.15233257872627137.[0m
[32m[I 2025-02-06 06:19:40,555][0m Trial 11 finished with value: 0.17817583703496748 and parameters: {'observation_period_num': 76, 'train_rates': 0.6008897774576551, 'learning_rate': 0.000863349428060124, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9128875418867168}. Best is trial 10 with value: 0.15233257872627137.[0m
[32m[I 2025-02-06 06:20:00,330][0m Trial 12 finished with value: 0.18266508969289086 and parameters: {'observation_period_num': 75, 'train_rates': 0.6941386188778302, 'learning_rate': 0.0009018024630559674, 'batch_size': 250, 'step_size': 15, 'gamma': 0.9874987198855578}. Best is trial 10 with value: 0.15233257872627137.[0m
[32m[I 2025-02-06 06:20:25,269][0m Trial 13 finished with value: 0.15551419798160868 and parameters: {'observation_period_num': 48, 'train_rates': 0.689792827648224, 'learning_rate': 0.0002935379526239618, 'batch_size': 210, 'step_size': 11, 'gamma': 0.9047538732898359}. Best is trial 10 with value: 0.15233257872627137.[0m
[32m[I 2025-02-06 06:20:48,106][0m Trial 14 finished with value: 0.22428192761836302 and parameters: {'observation_period_num': 104, 'train_rates': 0.6572133049892905, 'learning_rate': 0.00027389744122750917, 'batch_size': 214, 'step_size': 10, 'gamma': 0.9180513948342311}. Best is trial 10 with value: 0.15233257872627137.[0m
[32m[I 2025-02-06 06:21:11,580][0m Trial 15 finished with value: 0.39178429146700644 and parameters: {'observation_period_num': 171, 'train_rates': 0.6159629152776411, 'learning_rate': 1.7126317982689897e-05, 'batch_size': 203, 'step_size': 10, 'gamma': 0.9431694493877564}. Best is trial 10 with value: 0.15233257872627137.[0m
[32m[I 2025-02-06 06:21:57,630][0m Trial 16 finished with value: 0.16503103915292558 and parameters: {'observation_period_num': 49, 'train_rates': 0.6894821713281052, 'learning_rate': 8.078954448835508e-05, 'batch_size': 109, 'step_size': 13, 'gamma': 0.8880397375205373}. Best is trial 10 with value: 0.15233257872627137.[0m
[32m[I 2025-02-06 06:22:22,528][0m Trial 17 finished with value: 0.3573995940465347 and parameters: {'observation_period_num': 100, 'train_rates': 0.665748910508503, 'learning_rate': 1.3268918680184924e-05, 'batch_size': 194, 'step_size': 13, 'gamma': 0.945921521086732}. Best is trial 10 with value: 0.15233257872627137.[0m
[32m[I 2025-02-06 06:22:44,763][0m Trial 18 finished with value: 0.16190207988537592 and parameters: {'observation_period_num': 58, 'train_rates': 0.6997014149193772, 'learning_rate': 0.00042914159274132896, 'batch_size': 227, 'step_size': 9, 'gamma': 0.8263586677336291}. Best is trial 10 with value: 0.15233257872627137.[0m
[32m[I 2025-02-06 06:23:27,054][0m Trial 19 finished with value: 0.3339997547708692 and parameters: {'observation_period_num': 174, 'train_rates': 0.6432770372132639, 'learning_rate': 0.00014031362560824033, 'batch_size': 109, 'step_size': 12, 'gamma': 0.8886731113409202}. Best is trial 10 with value: 0.15233257872627137.[0m
[32m[I 2025-02-06 06:23:56,666][0m Trial 20 finished with value: 0.13261030140248212 and parameters: {'observation_period_num': 115, 'train_rates': 0.8181761742675158, 'learning_rate': 0.00044998969011550254, 'batch_size': 179, 'step_size': 14, 'gamma': 0.985679478074651}. Best is trial 20 with value: 0.13261030140248212.[0m
[32m[I 2025-02-06 06:24:27,866][0m Trial 21 finished with value: 0.18691503101742113 and parameters: {'observation_period_num': 110, 'train_rates': 0.8376453083784139, 'learning_rate': 0.0004482483471117688, 'batch_size': 185, 'step_size': 14, 'gamma': 0.9879645734227208}. Best is trial 20 with value: 0.13261030140248212.[0m
[32m[I 2025-02-06 06:24:53,735][0m Trial 22 finished with value: 0.12554811066157492 and parameters: {'observation_period_num': 128, 'train_rates': 0.827199931691542, 'learning_rate': 0.0003192841648402723, 'batch_size': 232, 'step_size': 11, 'gamma': 0.9665488547389418}. Best is trial 22 with value: 0.12554811066157492.[0m
[32m[I 2025-02-06 06:25:21,150][0m Trial 23 finished with value: 0.1699880973388583 and parameters: {'observation_period_num': 130, 'train_rates': 0.9166722627782119, 'learning_rate': 0.0004750569053808856, 'batch_size': 227, 'step_size': 14, 'gamma': 0.9671221429662517}. Best is trial 22 with value: 0.12554811066157492.[0m
[32m[I 2025-02-06 06:25:45,277][0m Trial 24 finished with value: 0.28676404382871545 and parameters: {'observation_period_num': 159, 'train_rates': 0.7944277006162659, 'learning_rate': 8.642927945119181e-05, 'batch_size': 238, 'step_size': 12, 'gamma': 0.9308009069876523}. Best is trial 22 with value: 0.12554811066157492.[0m
[32m[I 2025-02-06 06:26:15,535][0m Trial 25 finished with value: 0.18653816945152357 and parameters: {'observation_period_num': 186, 'train_rates': 0.8151625916754197, 'learning_rate': 4.045725343962028e-05, 'batch_size': 174, 'step_size': 14, 'gamma': 0.9679120418863127}. Best is trial 22 with value: 0.12554811066157492.[0m
[32m[I 2025-02-06 06:26:41,682][0m Trial 26 finished with value: 0.1845690906047821 and parameters: {'observation_period_num': 200, 'train_rates': 0.9192747799301583, 'learning_rate': 0.00015222817832207701, 'batch_size': 233, 'step_size': 10, 'gamma': 0.9394362880996675}. Best is trial 22 with value: 0.12554811066157492.[0m
[32m[I 2025-02-06 06:27:09,622][0m Trial 27 finished with value: 0.10262576178196937 and parameters: {'observation_period_num': 117, 'train_rates': 0.8253518130349398, 'learning_rate': 0.0005449493538313561, 'batch_size': 196, 'step_size': 14, 'gamma': 0.9631554567406962}. Best is trial 27 with value: 0.10262576178196937.[0m
[32m[I 2025-02-06 06:27:41,272][0m Trial 28 finished with value: 0.17515117808001665 and parameters: {'observation_period_num': 117, 'train_rates': 0.8303884759321918, 'learning_rate': 2.2026862328590172e-05, 'batch_size': 169, 'step_size': 11, 'gamma': 0.9752901160786271}. Best is trial 27 with value: 0.10262576178196937.[0m
[32m[I 2025-02-06 06:28:22,710][0m Trial 29 finished with value: 0.15584601138041101 and parameters: {'observation_period_num': 152, 'train_rates': 0.8745307456184431, 'learning_rate': 0.0005210756340012643, 'batch_size': 139, 'step_size': 8, 'gamma': 0.9563454094399891}. Best is trial 27 with value: 0.10262576178196937.[0m
[32m[I 2025-02-06 06:28:53,394][0m Trial 30 finished with value: 0.16313625872135162 and parameters: {'observation_period_num': 94, 'train_rates': 0.9537998638247869, 'learning_rate': 5.775383268976594e-05, 'batch_size': 197, 'step_size': 13, 'gamma': 0.9736141350819867}. Best is trial 27 with value: 0.10262576178196937.[0m
[32m[I 2025-02-06 06:29:17,590][0m Trial 31 finished with value: 0.24682597456845695 and parameters: {'observation_period_num': 124, 'train_rates': 0.7642210222073698, 'learning_rate': 0.000753458994235579, 'batch_size': 222, 'step_size': 15, 'gamma': 0.9320131906858293}. Best is trial 27 with value: 0.10262576178196937.[0m
[32m[I 2025-02-06 06:29:41,614][0m Trial 32 finished with value: 0.0848515737673332 and parameters: {'observation_period_num': 85, 'train_rates': 0.8182163836377717, 'learning_rate': 0.00021745058738318433, 'batch_size': 240, 'step_size': 14, 'gamma': 0.9511837023928487}. Best is trial 32 with value: 0.0848515737673332.[0m
[32m[I 2025-02-06 06:30:09,696][0m Trial 33 finished with value: 0.0982199961026208 and parameters: {'observation_period_num': 90, 'train_rates': 0.816232007192402, 'learning_rate': 0.00021886499358304368, 'batch_size': 196, 'step_size': 14, 'gamma': 0.9872473444931357}. Best is trial 32 with value: 0.0848515737673332.[0m
[32m[I 2025-02-06 06:30:36,109][0m Trial 34 finished with value: 0.10209198551966209 and parameters: {'observation_period_num': 91, 'train_rates': 0.8505102019482749, 'learning_rate': 0.00022403417498618607, 'batch_size': 237, 'step_size': 12, 'gamma': 0.9609815900365833}. Best is trial 32 with value: 0.0848515737673332.[0m
[32m[I 2025-02-06 06:30:59,567][0m Trial 35 finished with value: 0.2006459211691832 and parameters: {'observation_period_num': 252, 'train_rates': 0.8558789246719317, 'learning_rate': 0.00018173058013635962, 'batch_size': 242, 'step_size': 14, 'gamma': 0.9573262450459493}. Best is trial 32 with value: 0.0848515737673332.[0m
[32m[I 2025-02-06 06:31:31,109][0m Trial 36 finished with value: 0.10175857148760913 and parameters: {'observation_period_num': 88, 'train_rates': 0.8888579012852057, 'learning_rate': 7.527097241598643e-05, 'batch_size': 194, 'step_size': 5, 'gamma': 0.950797273483361}. Best is trial 32 with value: 0.0848515737673332.[0m
[32m[I 2025-02-06 06:32:01,286][0m Trial 37 finished with value: 0.07600536888594148 and parameters: {'observation_period_num': 62, 'train_rates': 0.8902089799926493, 'learning_rate': 9.31538531729939e-05, 'batch_size': 206, 'step_size': 5, 'gamma': 0.9485944371307999}. Best is trial 37 with value: 0.07600536888594148.[0m
[32m[I 2025-02-06 06:32:40,575][0m Trial 38 finished with value: 0.04739394597709179 and parameters: {'observation_period_num': 26, 'train_rates': 0.8948768225593838, 'learning_rate': 0.00010241472253908938, 'batch_size': 154, 'step_size': 5, 'gamma': 0.9297803582742555}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:33:34,730][0m Trial 39 finished with value: 0.04812871568969318 and parameters: {'observation_period_num': 28, 'train_rates': 0.9283827347653714, 'learning_rate': 0.00011713765810931017, 'batch_size': 113, 'step_size': 4, 'gamma': 0.9254132824874732}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:34:29,167][0m Trial 40 finished with value: 0.06142778304466129 and parameters: {'observation_period_num': 28, 'train_rates': 0.942421990400445, 'learning_rate': 0.00011552767935132298, 'batch_size': 111, 'step_size': 3, 'gamma': 0.8950778237239881}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:35:23,116][0m Trial 41 finished with value: 0.05258767903308196 and parameters: {'observation_period_num': 12, 'train_rates': 0.9447654145057178, 'learning_rate': 0.00012362744320035745, 'batch_size': 112, 'step_size': 3, 'gamma': 0.8978819377407707}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:36:17,289][0m Trial 42 finished with value: 0.07103385742920548 and parameters: {'observation_period_num': 7, 'train_rates': 0.9548025306404951, 'learning_rate': 0.00010857222345120162, 'batch_size': 113, 'step_size': 3, 'gamma': 0.8837817917122684}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:37:10,703][0m Trial 43 finished with value: 0.07444047182798386 and parameters: {'observation_period_num': 10, 'train_rates': 0.9802281837706152, 'learning_rate': 0.00011662022763996422, 'batch_size': 116, 'step_size': 2, 'gamma': 0.871332998635991}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:38:22,495][0m Trial 44 finished with value: 0.06708830782747645 and parameters: {'observation_period_num': 24, 'train_rates': 0.9566592360144077, 'learning_rate': 5.157026028363309e-05, 'batch_size': 83, 'step_size': 4, 'gamma': 0.8827982358570189}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:39:34,534][0m Trial 45 finished with value: 0.06577981556765736 and parameters: {'observation_period_num': 26, 'train_rates': 0.9455033963038626, 'learning_rate': 4.624631856307088e-05, 'batch_size': 83, 'step_size': 4, 'gamma': 0.9000202118133059}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:40:41,732][0m Trial 46 finished with value: 0.07220036644890834 and parameters: {'observation_period_num': 29, 'train_rates': 0.92746946084177, 'learning_rate': 3.2278747005528206e-05, 'batch_size': 88, 'step_size': 6, 'gamma': 0.8644446323036422}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:42:33,463][0m Trial 47 finished with value: 0.11325720697641373 and parameters: {'observation_period_num': 20, 'train_rates': 0.9378907002598982, 'learning_rate': 4.567111021905277e-05, 'batch_size': 52, 'step_size': 1, 'gamma': 0.9009782855536939}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:43:20,900][0m Trial 48 finished with value: 0.32087180998910314 and parameters: {'observation_period_num': 44, 'train_rates': 0.9076989310014155, 'learning_rate': 9.093442329592194e-06, 'batch_size': 127, 'step_size': 3, 'gamma': 0.8978091086097205}. Best is trial 38 with value: 0.04739394597709179.[0m
[32m[I 2025-02-06 06:44:22,418][0m Trial 49 finished with value: 0.08011192083358765 and parameters: {'observation_period_num': 38, 'train_rates': 0.9705631908978177, 'learning_rate': 6.33358825783928e-05, 'batch_size': 98, 'step_size': 4, 'gamma': 0.9243568812589784}. Best is trial 38 with value: 0.04739394597709179.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-06 06:44:22,429][0m A new study created in memory with name: no-name-ae45dc06-246d-4ff3-9b6e-98608c25e43f[0m
[32m[I 2025-02-06 06:44:56,054][0m Trial 0 finished with value: 0.03798363413813153 and parameters: {'observation_period_num': 8, 'train_rates': 0.9165309342780067, 'learning_rate': 0.0004116085375995412, 'batch_size': 181, 'step_size': 15, 'gamma': 0.968912937128094}. Best is trial 0 with value: 0.03798363413813153.[0m
[32m[I 2025-02-06 06:46:09,427][0m Trial 1 finished with value: 0.9066938910652046 and parameters: {'observation_period_num': 42, 'train_rates': 0.7267124500262847, 'learning_rate': 1.6687645149769945e-06, 'batch_size': 68, 'step_size': 3, 'gamma': 0.8548498898149572}. Best is trial 0 with value: 0.03798363413813153.[0m
[32m[I 2025-02-06 06:46:37,061][0m Trial 2 finished with value: 0.15446132720763714 and parameters: {'observation_period_num': 93, 'train_rates': 0.9165228283589106, 'learning_rate': 0.0003961288621878342, 'batch_size': 238, 'step_size': 5, 'gamma': 0.9796532097134941}. Best is trial 0 with value: 0.03798363413813153.[0m
[32m[I 2025-02-06 06:47:03,359][0m Trial 3 finished with value: 0.30562933911981227 and parameters: {'observation_period_num': 140, 'train_rates': 0.7910510258738437, 'learning_rate': 1.2514711399618778e-05, 'batch_size': 205, 'step_size': 13, 'gamma': 0.8798647383160807}. Best is trial 0 with value: 0.03798363413813153.[0m
[32m[I 2025-02-06 06:49:28,662][0m Trial 4 finished with value: 0.3474894027083607 and parameters: {'observation_period_num': 205, 'train_rates': 0.6138269314145057, 'learning_rate': 1.5107236386321602e-05, 'batch_size': 28, 'step_size': 12, 'gamma': 0.8812889762851125}. Best is trial 0 with value: 0.03798363413813153.[0m
[32m[I 2025-02-06 06:50:22,435][0m Trial 5 finished with value: 0.1673336060046342 and parameters: {'observation_period_num': 62, 'train_rates': 0.6173143231982459, 'learning_rate': 6.371161712858778e-05, 'batch_size': 83, 'step_size': 13, 'gamma': 0.8254869532263148}. Best is trial 0 with value: 0.03798363413813153.[0m
[32m[I 2025-02-06 06:51:09,719][0m Trial 6 finished with value: 0.36891498352851454 and parameters: {'observation_period_num': 186, 'train_rates': 0.7006929624699595, 'learning_rate': 3.846657604467349e-05, 'batch_size': 100, 'step_size': 9, 'gamma': 0.7794634376041378}. Best is trial 0 with value: 0.03798363413813153.[0m
[32m[I 2025-02-06 06:51:59,148][0m Trial 7 finished with value: 0.12846729900572718 and parameters: {'observation_period_num': 154, 'train_rates': 0.7924815784976612, 'learning_rate': 8.756795390876087e-05, 'batch_size': 109, 'step_size': 10, 'gamma': 0.8847803420359781}. Best is trial 0 with value: 0.03798363413813153.[0m
[32m[I 2025-02-06 06:52:26,675][0m Trial 8 finished with value: 0.14770197868347168 and parameters: {'observation_period_num': 62, 'train_rates': 0.952654688187929, 'learning_rate': 0.00022920855396514825, 'batch_size': 231, 'step_size': 4, 'gamma': 0.7662563109448206}. Best is trial 0 with value: 0.03798363413813153.[0m
[32m[I 2025-02-06 06:54:09,567][0m Trial 9 finished with value: 0.1650999465409447 and parameters: {'observation_period_num': 225, 'train_rates': 0.8131700005413537, 'learning_rate': 0.000405689388072685, 'batch_size': 48, 'step_size': 1, 'gamma': 0.8479963013622445}. Best is trial 0 with value: 0.03798363413813153.[0m
[32m[I 2025-02-06 06:54:45,622][0m Trial 10 finished with value: 0.03684883327646689 and parameters: {'observation_period_num': 10, 'train_rates': 0.8879385557813501, 'learning_rate': 0.0009262979773018518, 'batch_size': 168, 'step_size': 15, 'gamma': 0.9825269835543881}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 06:55:21,571][0m Trial 11 finished with value: 0.0692136748229629 and parameters: {'observation_period_num': 16, 'train_rates': 0.8876555032480026, 'learning_rate': 0.0008208800414648102, 'batch_size': 166, 'step_size': 15, 'gamma': 0.9892561630306651}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 06:55:59,847][0m Trial 12 finished with value: 0.04065095570458229 and parameters: {'observation_period_num': 9, 'train_rates': 0.8834972132129552, 'learning_rate': 0.000998622199634433, 'batch_size': 158, 'step_size': 15, 'gamma': 0.9292506497114935}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 06:56:32,952][0m Trial 13 finished with value: 0.1098310574889183 and parameters: {'observation_period_num': 88, 'train_rates': 0.9694231327182212, 'learning_rate': 0.00016544602422581835, 'batch_size': 186, 'step_size': 7, 'gamma': 0.9407908681210395}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 06:57:13,408][0m Trial 14 finished with value: 0.2583627440035343 and parameters: {'observation_period_num': 9, 'train_rates': 0.858971158550719, 'learning_rate': 1.0525700026718127e-06, 'batch_size': 143, 'step_size': 11, 'gamma': 0.9433247945828012}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 06:57:45,029][0m Trial 15 finished with value: 0.48364743518368636 and parameters: {'observation_period_num': 102, 'train_rates': 0.9277064067679689, 'learning_rate': 3.986586631419585e-06, 'batch_size': 196, 'step_size': 15, 'gamma': 0.9204481985508106}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 06:58:38,356][0m Trial 16 finished with value: 0.0656758708561339 and parameters: {'observation_period_num': 42, 'train_rates': 0.8452259180678735, 'learning_rate': 0.0004354989841301678, 'batch_size': 110, 'step_size': 7, 'gamma': 0.9607941408608598}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 06:59:09,781][0m Trial 17 finished with value: 0.079277403652668 and parameters: {'observation_period_num': 44, 'train_rates': 0.9782365017036841, 'learning_rate': 0.0001347514403162485, 'batch_size': 214, 'step_size': 13, 'gamma': 0.9177422428949975}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 06:59:43,649][0m Trial 18 finished with value: 0.19750487804412842 and parameters: {'observation_period_num': 173, 'train_rates': 0.9226951031210061, 'learning_rate': 0.0005671441471819069, 'batch_size': 172, 'step_size': 11, 'gamma': 0.9658586425494703}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 07:00:22,606][0m Trial 19 finished with value: 0.18367683014797945 and parameters: {'observation_period_num': 251, 'train_rates': 0.8402044325278842, 'learning_rate': 0.00025134684609259925, 'batch_size': 134, 'step_size': 14, 'gamma': 0.9035450011087719}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 07:01:00,740][0m Trial 20 finished with value: 0.32149621733755257 and parameters: {'observation_period_num': 117, 'train_rates': 0.7531207053760561, 'learning_rate': 1.5985505479593143e-05, 'batch_size': 139, 'step_size': 9, 'gamma': 0.9591806481576645}. Best is trial 10 with value: 0.03684883327646689.[0m
[32m[I 2025-02-06 07:01:39,704][0m Trial 21 finished with value: 0.027388363033967533 and parameters: {'observation_period_num': 5, 'train_rates': 0.8839016787321132, 'learning_rate': 0.000674165147135357, 'batch_size': 160, 'step_size': 15, 'gamma': 0.9358357495305698}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:02:13,278][0m Trial 22 finished with value: 0.06702491300383852 and parameters: {'observation_period_num': 28, 'train_rates': 0.8843493832000864, 'learning_rate': 0.0008851821421883517, 'batch_size': 182, 'step_size': 14, 'gamma': 0.9868020127048014}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:02:38,485][0m Trial 23 finished with value: 0.09716278314590454 and parameters: {'observation_period_num': 68, 'train_rates': 0.9366094273220229, 'learning_rate': 0.0002849813146156956, 'batch_size': 250, 'step_size': 12, 'gamma': 0.9505625046386195}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:03:18,412][0m Trial 24 finished with value: 0.07929528919096286 and parameters: {'observation_period_num': 24, 'train_rates': 0.900194360392249, 'learning_rate': 0.00011890913820675744, 'batch_size': 152, 'step_size': 15, 'gamma': 0.9753459751173131}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:04:03,994][0m Trial 25 finished with value: 0.027752279241161998 and parameters: {'observation_period_num': 5, 'train_rates': 0.8243137548537787, 'learning_rate': 0.0005453526779796516, 'batch_size': 125, 'step_size': 14, 'gamma': 0.8988532987580962}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:04:47,655][0m Trial 26 finished with value: 0.04696640657617691 and parameters: {'observation_period_num': 38, 'train_rates': 0.8066334482802395, 'learning_rate': 0.0006654348482923839, 'batch_size': 124, 'step_size': 12, 'gamma': 0.8952763846007378}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:05:47,355][0m Trial 27 finished with value: 0.199626245655173 and parameters: {'observation_period_num': 69, 'train_rates': 0.7591707686483967, 'learning_rate': 4.919746234993572e-05, 'batch_size': 86, 'step_size': 14, 'gamma': 0.9084062084603504}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:06:34,719][0m Trial 28 finished with value: 0.0703735413265399 and parameters: {'observation_period_num': 29, 'train_rates': 0.8568883765932278, 'learning_rate': 2.4869904088149284e-05, 'batch_size': 122, 'step_size': 13, 'gamma': 0.8200499431771399}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:07:11,029][0m Trial 29 finished with value: 0.031833258048877884 and parameters: {'observation_period_num': 6, 'train_rates': 0.828491566859148, 'learning_rate': 0.0005162184541270941, 'batch_size': 158, 'step_size': 11, 'gamma': 0.9324652227163813}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:07:50,219][0m Trial 30 finished with value: 0.03578661011773988 and parameters: {'observation_period_num': 5, 'train_rates': 0.8200375021555861, 'learning_rate': 0.00021995294869011633, 'batch_size': 150, 'step_size': 11, 'gamma': 0.9313794926535985}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:08:28,859][0m Trial 31 finished with value: 0.033461584927183444 and parameters: {'observation_period_num': 5, 'train_rates': 0.8188865008879542, 'learning_rate': 0.0004882888697377009, 'batch_size': 145, 'step_size': 10, 'gamma': 0.9313586480789646}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:09:10,469][0m Trial 32 finished with value: 0.1785240325259267 and parameters: {'observation_period_num': 43, 'train_rates': 0.7669362899997044, 'learning_rate': 0.0005502153238517767, 'batch_size': 125, 'step_size': 9, 'gamma': 0.8555030713731782}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:09:44,648][0m Trial 33 finished with value: 0.14681947429855616 and parameters: {'observation_period_num': 25, 'train_rates': 0.7159028368398046, 'learning_rate': 0.00029536223237157556, 'batch_size': 157, 'step_size': 7, 'gamma': 0.9124962356900611}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:10:13,011][0m Trial 34 finished with value: 0.36464113670793913 and parameters: {'observation_period_num': 53, 'train_rates': 0.8308584507396453, 'learning_rate': 4.92856186415111e-06, 'batch_size': 214, 'step_size': 10, 'gamma': 0.9286784085109691}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:11:09,115][0m Trial 35 finished with value: 0.23097628457171301 and parameters: {'observation_period_num': 87, 'train_rates': 0.7839586873783175, 'learning_rate': 0.0003950532047522571, 'batch_size': 90, 'step_size': 10, 'gamma': 0.8975486033114256}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:12:35,760][0m Trial 36 finished with value: 0.07001248932940698 and parameters: {'observation_period_num': 23, 'train_rates': 0.853673552922464, 'learning_rate': 0.0005622710771014915, 'batch_size': 64, 'step_size': 8, 'gamma': 0.8715024011229199}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:13:02,762][0m Trial 37 finished with value: 0.17236060267354633 and parameters: {'observation_period_num': 76, 'train_rates': 0.6792486802950329, 'learning_rate': 9.624906587879319e-05, 'batch_size': 193, 'step_size': 12, 'gamma': 0.9452298914855352}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:13:33,801][0m Trial 38 finished with value: 0.16760605436509024 and parameters: {'observation_period_num': 18, 'train_rates': 0.739330749848086, 'learning_rate': 0.00017616676795630484, 'batch_size': 174, 'step_size': 6, 'gamma': 0.8871935281873539}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:14:27,865][0m Trial 39 finished with value: 0.08292405411254528 and parameters: {'observation_period_num': 56, 'train_rates': 0.8666808111193279, 'learning_rate': 0.0003658336104530292, 'batch_size': 108, 'step_size': 14, 'gamma': 0.8635789422660956}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:15:11,092][0m Trial 40 finished with value: 0.05842815003272006 and parameters: {'observation_period_num': 36, 'train_rates': 0.7992521385989695, 'learning_rate': 6.416046372912305e-05, 'batch_size': 129, 'step_size': 13, 'gamma': 0.9328663850320978}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:15:51,296][0m Trial 41 finished with value: 0.03137157724321677 and parameters: {'observation_period_num': 6, 'train_rates': 0.8206984124313289, 'learning_rate': 0.0006409692883052044, 'batch_size': 148, 'step_size': 11, 'gamma': 0.9230831458059239}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:16:30,342][0m Trial 42 finished with value: 0.16416903713198583 and parameters: {'observation_period_num': 5, 'train_rates': 0.78003335704376, 'learning_rate': 0.0005905220931905795, 'batch_size': 142, 'step_size': 11, 'gamma': 0.9222649018346212}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:17:06,365][0m Trial 43 finished with value: 0.03744819403958509 and parameters: {'observation_period_num': 17, 'train_rates': 0.8275736531547376, 'learning_rate': 0.0007243357752320126, 'batch_size': 161, 'step_size': 10, 'gamma': 0.9531843497805043}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:17:58,356][0m Trial 44 finished with value: 0.04652027305051246 and parameters: {'observation_period_num': 32, 'train_rates': 0.9070332666943328, 'learning_rate': 0.00033574887583838616, 'batch_size': 115, 'step_size': 8, 'gamma': 0.8916751238039333}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:18:57,674][0m Trial 45 finished with value: 0.05493138363004051 and parameters: {'observation_period_num': 48, 'train_rates': 0.8721477183209737, 'learning_rate': 0.00042137466837619973, 'batch_size': 95, 'step_size': 2, 'gamma': 0.9049905008885372}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:19:36,367][0m Trial 46 finished with value: 0.043001523179312544 and parameters: {'observation_period_num': 16, 'train_rates': 0.8164599398379677, 'learning_rate': 0.00020468775384773144, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8754914151255916}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:20:11,546][0m Trial 47 finished with value: 0.04761591624036879 and parameters: {'observation_period_num': 16, 'train_rates': 0.8414089833505972, 'learning_rate': 0.0009839160145616872, 'batch_size': 176, 'step_size': 9, 'gamma': 0.9381711612699815}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:21:19,391][0m Trial 48 finished with value: 0.458506807934813 and parameters: {'observation_period_num': 144, 'train_rates': 0.6605248136719201, 'learning_rate': 0.0004906259233411318, 'batch_size': 68, 'step_size': 11, 'gamma': 0.9248462042592331}. Best is trial 21 with value: 0.027388363033967533.[0m
[32m[I 2025-02-06 07:24:53,671][0m Trial 49 finished with value: 0.16411237066259254 and parameters: {'observation_period_num': 5, 'train_rates': 0.7744069356526533, 'learning_rate': 0.0007034119171565776, 'batch_size': 24, 'step_size': 13, 'gamma': 0.8307937403717943}. Best is trial 21 with value: 0.027388363033967533.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-06 07:24:53,681][0m A new study created in memory with name: no-name-3d2dc7c5-aa88-4936-b9a0-bdd7dbef8aa0[0m
[32m[I 2025-02-06 07:25:39,714][0m Trial 0 finished with value: 0.9260997964047838 and parameters: {'observation_period_num': 188, 'train_rates': 0.7799086434735407, 'learning_rate': 1.9515498714472694e-06, 'batch_size': 109, 'step_size': 10, 'gamma': 0.7910737356402797}. Best is trial 0 with value: 0.9260997964047838.[0m
[32m[I 2025-02-06 07:26:34,199][0m Trial 1 finished with value: 0.09719094447791576 and parameters: {'observation_period_num': 138, 'train_rates': 0.8595539821601914, 'learning_rate': 0.0003337581049837662, 'batch_size': 99, 'step_size': 5, 'gamma': 0.9026157205222589}. Best is trial 1 with value: 0.09719094447791576.[0m
[32m[I 2025-02-06 07:29:06,004][0m Trial 2 finished with value: 0.23633399349911965 and parameters: {'observation_period_num': 53, 'train_rates': 0.7371056620672742, 'learning_rate': 6.860152479573477e-06, 'batch_size': 32, 'step_size': 13, 'gamma': 0.8554410602488796}. Best is trial 1 with value: 0.09719094447791576.[0m
[32m[I 2025-02-06 07:30:49,745][0m Trial 3 finished with value: 0.28486740881321476 and parameters: {'observation_period_num': 229, 'train_rates': 0.6257249167806862, 'learning_rate': 6.264512400035682e-05, 'batch_size': 40, 'step_size': 9, 'gamma': 0.9870987752477481}. Best is trial 1 with value: 0.09719094447791576.[0m
[32m[I 2025-02-06 07:32:14,481][0m Trial 4 finished with value: 0.3243129670080556 and parameters: {'observation_period_num': 122, 'train_rates': 0.7660186278419436, 'learning_rate': 1.1275511993358435e-05, 'batch_size': 60, 'step_size': 4, 'gamma': 0.9573490798397848}. Best is trial 1 with value: 0.09719094447791576.[0m
[32m[I 2025-02-06 07:32:35,293][0m Trial 5 finished with value: 0.1778519002117269 and parameters: {'observation_period_num': 32, 'train_rates': 0.6589430314488417, 'learning_rate': 9.140818242111881e-05, 'batch_size': 247, 'step_size': 12, 'gamma': 0.977192336744183}. Best is trial 1 with value: 0.09719094447791576.[0m
Early stopping at epoch 61
[32m[I 2025-02-06 07:32:51,265][0m Trial 6 finished with value: 1.4624441490142344 and parameters: {'observation_period_num': 84, 'train_rates': 0.6786080175463669, 'learning_rate': 1.519350722103196e-05, 'batch_size': 216, 'step_size': 1, 'gamma': 0.8024580157155611}. Best is trial 1 with value: 0.09719094447791576.[0m
[32m[I 2025-02-06 07:33:40,829][0m Trial 7 finished with value: 0.08790358901023865 and parameters: {'observation_period_num': 68, 'train_rates': 0.9743535743413496, 'learning_rate': 0.00013855257502459555, 'batch_size': 126, 'step_size': 12, 'gamma': 0.8295410311591227}. Best is trial 7 with value: 0.08790358901023865.[0m
[32m[I 2025-02-06 07:34:55,546][0m Trial 8 finished with value: 0.4228838562742155 and parameters: {'observation_period_num': 166, 'train_rates': 0.9044011646336363, 'learning_rate': 2.779816351254308e-06, 'batch_size': 74, 'step_size': 9, 'gamma': 0.9116933714392188}. Best is trial 7 with value: 0.08790358901023865.[0m
[32m[I 2025-02-06 07:35:28,356][0m Trial 9 finished with value: 0.5757313869945411 and parameters: {'observation_period_num': 167, 'train_rates': 0.8454154097110873, 'learning_rate': 1.5234240829134964e-06, 'batch_size': 176, 'step_size': 10, 'gamma': 0.986106106688853}. Best is trial 7 with value: 0.08790358901023865.[0m
[32m[I 2025-02-06 07:36:11,963][0m Trial 10 finished with value: 0.03552640974521637 and parameters: {'observation_period_num': 5, 'train_rates': 0.9720128362699987, 'learning_rate': 0.0008201059028559173, 'batch_size': 155, 'step_size': 14, 'gamma': 0.8325977168344202}. Best is trial 10 with value: 0.03552640974521637.[0m
[32m[I 2025-02-06 07:36:50,820][0m Trial 11 finished with value: 0.025262389332056046 and parameters: {'observation_period_num': 11, 'train_rates': 0.9842128583282893, 'learning_rate': 0.0008405298478883172, 'batch_size': 162, 'step_size': 15, 'gamma': 0.8374829207376403}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:37:27,731][0m Trial 12 finished with value: 0.02844851277768612 and parameters: {'observation_period_num': 7, 'train_rates': 0.9862280689749507, 'learning_rate': 0.000996062801567773, 'batch_size': 173, 'step_size': 15, 'gamma': 0.7561238235143871}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:38:00,328][0m Trial 13 finished with value: 0.030219656995317916 and parameters: {'observation_period_num': 6, 'train_rates': 0.922695775725069, 'learning_rate': 0.0009947650993988983, 'batch_size': 185, 'step_size': 15, 'gamma': 0.7765707334148255}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:38:31,121][0m Trial 14 finished with value: 0.08457235246896744 and parameters: {'observation_period_num': 92, 'train_rates': 0.9850637505782854, 'learning_rate': 0.0003060634822898398, 'batch_size': 204, 'step_size': 15, 'gamma': 0.7643488789963727}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:39:12,086][0m Trial 15 finished with value: 0.05273087471723557 and parameters: {'observation_period_num': 40, 'train_rates': 0.9125557669446801, 'learning_rate': 0.00033861915816139204, 'batch_size': 147, 'step_size': 7, 'gamma': 0.7539413253026531}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:39:34,483][0m Trial 16 finished with value: 0.13629789353765515 and parameters: {'observation_period_num': 104, 'train_rates': 0.8451331100701664, 'learning_rate': 3.6892486420316825e-05, 'batch_size': 245, 'step_size': 12, 'gamma': 0.8849670632358235}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:40:08,663][0m Trial 17 finished with value: 0.046960177039032554 and parameters: {'observation_period_num': 29, 'train_rates': 0.9314965449528476, 'learning_rate': 0.0001933772988299787, 'batch_size': 167, 'step_size': 15, 'gamma': 0.8165223154710268}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:40:36,234][0m Trial 18 finished with value: 0.19843138632150104 and parameters: {'observation_period_num': 249, 'train_rates': 0.8829987537259989, 'learning_rate': 0.0005944392897715191, 'batch_size': 211, 'step_size': 13, 'gamma': 0.8557939165822858}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:41:20,360][0m Trial 19 finished with value: 0.05773476888679964 and parameters: {'observation_period_num': 64, 'train_rates': 0.8165565963586054, 'learning_rate': 0.00040835139942896695, 'batch_size': 130, 'step_size': 6, 'gamma': 0.9389414086728389}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:41:53,988][0m Trial 20 finished with value: 0.1961686909198761 and parameters: {'observation_period_num': 23, 'train_rates': 0.9428677607096304, 'learning_rate': 0.00017194847243158086, 'batch_size': 191, 'step_size': 2, 'gamma': 0.7837582376232988}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:42:30,112][0m Trial 21 finished with value: 0.03185683488845825 and parameters: {'observation_period_num': 7, 'train_rates': 0.9486026294383699, 'learning_rate': 0.0005752125591679544, 'batch_size': 181, 'step_size': 15, 'gamma': 0.7774718215916117}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:43:06,950][0m Trial 22 finished with value: 0.046763867139816284 and parameters: {'observation_period_num': 49, 'train_rates': 0.9877242669338759, 'learning_rate': 0.0009927924964329218, 'batch_size': 160, 'step_size': 14, 'gamma': 0.751384357372089}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:43:32,969][0m Trial 23 finished with value: 0.032713303342461586 and parameters: {'observation_period_num': 6, 'train_rates': 0.8970468601307655, 'learning_rate': 0.0009184103874066371, 'batch_size': 228, 'step_size': 11, 'gamma': 0.8086450961407787}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:44:02,039][0m Trial 24 finished with value: 0.037608254700899124 and parameters: {'observation_period_num': 27, 'train_rates': 0.9495061782777181, 'learning_rate': 0.0005641632097721181, 'batch_size': 191, 'step_size': 14, 'gamma': 0.7726558982079724}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:44:41,256][0m Trial 25 finished with value: 0.07866166968265345 and parameters: {'observation_period_num': 65, 'train_rates': 0.9334242305363305, 'learning_rate': 0.00027548255034719275, 'batch_size': 148, 'step_size': 15, 'gamma': 0.8442183780354067}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:45:09,571][0m Trial 26 finished with value: 0.06288292634519284 and parameters: {'observation_period_num': 17, 'train_rates': 0.8739468600997298, 'learning_rate': 8.02559649837024e-05, 'batch_size': 192, 'step_size': 13, 'gamma': 0.7954711492006196}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:45:52,477][0m Trial 27 finished with value: 0.08028654664974406 and parameters: {'observation_period_num': 43, 'train_rates': 0.8136290597312648, 'learning_rate': 3.318563136719232e-05, 'batch_size': 115, 'step_size': 11, 'gamma': 0.8203317829041222}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:46:17,996][0m Trial 28 finished with value: 0.08638954907655716 and parameters: {'observation_period_num': 79, 'train_rates': 0.9632533306422594, 'learning_rate': 0.0004912518991711462, 'batch_size': 225, 'step_size': 13, 'gamma': 0.8699410127270752}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:47:08,947][0m Trial 29 finished with value: 0.12816774910767173 and parameters: {'observation_period_num': 212, 'train_rates': 0.9172698365693526, 'learning_rate': 0.00013128656477793835, 'batch_size': 100, 'step_size': 11, 'gamma': 0.7873047520230727}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:47:43,487][0m Trial 30 finished with value: 0.22106894786680392 and parameters: {'observation_period_num': 124, 'train_rates': 0.7205241940532805, 'learning_rate': 0.0007169944274273323, 'batch_size': 139, 'step_size': 14, 'gamma': 0.7665672342688088}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:48:17,294][0m Trial 31 finished with value: 0.05190687254071236 and parameters: {'observation_period_num': 5, 'train_rates': 0.9438902350609637, 'learning_rate': 0.0009932656832740544, 'batch_size': 177, 'step_size': 15, 'gamma': 0.7777544581062871}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:48:49,689][0m Trial 32 finished with value: 0.035115670412778854 and parameters: {'observation_period_num': 15, 'train_rates': 0.9566040507846716, 'learning_rate': 0.0004672863887917067, 'batch_size': 174, 'step_size': 15, 'gamma': 0.7959777597561959}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:49:20,978][0m Trial 33 finished with value: 0.05812007561326027 and parameters: {'observation_period_num': 52, 'train_rates': 0.9896556001759901, 'learning_rate': 0.0005926722767243853, 'batch_size': 196, 'step_size': 14, 'gamma': 0.7674802859496658}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:49:54,424][0m Trial 34 finished with value: 0.050288915104134295 and parameters: {'observation_period_num': 35, 'train_rates': 0.9158521795122954, 'learning_rate': 0.00027983749219342486, 'batch_size': 181, 'step_size': 13, 'gamma': 0.7502334473932017}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:50:32,009][0m Trial 35 finished with value: 0.028827162909096683 and parameters: {'observation_period_num': 13, 'train_rates': 0.8819512881503686, 'learning_rate': 0.0006679315630742457, 'batch_size': 158, 'step_size': 15, 'gamma': 0.7877260576023932}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:51:09,902][0m Trial 36 finished with value: 0.6973278628928321 and parameters: {'observation_period_num': 22, 'train_rates': 0.8710835778169362, 'learning_rate': 3.989157765187031e-06, 'batch_size': 157, 'step_size': 8, 'gamma': 0.8101979806414955}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:56:21,334][0m Trial 37 finished with value: 0.21606152876079263 and parameters: {'observation_period_num': 43, 'train_rates': 0.7740794209215064, 'learning_rate': 0.00023874848766607386, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8335276727545265}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:57:24,846][0m Trial 38 finished with value: 0.3417768610848321 and parameters: {'observation_period_num': 100, 'train_rates': 0.8520388334305009, 'learning_rate': 1.6439830060963424e-05, 'batch_size': 85, 'step_size': 4, 'gamma': 0.7916063155155768}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:58:13,267][0m Trial 39 finished with value: 0.12082454521329172 and parameters: {'observation_period_num': 140, 'train_rates': 0.8898863955599552, 'learning_rate': 5.13645074647686e-05, 'batch_size': 119, 'step_size': 14, 'gamma': 0.8513668100076293}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:58:46,249][0m Trial 40 finished with value: 0.15598455288595345 and parameters: {'observation_period_num': 59, 'train_rates': 0.6043257798245159, 'learning_rate': 0.0003892653764187567, 'batch_size': 142, 'step_size': 10, 'gamma': 0.8980512970639551}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 07:59:23,092][0m Trial 41 finished with value: 0.03497045859694481 and parameters: {'observation_period_num': 15, 'train_rates': 0.9622183193897261, 'learning_rate': 0.0006941214729856739, 'batch_size': 171, 'step_size': 15, 'gamma': 0.7845072507623405}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 08:00:00,643][0m Trial 42 finished with value: 0.0316796996674248 and parameters: {'observation_period_num': 5, 'train_rates': 0.9276543934260435, 'learning_rate': 0.0006317367790777626, 'batch_size': 164, 'step_size': 15, 'gamma': 0.7593781456350737}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 08:00:46,410][0m Trial 43 finished with value: 0.05561096935647808 and parameters: {'observation_period_num': 32, 'train_rates': 0.9250884008964153, 'learning_rate': 0.0007287111994419522, 'batch_size': 133, 'step_size': 14, 'gamma': 0.7627191355339458}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 08:01:25,944][0m Trial 44 finished with value: 0.027813073247671127 and parameters: {'observation_period_num': 19, 'train_rates': 0.9708291727400011, 'learning_rate': 0.00039465526981561977, 'batch_size': 161, 'step_size': 13, 'gamma': 0.8010171249272379}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 08:01:56,976][0m Trial 45 finished with value: 0.032758377492427826 and parameters: {'observation_period_num': 22, 'train_rates': 0.9722172575252094, 'learning_rate': 0.00040316797756043355, 'batch_size': 204, 'step_size': 13, 'gamma': 0.8015919714728649}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 08:02:36,535][0m Trial 46 finished with value: 0.10230065882205963 and parameters: {'observation_period_num': 73, 'train_rates': 0.9670157103036504, 'learning_rate': 0.00011976734564469363, 'batch_size': 159, 'step_size': 13, 'gamma': 0.8252604415450329}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 08:03:15,692][0m Trial 47 finished with value: 0.04740656316280365 and parameters: {'observation_period_num': 37, 'train_rates': 0.9026312124472622, 'learning_rate': 0.0001881716137277678, 'batch_size': 152, 'step_size': 14, 'gamma': 0.8102906758298466}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 08:04:06,811][0m Trial 48 finished with value: 0.037990961223840714 and parameters: {'observation_period_num': 54, 'train_rates': 0.9897398977855334, 'learning_rate': 0.0009777552998490623, 'batch_size': 122, 'step_size': 12, 'gamma': 0.8647563528383181}. Best is trial 11 with value: 0.025262389332056046.[0m
[32m[I 2025-02-06 08:04:34,077][0m Trial 49 finished with value: 0.4269876419743405 and parameters: {'observation_period_num': 173, 'train_rates': 0.7162276170266118, 'learning_rate': 2.0668116944207864e-05, 'batch_size': 185, 'step_size': 15, 'gamma': 0.8371482876595037}. Best is trial 11 with value: 0.025262389332056046.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-06 08:04:34,088][0m A new study created in memory with name: no-name-9c3ee6fc-10f5-49eb-8c87-515d90791740[0m
[32m[I 2025-02-06 08:05:16,048][0m Trial 0 finished with value: 0.8155959896158644 and parameters: {'observation_period_num': 78, 'train_rates': 0.6545695436894033, 'learning_rate': 1.5405728878095555e-06, 'batch_size': 114, 'step_size': 6, 'gamma': 0.9872770903692007}. Best is trial 0 with value: 0.8155959896158644.[0m
[32m[I 2025-02-06 08:06:38,493][0m Trial 1 finished with value: 0.28639745853078646 and parameters: {'observation_period_num': 53, 'train_rates': 0.7376520893411523, 'learning_rate': 8.412722252239888e-06, 'batch_size': 60, 'step_size': 15, 'gamma': 0.8449044076799769}. Best is trial 1 with value: 0.28639745853078646.[0m
[32m[I 2025-02-06 08:08:07,627][0m Trial 2 finished with value: 0.3483819906894186 and parameters: {'observation_period_num': 170, 'train_rates': 0.667355890061167, 'learning_rate': 9.4335250779088e-06, 'batch_size': 50, 'step_size': 14, 'gamma': 0.8723524682372672}. Best is trial 1 with value: 0.28639745853078646.[0m
[32m[I 2025-02-06 08:12:42,991][0m Trial 3 finished with value: 0.0749219377543736 and parameters: {'observation_period_num': 105, 'train_rates': 0.8417038424947536, 'learning_rate': 1.2790992190955024e-05, 'batch_size': 19, 'step_size': 6, 'gamma': 0.9582331826052005}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:13:06,645][0m Trial 4 finished with value: 0.2226327631821614 and parameters: {'observation_period_num': 156, 'train_rates': 0.8129580669197961, 'learning_rate': 2.2711101956469438e-05, 'batch_size': 249, 'step_size': 8, 'gamma': 0.9855801797880056}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:15:30,325][0m Trial 5 finished with value: 0.47432962361202446 and parameters: {'observation_period_num': 65, 'train_rates': 0.6783036830231657, 'learning_rate': 1.3903636779074308e-06, 'batch_size': 32, 'step_size': 15, 'gamma': 0.8616916184775246}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:16:00,631][0m Trial 6 finished with value: 0.22225113213062286 and parameters: {'observation_period_num': 76, 'train_rates': 0.9503928080247219, 'learning_rate': 2.1720612103757433e-05, 'batch_size': 222, 'step_size': 13, 'gamma': 0.9360673797849093}. Best is trial 3 with value: 0.0749219377543736.[0m
Early stopping at epoch 94
[32m[I 2025-02-06 08:16:40,870][0m Trial 7 finished with value: 0.7746189835920902 and parameters: {'observation_period_num': 207, 'train_rates': 0.7989827890770034, 'learning_rate': 1.4084435838994655e-05, 'batch_size': 129, 'step_size': 1, 'gamma': 0.8757238096918072}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:17:43,199][0m Trial 8 finished with value: 0.25442280264297723 and parameters: {'observation_period_num': 62, 'train_rates': 0.6564812464344452, 'learning_rate': 1.9637492846082225e-05, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9761333383555936}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:18:48,483][0m Trial 9 finished with value: 0.5752993786730275 and parameters: {'observation_period_num': 16, 'train_rates': 0.6101057864699435, 'learning_rate': 2.423526142568444e-06, 'batch_size': 70, 'step_size': 3, 'gamma': 0.8202217088001023}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:19:20,891][0m Trial 10 finished with value: 0.187275656272845 and parameters: {'observation_period_num': 237, 'train_rates': 0.9108238319399347, 'learning_rate': 0.0002839645069125438, 'batch_size': 179, 'step_size': 10, 'gamma': 0.7734688569908329}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:19:52,523][0m Trial 11 finished with value: 0.16677571002763647 and parameters: {'observation_period_num': 242, 'train_rates': 0.903743010287672, 'learning_rate': 0.0003295030150832575, 'batch_size': 185, 'step_size': 11, 'gamma': 0.7634822848530424}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:20:24,015][0m Trial 12 finished with value: 0.11793869534104141 and parameters: {'observation_period_num': 128, 'train_rates': 0.8628455810312581, 'learning_rate': 0.00014627263524465343, 'batch_size': 178, 'step_size': 11, 'gamma': 0.7569153690083563}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:20:59,517][0m Trial 13 finished with value: 0.12516354983138325 and parameters: {'observation_period_num': 115, 'train_rates': 0.858914980759745, 'learning_rate': 9.578939056386886e-05, 'batch_size': 170, 'step_size': 5, 'gamma': 0.9157661159628462}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:21:51,350][0m Trial 14 finished with value: 0.09147756666010409 and parameters: {'observation_period_num': 121, 'train_rates': 0.8393569903647994, 'learning_rate': 9.526429212497247e-05, 'batch_size': 107, 'step_size': 10, 'gamma': 0.8005354084826695}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:25:37,357][0m Trial 15 finished with value: 0.26462652661326497 and parameters: {'observation_period_num': 108, 'train_rates': 0.7446834827647192, 'learning_rate': 0.0008695134037489022, 'batch_size': 21, 'step_size': 9, 'gamma': 0.8103243429602177}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:26:41,403][0m Trial 16 finished with value: 0.1878458857536316 and parameters: {'observation_period_num': 156, 'train_rates': 0.9893757501412945, 'learning_rate': 6.484491907560085e-05, 'batch_size': 94, 'step_size': 4, 'gamma': 0.917054498563378}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:27:19,248][0m Trial 17 finished with value: 0.33216832630965626 and parameters: {'observation_period_num': 14, 'train_rates': 0.7505023560447629, 'learning_rate': 4.428255635500459e-06, 'batch_size': 147, 'step_size': 12, 'gamma': 0.7958373294693487}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:28:09,480][0m Trial 18 finished with value: 0.15829620785339707 and parameters: {'observation_period_num': 193, 'train_rates': 0.8314980302867422, 'learning_rate': 5.040205404242019e-05, 'batch_size': 107, 'step_size': 8, 'gamma': 0.907043387637049}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:30:38,938][0m Trial 19 finished with value: 0.10461560210841857 and parameters: {'observation_period_num': 95, 'train_rates': 0.8904313716138497, 'learning_rate': 4.117032805345535e-05, 'batch_size': 37, 'step_size': 3, 'gamma': 0.835884784504751}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:31:35,482][0m Trial 20 finished with value: 0.508769826550613 and parameters: {'observation_period_num': 140, 'train_rates': 0.7774601910838612, 'learning_rate': 5.147276643101612e-06, 'batch_size': 89, 'step_size': 6, 'gamma': 0.9541362367051313}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:37:11,744][0m Trial 21 finished with value: 0.10288299811192048 and parameters: {'observation_period_num': 99, 'train_rates': 0.8909419493475921, 'learning_rate': 4.082000248119923e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.8363221368588238}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:41:07,670][0m Trial 22 finished with value: 0.07767501066105784 and parameters: {'observation_period_num': 100, 'train_rates': 0.8464796585508856, 'learning_rate': 0.00012898715886196566, 'batch_size': 22, 'step_size': 2, 'gamma': 0.7945045760768561}. Best is trial 3 with value: 0.0749219377543736.[0m
Early stopping at epoch 74
[32m[I 2025-02-06 08:42:22,300][0m Trial 23 finished with value: 0.17644957496460775 and parameters: {'observation_period_num': 131, 'train_rates': 0.8361988801777535, 'learning_rate': 0.00018357324928055846, 'batch_size': 53, 'step_size': 1, 'gamma': 0.7877261323097525}. Best is trial 3 with value: 0.0749219377543736.[0m
[32m[I 2025-02-06 08:44:29,072][0m Trial 24 finished with value: 0.06809320457883783 and parameters: {'observation_period_num': 40, 'train_rates': 0.9365325703687593, 'learning_rate': 0.0007095843229904709, 'batch_size': 46, 'step_size': 5, 'gamma': 0.7896389877154872}. Best is trial 24 with value: 0.06809320457883783.[0m
[32m[I 2025-02-06 08:46:50,819][0m Trial 25 finished with value: 0.06354509809613228 and parameters: {'observation_period_num': 37, 'train_rates': 0.9574395076338553, 'learning_rate': 0.0007902610319893282, 'batch_size': 42, 'step_size': 4, 'gamma': 0.8924678284132803}. Best is trial 25 with value: 0.06354509809613228.[0m
[32m[I 2025-02-06 08:49:18,327][0m Trial 26 finished with value: 0.061542030073852345 and parameters: {'observation_period_num': 37, 'train_rates': 0.9492261145572768, 'learning_rate': 0.000954265098591911, 'batch_size': 40, 'step_size': 5, 'gamma': 0.8871440427310977}. Best is trial 26 with value: 0.061542030073852345.[0m
[32m[I 2025-02-06 08:51:32,555][0m Trial 27 finished with value: 0.05617921521414572 and parameters: {'observation_period_num': 33, 'train_rates': 0.9542450661601878, 'learning_rate': 0.0009618807455885124, 'batch_size': 44, 'step_size': 4, 'gamma': 0.9081532579158242}. Best is trial 27 with value: 0.05617921521414572.[0m
[32m[I 2025-02-06 08:52:54,774][0m Trial 28 finished with value: 0.04539773785754254 and parameters: {'observation_period_num': 31, 'train_rates': 0.9677605040032935, 'learning_rate': 0.0004817397048563425, 'batch_size': 74, 'step_size': 4, 'gamma': 0.8945937785157013}. Best is trial 28 with value: 0.04539773785754254.[0m
[32m[I 2025-02-06 08:54:13,499][0m Trial 29 finished with value: 0.02828124724328518 and parameters: {'observation_period_num': 5, 'train_rates': 0.9845217270587265, 'learning_rate': 0.00046638797923573706, 'batch_size': 80, 'step_size': 6, 'gamma': 0.8863868985436688}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 08:55:03,048][0m Trial 30 finished with value: 0.03604549542069435 and parameters: {'observation_period_num': 6, 'train_rates': 0.9848613519211953, 'learning_rate': 0.00047726043463954394, 'batch_size': 127, 'step_size': 6, 'gamma': 0.9331584547928339}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 08:55:51,039][0m Trial 31 finished with value: 0.03655773773789406 and parameters: {'observation_period_num': 5, 'train_rates': 0.9790771172532149, 'learning_rate': 0.00041500537667721647, 'batch_size': 137, 'step_size': 7, 'gamma': 0.937177843372806}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 08:56:40,289][0m Trial 32 finished with value: 0.037121377885341644 and parameters: {'observation_period_num': 5, 'train_rates': 0.9822642735520669, 'learning_rate': 0.0004906589988547369, 'batch_size': 129, 'step_size': 7, 'gamma': 0.9397214410198367}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 08:57:28,010][0m Trial 33 finished with value: 0.044463083148002625 and parameters: {'observation_period_num': 6, 'train_rates': 0.9882852403584887, 'learning_rate': 0.000349867207716723, 'batch_size': 133, 'step_size': 7, 'gamma': 0.938745235776587}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 08:58:12,265][0m Trial 34 finished with value: 0.040850030817091466 and parameters: {'observation_period_num': 6, 'train_rates': 0.9254399953476614, 'learning_rate': 0.0005169535090354645, 'batch_size': 143, 'step_size': 7, 'gamma': 0.9474275079327983}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 08:58:53,275][0m Trial 35 finished with value: 0.062456753104925156 and parameters: {'observation_period_num': 22, 'train_rates': 0.9761433727589939, 'learning_rate': 0.000263361656751851, 'batch_size': 155, 'step_size': 9, 'gamma': 0.9679826849283568}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 08:59:45,153][0m Trial 36 finished with value: 0.0955030546344892 and parameters: {'observation_period_num': 47, 'train_rates': 0.932012910337767, 'learning_rate': 0.0005175797996235815, 'batch_size': 118, 'step_size': 6, 'gamma': 0.9402153662640019}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:00:10,309][0m Trial 37 finished with value: 0.20574590258408768 and parameters: {'observation_period_num': 62, 'train_rates': 0.6915138990660242, 'learning_rate': 0.00020189572621208412, 'batch_size': 207, 'step_size': 7, 'gamma': 0.9284253387277077}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:01:03,007][0m Trial 38 finished with value: 0.040124114602804184 and parameters: {'observation_period_num': 23, 'train_rates': 0.9727325873919633, 'learning_rate': 0.0004005372903377393, 'batch_size': 123, 'step_size': 8, 'gamma': 0.8573220794126029}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:02:02,435][0m Trial 39 finished with value: 0.1130249924526727 and parameters: {'observation_period_num': 80, 'train_rates': 0.9161422561450301, 'learning_rate': 0.00022357668448300355, 'batch_size': 98, 'step_size': 9, 'gamma': 0.9627080547692781}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:02:40,489][0m Trial 40 finished with value: 0.07388241075272306 and parameters: {'observation_period_num': 51, 'train_rates': 0.8776574113829688, 'learning_rate': 0.0006821735946303835, 'batch_size': 157, 'step_size': 6, 'gamma': 0.9839110189994732}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:03:28,937][0m Trial 41 finished with value: 0.03392244130373001 and parameters: {'observation_period_num': 22, 'train_rates': 0.9721738395507714, 'learning_rate': 0.0005135288529764302, 'batch_size': 131, 'step_size': 8, 'gamma': 0.8596395583409987}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:04:15,854][0m Trial 42 finished with value: 0.034617372842013154 and parameters: {'observation_period_num': 9, 'train_rates': 0.9412196468338233, 'learning_rate': 0.0005838771415211391, 'batch_size': 134, 'step_size': 8, 'gamma': 0.8756610859399477}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:04:55,538][0m Trial 43 finished with value: 0.04157430226721707 and parameters: {'observation_period_num': 25, 'train_rates': 0.9425503464892063, 'learning_rate': 0.0006325040080278714, 'batch_size': 159, 'step_size': 8, 'gamma': 0.8621261161280941}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:06:05,881][0m Trial 44 finished with value: 0.03637422769538407 and parameters: {'observation_period_num': 16, 'train_rates': 0.9651489829644757, 'learning_rate': 0.0003499101258943439, 'batch_size': 86, 'step_size': 8, 'gamma': 0.875684928592746}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:07:16,725][0m Trial 45 finished with value: 0.045213878568675786 and parameters: {'observation_period_num': 19, 'train_rates': 0.963533446583388, 'learning_rate': 0.0003250196969494408, 'batch_size': 87, 'step_size': 10, 'gamma': 0.8768977668911151}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:08:48,079][0m Trial 46 finished with value: 0.06808154064584786 and parameters: {'observation_period_num': 75, 'train_rates': 0.926224452763093, 'learning_rate': 0.00015589884708772705, 'batch_size': 63, 'step_size': 8, 'gamma': 0.854175602165313}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:09:58,399][0m Trial 47 finished with value: 0.056336793557300674 and parameters: {'observation_period_num': 46, 'train_rates': 0.9061859578264008, 'learning_rate': 0.00010631019147914287, 'batch_size': 81, 'step_size': 9, 'gamma': 0.8723257402543585}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:10:54,940][0m Trial 48 finished with value: 0.06032296695129972 and parameters: {'observation_period_num': 55, 'train_rates': 0.9511143040682455, 'learning_rate': 0.0003087457256434355, 'batch_size': 109, 'step_size': 6, 'gamma': 0.840054324550715}. Best is trial 29 with value: 0.02828124724328518.[0m
[32m[I 2025-02-06 09:11:58,254][0m Trial 49 finished with value: 0.03715304435732273 and parameters: {'observation_period_num': 15, 'train_rates': 0.9647890686020606, 'learning_rate': 0.00024272981226905962, 'batch_size': 98, 'step_size': 11, 'gamma': 0.8819154775322908}. Best is trial 29 with value: 0.02828124724328518.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 6, 'train_rates': 0.9321561644365255, 'learning_rate': 4.668291486710994e-05, 'batch_size': 31, 'step_size': 14, 'gamma': 0.9299588678272361}
Epoch 1/300, trend Loss: 0.4255 | 0.2813
Epoch 2/300, trend Loss: 0.1518 | 0.1720
Epoch 3/300, trend Loss: 0.1309 | 0.1415
Epoch 4/300, trend Loss: 0.1219 | 0.1182
Epoch 5/300, trend Loss: 0.1153 | 0.1015
Epoch 6/300, trend Loss: 0.1106 | 0.0908
Epoch 7/300, trend Loss: 0.1073 | 0.0834
Epoch 8/300, trend Loss: 0.1049 | 0.0785
Epoch 9/300, trend Loss: 0.1033 | 0.0752
Epoch 10/300, trend Loss: 0.1018 | 0.0729
Epoch 11/300, trend Loss: 0.1004 | 0.0710
Epoch 12/300, trend Loss: 0.0992 | 0.0695
Epoch 13/300, trend Loss: 0.0980 | 0.0680
Epoch 14/300, trend Loss: 0.0967 | 0.0666
Epoch 15/300, trend Loss: 0.0954 | 0.0648
Epoch 16/300, trend Loss: 0.0941 | 0.0634
Epoch 17/300, trend Loss: 0.0926 | 0.0619
Epoch 18/300, trend Loss: 0.0910 | 0.0602
Epoch 19/300, trend Loss: 0.0894 | 0.0584
Epoch 20/300, trend Loss: 0.0877 | 0.0565
Epoch 21/300, trend Loss: 0.0859 | 0.0544
Epoch 22/300, trend Loss: 0.0840 | 0.0523
Epoch 23/300, trend Loss: 0.0826 | 0.0506
Epoch 24/300, trend Loss: 0.0814 | 0.0491
Epoch 25/300, trend Loss: 0.0805 | 0.0479
Epoch 26/300, trend Loss: 0.0797 | 0.0468
Epoch 27/300, trend Loss: 0.0790 | 0.0459
Epoch 28/300, trend Loss: 0.0784 | 0.0450
Epoch 29/300, trend Loss: 0.0777 | 0.0440
Epoch 30/300, trend Loss: 0.0772 | 0.0434
Epoch 31/300, trend Loss: 0.0767 | 0.0427
Epoch 32/300, trend Loss: 0.0762 | 0.0422
Epoch 33/300, trend Loss: 0.0757 | 0.0416
Epoch 34/300, trend Loss: 0.0752 | 0.0411
Epoch 35/300, trend Loss: 0.0747 | 0.0406
Epoch 36/300, trend Loss: 0.0742 | 0.0401
Epoch 37/300, trend Loss: 0.0738 | 0.0398
Epoch 38/300, trend Loss: 0.0734 | 0.0394
Epoch 39/300, trend Loss: 0.0731 | 0.0391
Epoch 40/300, trend Loss: 0.0727 | 0.0387
Epoch 41/300, trend Loss: 0.0723 | 0.0384
Epoch 42/300, trend Loss: 0.0719 | 0.0381
Epoch 43/300, trend Loss: 0.0715 | 0.0377
Epoch 44/300, trend Loss: 0.0711 | 0.0374
Epoch 45/300, trend Loss: 0.0707 | 0.0371
Epoch 46/300, trend Loss: 0.0703 | 0.0368
Epoch 47/300, trend Loss: 0.0700 | 0.0365
Epoch 48/300, trend Loss: 0.0697 | 0.0363
Epoch 49/300, trend Loss: 0.0693 | 0.0361
Epoch 50/300, trend Loss: 0.0691 | 0.0361
Epoch 51/300, trend Loss: 0.0688 | 0.0362
Epoch 52/300, trend Loss: 0.0685 | 0.0361
Epoch 53/300, trend Loss: 0.0682 | 0.0360
Epoch 54/300, trend Loss: 0.0680 | 0.0359
Epoch 55/300, trend Loss: 0.0677 | 0.0358
Epoch 56/300, trend Loss: 0.0675 | 0.0356
Epoch 57/300, trend Loss: 0.0673 | 0.0357
Epoch 58/300, trend Loss: 0.0671 | 0.0358
Epoch 59/300, trend Loss: 0.0669 | 0.0356
Epoch 60/300, trend Loss: 0.0667 | 0.0355
Epoch 61/300, trend Loss: 0.0665 | 0.0354
Epoch 62/300, trend Loss: 0.0663 | 0.0353
Epoch 63/300, trend Loss: 0.0661 | 0.0351
Epoch 64/300, trend Loss: 0.0659 | 0.0350
Epoch 65/300, trend Loss: 0.0657 | 0.0351
Epoch 66/300, trend Loss: 0.0655 | 0.0349
Epoch 67/300, trend Loss: 0.0653 | 0.0346
Epoch 68/300, trend Loss: 0.0651 | 0.0344
Epoch 69/300, trend Loss: 0.0649 | 0.0341
Epoch 70/300, trend Loss: 0.0647 | 0.0338
Epoch 71/300, trend Loss: 0.0645 | 0.0332
Epoch 72/300, trend Loss: 0.0643 | 0.0331
Epoch 73/300, trend Loss: 0.0641 | 0.0328
Epoch 74/300, trend Loss: 0.0639 | 0.0325
Epoch 75/300, trend Loss: 0.0637 | 0.0322
Epoch 76/300, trend Loss: 0.0636 | 0.0320
Epoch 77/300, trend Loss: 0.0634 | 0.0318
Epoch 78/300, trend Loss: 0.0632 | 0.0312
Epoch 79/300, trend Loss: 0.0631 | 0.0311
Epoch 80/300, trend Loss: 0.0629 | 0.0309
Epoch 81/300, trend Loss: 0.0628 | 0.0308
Epoch 82/300, trend Loss: 0.0627 | 0.0306
Epoch 83/300, trend Loss: 0.0625 | 0.0305
Epoch 84/300, trend Loss: 0.0624 | 0.0304
Epoch 85/300, trend Loss: 0.0623 | 0.0304
Epoch 86/300, trend Loss: 0.0623 | 0.0305
Epoch 87/300, trend Loss: 0.0622 | 0.0305
Epoch 88/300, trend Loss: 0.0620 | 0.0305
Epoch 89/300, trend Loss: 0.0619 | 0.0304
Epoch 90/300, trend Loss: 0.0618 | 0.0303
Epoch 91/300, trend Loss: 0.0617 | 0.0302
Epoch 92/300, trend Loss: 0.0616 | 0.0297
Epoch 93/300, trend Loss: 0.0615 | 0.0297
Epoch 94/300, trend Loss: 0.0614 | 0.0297
Epoch 95/300, trend Loss: 0.0612 | 0.0296
Epoch 96/300, trend Loss: 0.0611 | 0.0295
Epoch 97/300, trend Loss: 0.0610 | 0.0294
Epoch 98/300, trend Loss: 0.0610 | 0.0293
Epoch 99/300, trend Loss: 0.0608 | 0.0288
Epoch 100/300, trend Loss: 0.0608 | 0.0288
Epoch 101/300, trend Loss: 0.0607 | 0.0288
Epoch 102/300, trend Loss: 0.0606 | 0.0287
Epoch 103/300, trend Loss: 0.0605 | 0.0287
Epoch 104/300, trend Loss: 0.0604 | 0.0286
Epoch 105/300, trend Loss: 0.0604 | 0.0285
Epoch 106/300, trend Loss: 0.0603 | 0.0282
Epoch 107/300, trend Loss: 0.0602 | 0.0281
Epoch 108/300, trend Loss: 0.0601 | 0.0281
Epoch 109/300, trend Loss: 0.0601 | 0.0281
Epoch 110/300, trend Loss: 0.0600 | 0.0280
Epoch 111/300, trend Loss: 0.0599 | 0.0280
Epoch 112/300, trend Loss: 0.0598 | 0.0279
Epoch 113/300, trend Loss: 0.0598 | 0.0277
Epoch 114/300, trend Loss: 0.0597 | 0.0276
Epoch 115/300, trend Loss: 0.0596 | 0.0276
Epoch 116/300, trend Loss: 0.0596 | 0.0276
Epoch 117/300, trend Loss: 0.0595 | 0.0276
Epoch 118/300, trend Loss: 0.0594 | 0.0275
Epoch 119/300, trend Loss: 0.0594 | 0.0275
Epoch 120/300, trend Loss: 0.0593 | 0.0273
Epoch 121/300, trend Loss: 0.0592 | 0.0272
Epoch 122/300, trend Loss: 0.0592 | 0.0272
Epoch 123/300, trend Loss: 0.0591 | 0.0272
Epoch 124/300, trend Loss: 0.0590 | 0.0272
Epoch 125/300, trend Loss: 0.0590 | 0.0271
Epoch 126/300, trend Loss: 0.0589 | 0.0271
Epoch 127/300, trend Loss: 0.0589 | 0.0269
Epoch 128/300, trend Loss: 0.0588 | 0.0269
Epoch 129/300, trend Loss: 0.0587 | 0.0269
Epoch 130/300, trend Loss: 0.0587 | 0.0269
Epoch 131/300, trend Loss: 0.0586 | 0.0268
Epoch 132/300, trend Loss: 0.0586 | 0.0268
Epoch 133/300, trend Loss: 0.0585 | 0.0268
Epoch 134/300, trend Loss: 0.0585 | 0.0267
Epoch 135/300, trend Loss: 0.0584 | 0.0267
Epoch 136/300, trend Loss: 0.0583 | 0.0266
Epoch 137/300, trend Loss: 0.0583 | 0.0266
Epoch 138/300, trend Loss: 0.0582 | 0.0266
Epoch 139/300, trend Loss: 0.0582 | 0.0265
Epoch 140/300, trend Loss: 0.0581 | 0.0265
Epoch 141/300, trend Loss: 0.0581 | 0.0264
Epoch 142/300, trend Loss: 0.0580 | 0.0264
Epoch 143/300, trend Loss: 0.0580 | 0.0264
Epoch 144/300, trend Loss: 0.0580 | 0.0264
Epoch 145/300, trend Loss: 0.0579 | 0.0263
Epoch 146/300, trend Loss: 0.0579 | 0.0263
Epoch 147/300, trend Loss: 0.0578 | 0.0263
Epoch 148/300, trend Loss: 0.0578 | 0.0263
Epoch 149/300, trend Loss: 0.0577 | 0.0263
Epoch 150/300, trend Loss: 0.0577 | 0.0263
Epoch 151/300, trend Loss: 0.0576 | 0.0263
Epoch 152/300, trend Loss: 0.0576 | 0.0262
Epoch 153/300, trend Loss: 0.0575 | 0.0262
Epoch 154/300, trend Loss: 0.0575 | 0.0262
Epoch 155/300, trend Loss: 0.0574 | 0.0262
Epoch 156/300, trend Loss: 0.0574 | 0.0262
Epoch 157/300, trend Loss: 0.0573 | 0.0262
Epoch 158/300, trend Loss: 0.0573 | 0.0261
Epoch 159/300, trend Loss: 0.0573 | 0.0261
Epoch 160/300, trend Loss: 0.0572 | 0.0261
Epoch 161/300, trend Loss: 0.0572 | 0.0260
Epoch 162/300, trend Loss: 0.0571 | 0.0260
Epoch 163/300, trend Loss: 0.0571 | 0.0260
Epoch 164/300, trend Loss: 0.0570 | 0.0260
Epoch 165/300, trend Loss: 0.0570 | 0.0259
Epoch 166/300, trend Loss: 0.0570 | 0.0259
Epoch 167/300, trend Loss: 0.0569 | 0.0259
Epoch 168/300, trend Loss: 0.0569 | 0.0258
Epoch 169/300, trend Loss: 0.0568 | 0.0258
Epoch 170/300, trend Loss: 0.0568 | 0.0257
Epoch 171/300, trend Loss: 0.0568 | 0.0257
Epoch 172/300, trend Loss: 0.0567 | 0.0257
Epoch 173/300, trend Loss: 0.0567 | 0.0256
Epoch 174/300, trend Loss: 0.0567 | 0.0256
Epoch 175/300, trend Loss: 0.0566 | 0.0256
Epoch 176/300, trend Loss: 0.0566 | 0.0255
Epoch 177/300, trend Loss: 0.0566 | 0.0255
Epoch 178/300, trend Loss: 0.0565 | 0.0254
Epoch 179/300, trend Loss: 0.0565 | 0.0254
Epoch 180/300, trend Loss: 0.0565 | 0.0254
Epoch 181/300, trend Loss: 0.0564 | 0.0253
Epoch 182/300, trend Loss: 0.0564 | 0.0253
Epoch 183/300, trend Loss: 0.0564 | 0.0253
Epoch 184/300, trend Loss: 0.0563 | 0.0253
Epoch 185/300, trend Loss: 0.0563 | 0.0253
Epoch 186/300, trend Loss: 0.0563 | 0.0253
Epoch 187/300, trend Loss: 0.0562 | 0.0252
Epoch 188/300, trend Loss: 0.0562 | 0.0252
Epoch 189/300, trend Loss: 0.0562 | 0.0252
Epoch 190/300, trend Loss: 0.0561 | 0.0253
Epoch 191/300, trend Loss: 0.0561 | 0.0252
Epoch 192/300, trend Loss: 0.0561 | 0.0252
Epoch 193/300, trend Loss: 0.0560 | 0.0252
Epoch 194/300, trend Loss: 0.0560 | 0.0252
Epoch 195/300, trend Loss: 0.0560 | 0.0252
Epoch 196/300, trend Loss: 0.0560 | 0.0252
Epoch 197/300, trend Loss: 0.0559 | 0.0253
Epoch 198/300, trend Loss: 0.0559 | 0.0252
Epoch 199/300, trend Loss: 0.0559 | 0.0252
Epoch 200/300, trend Loss: 0.0559 | 0.0252
Epoch 201/300, trend Loss: 0.0558 | 0.0252
Epoch 202/300, trend Loss: 0.0558 | 0.0252
Epoch 203/300, trend Loss: 0.0558 | 0.0251
Epoch 204/300, trend Loss: 0.0558 | 0.0251
Epoch 205/300, trend Loss: 0.0558 | 0.0251
Epoch 206/300, trend Loss: 0.0558 | 0.0251
Epoch 207/300, trend Loss: 0.0557 | 0.0250
Epoch 208/300, trend Loss: 0.0557 | 0.0250
Epoch 209/300, trend Loss: 0.0557 | 0.0249
Epoch 210/300, trend Loss: 0.0557 | 0.0249
Epoch 211/300, trend Loss: 0.0557 | 0.0248
Epoch 212/300, trend Loss: 0.0556 | 0.0248
Epoch 213/300, trend Loss: 0.0556 | 0.0248
Epoch 214/300, trend Loss: 0.0556 | 0.0248
Epoch 215/300, trend Loss: 0.0555 | 0.0248
Epoch 216/300, trend Loss: 0.0555 | 0.0248
Epoch 217/300, trend Loss: 0.0555 | 0.0248
Epoch 218/300, trend Loss: 0.0554 | 0.0247
Epoch 219/300, trend Loss: 0.0554 | 0.0247
Epoch 220/300, trend Loss: 0.0554 | 0.0247
Epoch 221/300, trend Loss: 0.0553 | 0.0247
Epoch 222/300, trend Loss: 0.0553 | 0.0247
Epoch 223/300, trend Loss: 0.0553 | 0.0247
Epoch 224/300, trend Loss: 0.0553 | 0.0247
Epoch 225/300, trend Loss: 0.0552 | 0.0246
Epoch 226/300, trend Loss: 0.0552 | 0.0246
Epoch 227/300, trend Loss: 0.0552 | 0.0246
Epoch 228/300, trend Loss: 0.0552 | 0.0246
Epoch 229/300, trend Loss: 0.0552 | 0.0246
Epoch 230/300, trend Loss: 0.0551 | 0.0246
Epoch 231/300, trend Loss: 0.0551 | 0.0246
Epoch 232/300, trend Loss: 0.0551 | 0.0246
Epoch 233/300, trend Loss: 0.0551 | 0.0246
Epoch 234/300, trend Loss: 0.0550 | 0.0245
Epoch 235/300, trend Loss: 0.0550 | 0.0245
Epoch 236/300, trend Loss: 0.0550 | 0.0245
Epoch 237/300, trend Loss: 0.0550 | 0.0245
Epoch 238/300, trend Loss: 0.0550 | 0.0245
Epoch 239/300, trend Loss: 0.0549 | 0.0245
Epoch 240/300, trend Loss: 0.0549 | 0.0245
Epoch 241/300, trend Loss: 0.0549 | 0.0245
Epoch 242/300, trend Loss: 0.0549 | 0.0245
Epoch 243/300, trend Loss: 0.0549 | 0.0245
Epoch 244/300, trend Loss: 0.0548 | 0.0244
Epoch 245/300, trend Loss: 0.0548 | 0.0244
Epoch 246/300, trend Loss: 0.0548 | 0.0245
Epoch 247/300, trend Loss: 0.0548 | 0.0245
Epoch 248/300, trend Loss: 0.0548 | 0.0245
Epoch 249/300, trend Loss: 0.0547 | 0.0245
Epoch 250/300, trend Loss: 0.0547 | 0.0244
Epoch 251/300, trend Loss: 0.0547 | 0.0244
Epoch 252/300, trend Loss: 0.0547 | 0.0244
Epoch 253/300, trend Loss: 0.0547 | 0.0245
Epoch 254/300, trend Loss: 0.0546 | 0.0245
Epoch 255/300, trend Loss: 0.0546 | 0.0245
Epoch 256/300, trend Loss: 0.0546 | 0.0245
Epoch 257/300, trend Loss: 0.0546 | 0.0244
Epoch 258/300, trend Loss: 0.0546 | 0.0244
Epoch 259/300, trend Loss: 0.0545 | 0.0244
Epoch 260/300, trend Loss: 0.0545 | 0.0245
Epoch 261/300, trend Loss: 0.0545 | 0.0245
Epoch 262/300, trend Loss: 0.0545 | 0.0245
Epoch 263/300, trend Loss: 0.0545 | 0.0245
Epoch 264/300, trend Loss: 0.0545 | 0.0245
Epoch 265/300, trend Loss: 0.0544 | 0.0245
Epoch 266/300, trend Loss: 0.0544 | 0.0244
Epoch 267/300, trend Loss: 0.0544 | 0.0244
Epoch 268/300, trend Loss: 0.0544 | 0.0244
Epoch 269/300, trend Loss: 0.0544 | 0.0244
Epoch 270/300, trend Loss: 0.0544 | 0.0244
Epoch 271/300, trend Loss: 0.0543 | 0.0244
Epoch 272/300, trend Loss: 0.0543 | 0.0244
Epoch 273/300, trend Loss: 0.0543 | 0.0244
Epoch 274/300, trend Loss: 0.0543 | 0.0244
Epoch 275/300, trend Loss: 0.0543 | 0.0244
Epoch 276/300, trend Loss: 0.0543 | 0.0244
Epoch 277/300, trend Loss: 0.0543 | 0.0244
Epoch 278/300, trend Loss: 0.0543 | 0.0244
Epoch 279/300, trend Loss: 0.0542 | 0.0244
Epoch 280/300, trend Loss: 0.0542 | 0.0244
Epoch 281/300, trend Loss: 0.0542 | 0.0242
Epoch 282/300, trend Loss: 0.0542 | 0.0242
Epoch 283/300, trend Loss: 0.0542 | 0.0242
Epoch 284/300, trend Loss: 0.0542 | 0.0242
Epoch 285/300, trend Loss: 0.0542 | 0.0242
Epoch 286/300, trend Loss: 0.0542 | 0.0242
Epoch 287/300, trend Loss: 0.0541 | 0.0242
Epoch 288/300, trend Loss: 0.0541 | 0.0241
Epoch 289/300, trend Loss: 0.0541 | 0.0241
Epoch 290/300, trend Loss: 0.0541 | 0.0240
Epoch 291/300, trend Loss: 0.0541 | 0.0240
Epoch 292/300, trend Loss: 0.0541 | 0.0240
Epoch 293/300, trend Loss: 0.0541 | 0.0240
Epoch 294/300, trend Loss: 0.0540 | 0.0240
Epoch 295/300, trend Loss: 0.0540 | 0.0239
Epoch 296/300, trend Loss: 0.0540 | 0.0239
Epoch 297/300, trend Loss: 0.0540 | 0.0239
Epoch 298/300, trend Loss: 0.0540 | 0.0239
Epoch 299/300, trend Loss: 0.0540 | 0.0239
Epoch 300/300, trend Loss: 0.0540 | 0.0239
Training seasonal_0 component with params: {'observation_period_num': 53, 'train_rates': 0.9851264067806168, 'learning_rate': 0.00018701567512985115, 'batch_size': 47, 'step_size': 14, 'gamma': 0.8825802598972957}
Epoch 1/300, seasonal_0 Loss: 0.2652 | 0.1986
Epoch 2/300, seasonal_0 Loss: 0.1670 | 0.1640
Epoch 3/300, seasonal_0 Loss: 0.1447 | 0.1252
Epoch 4/300, seasonal_0 Loss: 0.1253 | 0.1272
Epoch 5/300, seasonal_0 Loss: 0.1239 | 0.1121
Epoch 6/300, seasonal_0 Loss: 0.1185 | 0.1042
Epoch 7/300, seasonal_0 Loss: 0.1096 | 0.0964
Epoch 8/300, seasonal_0 Loss: 0.1028 | 0.0868
Epoch 9/300, seasonal_0 Loss: 0.0977 | 0.0878
Epoch 10/300, seasonal_0 Loss: 0.0935 | 0.0862
Epoch 11/300, seasonal_0 Loss: 0.0902 | 0.0847
Epoch 12/300, seasonal_0 Loss: 0.0873 | 0.0833
Epoch 13/300, seasonal_0 Loss: 0.0849 | 0.0821
Epoch 14/300, seasonal_0 Loss: 0.0829 | 0.0804
Epoch 15/300, seasonal_0 Loss: 0.0813 | 0.0748
Epoch 16/300, seasonal_0 Loss: 0.0796 | 0.0787
Epoch 17/300, seasonal_0 Loss: 0.0777 | 0.0793
Epoch 18/300, seasonal_0 Loss: 0.0766 | 0.0818
Epoch 19/300, seasonal_0 Loss: 0.0756 | 0.0806
Epoch 20/300, seasonal_0 Loss: 0.0748 | 0.0811
Epoch 21/300, seasonal_0 Loss: 0.0741 | 0.0830
Epoch 22/300, seasonal_0 Loss: 0.0737 | 0.0869
Epoch 23/300, seasonal_0 Loss: 0.0746 | 0.0959
Epoch 24/300, seasonal_0 Loss: 0.0760 | 0.0825
Epoch 25/300, seasonal_0 Loss: 0.0771 | 0.0775
Epoch 26/300, seasonal_0 Loss: 0.0776 | 0.0750
Epoch 27/300, seasonal_0 Loss: 0.0787 | 0.1014
Epoch 28/300, seasonal_0 Loss: 0.0793 | 0.0720
Epoch 29/300, seasonal_0 Loss: 0.0825 | 0.0673
Epoch 30/300, seasonal_0 Loss: 0.0801 | 0.0645
Epoch 31/300, seasonal_0 Loss: 0.0812 | 0.1746
Epoch 32/300, seasonal_0 Loss: 0.0813 | 0.0613
Epoch 33/300, seasonal_0 Loss: 0.0770 | 0.0713
Epoch 34/300, seasonal_0 Loss: 0.0749 | 0.0674
Epoch 35/300, seasonal_0 Loss: 0.0693 | 0.0697
Epoch 36/300, seasonal_0 Loss: 0.0672 | 0.0755
Epoch 37/300, seasonal_0 Loss: 0.0662 | 0.0639
Epoch 38/300, seasonal_0 Loss: 0.0676 | 0.0621
Epoch 39/300, seasonal_0 Loss: 0.0681 | 0.0603
Epoch 40/300, seasonal_0 Loss: 0.0673 | 0.0606
Epoch 41/300, seasonal_0 Loss: 0.0672 | 0.0618
Epoch 42/300, seasonal_0 Loss: 0.0666 | 0.0638
Epoch 43/300, seasonal_0 Loss: 0.0662 | 0.0640
Epoch 44/300, seasonal_0 Loss: 0.0661 | 0.0680
Epoch 45/300, seasonal_0 Loss: 0.0670 | 0.0651
Epoch 46/300, seasonal_0 Loss: 0.0660 | 0.0654
Epoch 47/300, seasonal_0 Loss: 0.0660 | 0.0717
Epoch 48/300, seasonal_0 Loss: 0.0653 | 0.0627
Epoch 49/300, seasonal_0 Loss: 0.0669 | 0.0720
Epoch 50/300, seasonal_0 Loss: 0.0676 | 0.0663
Epoch 51/300, seasonal_0 Loss: 0.0711 | 0.0786
Epoch 52/300, seasonal_0 Loss: 0.0684 | 0.0699
Epoch 53/300, seasonal_0 Loss: 0.0697 | 0.0799
Epoch 54/300, seasonal_0 Loss: 0.0718 | 0.0789
Epoch 55/300, seasonal_0 Loss: 0.0741 | 0.0971
Epoch 56/300, seasonal_0 Loss: 0.0642 | 0.0681
Epoch 57/300, seasonal_0 Loss: 0.0674 | 0.0604
Epoch 58/300, seasonal_0 Loss: 0.0623 | 0.0559
Epoch 59/300, seasonal_0 Loss: 0.0605 | 0.0555
Epoch 60/300, seasonal_0 Loss: 0.0609 | 0.0583
Epoch 61/300, seasonal_0 Loss: 0.0611 | 0.0736
Epoch 62/300, seasonal_0 Loss: 0.0621 | 0.1070
Epoch 63/300, seasonal_0 Loss: 0.0629 | 0.1093
Epoch 64/300, seasonal_0 Loss: 0.0626 | 0.0899
Epoch 65/300, seasonal_0 Loss: 0.0617 | 0.0796
Epoch 66/300, seasonal_0 Loss: 0.0609 | 0.0690
Epoch 67/300, seasonal_0 Loss: 0.0598 | 0.0657
Epoch 68/300, seasonal_0 Loss: 0.0599 | 0.0635
Epoch 69/300, seasonal_0 Loss: 0.0606 | 0.0619
Epoch 70/300, seasonal_0 Loss: 0.0593 | 0.0608
Epoch 71/300, seasonal_0 Loss: 0.0569 | 0.0578
Epoch 72/300, seasonal_0 Loss: 0.0555 | 0.0564
Epoch 73/300, seasonal_0 Loss: 0.0546 | 0.0554
Epoch 74/300, seasonal_0 Loss: 0.0541 | 0.0548
Epoch 75/300, seasonal_0 Loss: 0.0536 | 0.0545
Epoch 76/300, seasonal_0 Loss: 0.0532 | 0.0545
Epoch 77/300, seasonal_0 Loss: 0.0526 | 0.0543
Epoch 78/300, seasonal_0 Loss: 0.0521 | 0.0542
Epoch 79/300, seasonal_0 Loss: 0.0517 | 0.0533
Epoch 80/300, seasonal_0 Loss: 0.0513 | 0.0526
Epoch 81/300, seasonal_0 Loss: 0.0508 | 0.0518
Epoch 82/300, seasonal_0 Loss: 0.0504 | 0.0508
Epoch 83/300, seasonal_0 Loss: 0.0498 | 0.0500
Epoch 84/300, seasonal_0 Loss: 0.0489 | 0.0496
Epoch 85/300, seasonal_0 Loss: 0.0471 | 0.0536
Epoch 86/300, seasonal_0 Loss: 0.0455 | 0.0517
Epoch 87/300, seasonal_0 Loss: 0.0446 | 0.0476
Epoch 88/300, seasonal_0 Loss: 0.0437 | 0.0468
Epoch 89/300, seasonal_0 Loss: 0.0431 | 0.0472
Epoch 90/300, seasonal_0 Loss: 0.0426 | 0.0472
Epoch 91/300, seasonal_0 Loss: 0.0422 | 0.0469
Epoch 92/300, seasonal_0 Loss: 0.0419 | 0.0468
Epoch 93/300, seasonal_0 Loss: 0.0417 | 0.0469
Epoch 94/300, seasonal_0 Loss: 0.0414 | 0.0470
Epoch 95/300, seasonal_0 Loss: 0.0411 | 0.0474
Epoch 96/300, seasonal_0 Loss: 0.0409 | 0.0478
Epoch 97/300, seasonal_0 Loss: 0.0406 | 0.0483
Epoch 98/300, seasonal_0 Loss: 0.0405 | 0.0489
Epoch 99/300, seasonal_0 Loss: 0.0404 | 0.0522
Epoch 100/300, seasonal_0 Loss: 0.0405 | 0.0525
Epoch 101/300, seasonal_0 Loss: 0.0406 | 0.0526
Epoch 102/300, seasonal_0 Loss: 0.0407 | 0.0527
Epoch 103/300, seasonal_0 Loss: 0.0406 | 0.0526
Epoch 104/300, seasonal_0 Loss: 0.0403 | 0.0521
Epoch 105/300, seasonal_0 Loss: 0.0400 | 0.0513
Epoch 106/300, seasonal_0 Loss: 0.0396 | 0.0509
Epoch 107/300, seasonal_0 Loss: 0.0393 | 0.0468
Epoch 108/300, seasonal_0 Loss: 0.0389 | 0.0451
Epoch 109/300, seasonal_0 Loss: 0.0387 | 0.0443
Epoch 110/300, seasonal_0 Loss: 0.0388 | 0.0440
Epoch 111/300, seasonal_0 Loss: 0.0389 | 0.0437
Epoch 112/300, seasonal_0 Loss: 0.0388 | 0.0432
Epoch 113/300, seasonal_0 Loss: 0.0385 | 0.0424
Epoch 114/300, seasonal_0 Loss: 0.0383 | 0.0423
Epoch 115/300, seasonal_0 Loss: 0.0381 | 0.0425
Epoch 116/300, seasonal_0 Loss: 0.0378 | 0.0425
Epoch 117/300, seasonal_0 Loss: 0.0374 | 0.0426
Epoch 118/300, seasonal_0 Loss: 0.0372 | 0.0426
Epoch 119/300, seasonal_0 Loss: 0.0371 | 0.0424
Epoch 120/300, seasonal_0 Loss: 0.0370 | 0.0420
Epoch 121/300, seasonal_0 Loss: 0.0370 | 0.0420
Epoch 122/300, seasonal_0 Loss: 0.0370 | 0.0422
Epoch 123/300, seasonal_0 Loss: 0.0370 | 0.0426
Epoch 124/300, seasonal_0 Loss: 0.0370 | 0.0432
Epoch 125/300, seasonal_0 Loss: 0.0369 | 0.0440
Epoch 126/300, seasonal_0 Loss: 0.0369 | 0.0451
Epoch 127/300, seasonal_0 Loss: 0.0368 | 0.0476
Epoch 128/300, seasonal_0 Loss: 0.0367 | 0.0500
Epoch 129/300, seasonal_0 Loss: 0.0366 | 0.0519
Epoch 130/300, seasonal_0 Loss: 0.0365 | 0.0526
Epoch 131/300, seasonal_0 Loss: 0.0364 | 0.0520
Epoch 132/300, seasonal_0 Loss: 0.0362 | 0.0503
Epoch 133/300, seasonal_0 Loss: 0.0360 | 0.0482
Epoch 134/300, seasonal_0 Loss: 0.0359 | 0.0456
Epoch 135/300, seasonal_0 Loss: 0.0359 | 0.0434
Epoch 136/300, seasonal_0 Loss: 0.0359 | 0.0423
Epoch 137/300, seasonal_0 Loss: 0.0359 | 0.0420
Epoch 138/300, seasonal_0 Loss: 0.0359 | 0.0418
Epoch 139/300, seasonal_0 Loss: 0.0356 | 0.0419
Epoch 140/300, seasonal_0 Loss: 0.0353 | 0.0424
Epoch 141/300, seasonal_0 Loss: 0.0352 | 0.0437
Epoch 142/300, seasonal_0 Loss: 0.0351 | 0.0447
Epoch 143/300, seasonal_0 Loss: 0.0350 | 0.0449
Epoch 144/300, seasonal_0 Loss: 0.0349 | 0.0449
Epoch 145/300, seasonal_0 Loss: 0.0347 | 0.0450
Epoch 146/300, seasonal_0 Loss: 0.0345 | 0.0451
Epoch 147/300, seasonal_0 Loss: 0.0344 | 0.0451
Epoch 148/300, seasonal_0 Loss: 0.0344 | 0.0452
Epoch 149/300, seasonal_0 Loss: 0.0344 | 0.0447
Epoch 150/300, seasonal_0 Loss: 0.0344 | 0.0442
Epoch 151/300, seasonal_0 Loss: 0.0343 | 0.0437
Epoch 152/300, seasonal_0 Loss: 0.0341 | 0.0433
Epoch 153/300, seasonal_0 Loss: 0.0339 | 0.0430
Epoch 154/300, seasonal_0 Loss: 0.0337 | 0.0428
Epoch 155/300, seasonal_0 Loss: 0.0336 | 0.0426
Epoch 156/300, seasonal_0 Loss: 0.0336 | 0.0428
Epoch 157/300, seasonal_0 Loss: 0.0336 | 0.0430
Epoch 158/300, seasonal_0 Loss: 0.0336 | 0.0432
Epoch 159/300, seasonal_0 Loss: 0.0335 | 0.0432
Epoch 160/300, seasonal_0 Loss: 0.0334 | 0.0432
Epoch 161/300, seasonal_0 Loss: 0.0333 | 0.0433
Epoch 162/300, seasonal_0 Loss: 0.0331 | 0.0435
Epoch 163/300, seasonal_0 Loss: 0.0331 | 0.0437
Epoch 164/300, seasonal_0 Loss: 0.0331 | 0.0440
Epoch 165/300, seasonal_0 Loss: 0.0331 | 0.0443
Epoch 166/300, seasonal_0 Loss: 0.0331 | 0.0444
Epoch 167/300, seasonal_0 Loss: 0.0330 | 0.0445
Epoch 168/300, seasonal_0 Loss: 0.0330 | 0.0445
Epoch 169/300, seasonal_0 Loss: 0.0329 | 0.0446
Epoch 170/300, seasonal_0 Loss: 0.0328 | 0.0441
Epoch 171/300, seasonal_0 Loss: 0.0327 | 0.0436
Epoch 172/300, seasonal_0 Loss: 0.0326 | 0.0433
Epoch 173/300, seasonal_0 Loss: 0.0326 | 0.0431
Epoch 174/300, seasonal_0 Loss: 0.0325 | 0.0430
Epoch 175/300, seasonal_0 Loss: 0.0325 | 0.0429
Epoch 176/300, seasonal_0 Loss: 0.0324 | 0.0428
Epoch 177/300, seasonal_0 Loss: 0.0323 | 0.0428
Epoch 178/300, seasonal_0 Loss: 0.0323 | 0.0430
Epoch 179/300, seasonal_0 Loss: 0.0322 | 0.0432
Epoch 180/300, seasonal_0 Loss: 0.0322 | 0.0434
Epoch 181/300, seasonal_0 Loss: 0.0321 | 0.0436
Epoch 182/300, seasonal_0 Loss: 0.0321 | 0.0438
Epoch 183/300, seasonal_0 Loss: 0.0321 | 0.0441
Epoch 184/300, seasonal_0 Loss: 0.0320 | 0.0442
Epoch 185/300, seasonal_0 Loss: 0.0320 | 0.0441
Epoch 186/300, seasonal_0 Loss: 0.0320 | 0.0440
Epoch 187/300, seasonal_0 Loss: 0.0319 | 0.0439
Epoch 188/300, seasonal_0 Loss: 0.0319 | 0.0437
Epoch 189/300, seasonal_0 Loss: 0.0318 | 0.0436
Epoch 190/300, seasonal_0 Loss: 0.0317 | 0.0434
Epoch 191/300, seasonal_0 Loss: 0.0317 | 0.0433
Epoch 192/300, seasonal_0 Loss: 0.0317 | 0.0432
Epoch 193/300, seasonal_0 Loss: 0.0316 | 0.0432
Epoch 194/300, seasonal_0 Loss: 0.0316 | 0.0432
Epoch 195/300, seasonal_0 Loss: 0.0316 | 0.0433
Epoch 196/300, seasonal_0 Loss: 0.0315 | 0.0433
Epoch 197/300, seasonal_0 Loss: 0.0315 | 0.0434
Epoch 198/300, seasonal_0 Loss: 0.0314 | 0.0435
Epoch 199/300, seasonal_0 Loss: 0.0314 | 0.0437
Epoch 200/300, seasonal_0 Loss: 0.0314 | 0.0438
Epoch 201/300, seasonal_0 Loss: 0.0314 | 0.0438
Epoch 202/300, seasonal_0 Loss: 0.0313 | 0.0438
Epoch 203/300, seasonal_0 Loss: 0.0313 | 0.0438
Epoch 204/300, seasonal_0 Loss: 0.0313 | 0.0438
Epoch 205/300, seasonal_0 Loss: 0.0312 | 0.0437
Epoch 206/300, seasonal_0 Loss: 0.0312 | 0.0436
Epoch 207/300, seasonal_0 Loss: 0.0312 | 0.0435
Epoch 208/300, seasonal_0 Loss: 0.0311 | 0.0434
Epoch 209/300, seasonal_0 Loss: 0.0311 | 0.0434
Epoch 210/300, seasonal_0 Loss: 0.0311 | 0.0433
Epoch 211/300, seasonal_0 Loss: 0.0310 | 0.0433
Epoch 212/300, seasonal_0 Loss: 0.0310 | 0.0433
Epoch 213/300, seasonal_0 Loss: 0.0310 | 0.0433
Epoch 214/300, seasonal_0 Loss: 0.0310 | 0.0434
Epoch 215/300, seasonal_0 Loss: 0.0309 | 0.0434
Epoch 216/300, seasonal_0 Loss: 0.0309 | 0.0435
Epoch 217/300, seasonal_0 Loss: 0.0309 | 0.0435
Epoch 218/300, seasonal_0 Loss: 0.0309 | 0.0436
Epoch 219/300, seasonal_0 Loss: 0.0308 | 0.0436
Epoch 220/300, seasonal_0 Loss: 0.0308 | 0.0436
Epoch 221/300, seasonal_0 Loss: 0.0308 | 0.0436
Epoch 222/300, seasonal_0 Loss: 0.0308 | 0.0436
Epoch 223/300, seasonal_0 Loss: 0.0307 | 0.0436
Epoch 224/300, seasonal_0 Loss: 0.0307 | 0.0435
Epoch 225/300, seasonal_0 Loss: 0.0307 | 0.0435
Epoch 226/300, seasonal_0 Loss: 0.0307 | 0.0434
Epoch 227/300, seasonal_0 Loss: 0.0307 | 0.0434
Epoch 228/300, seasonal_0 Loss: 0.0306 | 0.0434
Epoch 229/300, seasonal_0 Loss: 0.0306 | 0.0434
Epoch 230/300, seasonal_0 Loss: 0.0306 | 0.0434
Epoch 231/300, seasonal_0 Loss: 0.0306 | 0.0435
Epoch 232/300, seasonal_0 Loss: 0.0305 | 0.0435
Epoch 233/300, seasonal_0 Loss: 0.0305 | 0.0436
Epoch 234/300, seasonal_0 Loss: 0.0305 | 0.0436
Epoch 235/300, seasonal_0 Loss: 0.0305 | 0.0436
Epoch 236/300, seasonal_0 Loss: 0.0305 | 0.0436
Epoch 237/300, seasonal_0 Loss: 0.0305 | 0.0436
Epoch 238/300, seasonal_0 Loss: 0.0304 | 0.0436
Epoch 239/300, seasonal_0 Loss: 0.0304 | 0.0436
Epoch 240/300, seasonal_0 Loss: 0.0304 | 0.0435
Epoch 241/300, seasonal_0 Loss: 0.0304 | 0.0435
Epoch 242/300, seasonal_0 Loss: 0.0304 | 0.0435
Epoch 243/300, seasonal_0 Loss: 0.0304 | 0.0435
Epoch 244/300, seasonal_0 Loss: 0.0303 | 0.0435
Epoch 245/300, seasonal_0 Loss: 0.0303 | 0.0435
Epoch 246/300, seasonal_0 Loss: 0.0303 | 0.0435
Epoch 247/300, seasonal_0 Loss: 0.0303 | 0.0435
Epoch 248/300, seasonal_0 Loss: 0.0303 | 0.0435
Epoch 249/300, seasonal_0 Loss: 0.0303 | 0.0436
Epoch 250/300, seasonal_0 Loss: 0.0302 | 0.0436
Epoch 251/300, seasonal_0 Loss: 0.0302 | 0.0436
Epoch 252/300, seasonal_0 Loss: 0.0302 | 0.0435
Epoch 253/300, seasonal_0 Loss: 0.0302 | 0.0435
Epoch 254/300, seasonal_0 Loss: 0.0302 | 0.0435
Epoch 255/300, seasonal_0 Loss: 0.0302 | 0.0435
Epoch 256/300, seasonal_0 Loss: 0.0302 | 0.0435
Epoch 257/300, seasonal_0 Loss: 0.0301 | 0.0435
Epoch 258/300, seasonal_0 Loss: 0.0301 | 0.0435
Epoch 259/300, seasonal_0 Loss: 0.0301 | 0.0435
Epoch 260/300, seasonal_0 Loss: 0.0301 | 0.0435
Epoch 261/300, seasonal_0 Loss: 0.0301 | 0.0435
Epoch 262/300, seasonal_0 Loss: 0.0301 | 0.0435
Epoch 263/300, seasonal_0 Loss: 0.0301 | 0.0435
Epoch 264/300, seasonal_0 Loss: 0.0301 | 0.0435
Epoch 265/300, seasonal_0 Loss: 0.0301 | 0.0435
Epoch 266/300, seasonal_0 Loss: 0.0300 | 0.0435
Epoch 267/300, seasonal_0 Loss: 0.0300 | 0.0435
Epoch 268/300, seasonal_0 Loss: 0.0300 | 0.0435
Epoch 269/300, seasonal_0 Loss: 0.0300 | 0.0435
Epoch 270/300, seasonal_0 Loss: 0.0300 | 0.0435
Epoch 271/300, seasonal_0 Loss: 0.0300 | 0.0435
Epoch 272/300, seasonal_0 Loss: 0.0300 | 0.0436
Epoch 273/300, seasonal_0 Loss: 0.0300 | 0.0436
Epoch 274/300, seasonal_0 Loss: 0.0300 | 0.0435
Epoch 275/300, seasonal_0 Loss: 0.0299 | 0.0435
Epoch 276/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 277/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 278/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 279/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 280/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 281/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 282/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 283/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 284/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 285/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 286/300, seasonal_0 Loss: 0.0299 | 0.0436
Epoch 287/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 288/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 289/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 290/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 291/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 292/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 293/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 294/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 295/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 296/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 297/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 298/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 299/300, seasonal_0 Loss: 0.0298 | 0.0436
Epoch 300/300, seasonal_0 Loss: 0.0298 | 0.0436
Training seasonal_1 component with params: {'observation_period_num': 26, 'train_rates': 0.8948768225593838, 'learning_rate': 0.00010241472253908938, 'batch_size': 154, 'step_size': 5, 'gamma': 0.9297803582742555}
Epoch 1/300, seasonal_1 Loss: 0.9567 | 0.6614
Epoch 2/300, seasonal_1 Loss: 0.4777 | 0.5620
Epoch 3/300, seasonal_1 Loss: 0.2989 | 0.3641
Epoch 4/300, seasonal_1 Loss: 0.2544 | 0.3452
Epoch 5/300, seasonal_1 Loss: 0.2130 | 0.2766
Epoch 6/300, seasonal_1 Loss: 0.1894 | 0.2681
Epoch 7/300, seasonal_1 Loss: 0.1763 | 0.2397
Epoch 8/300, seasonal_1 Loss: 0.1680 | 0.2336
Epoch 9/300, seasonal_1 Loss: 0.1608 | 0.2115
Epoch 10/300, seasonal_1 Loss: 0.1535 | 0.1962
Epoch 11/300, seasonal_1 Loss: 0.1478 | 0.1874
Epoch 12/300, seasonal_1 Loss: 0.1431 | 0.1766
Epoch 13/300, seasonal_1 Loss: 0.1388 | 0.1658
Epoch 14/300, seasonal_1 Loss: 0.1347 | 0.1555
Epoch 15/300, seasonal_1 Loss: 0.1312 | 0.1462
Epoch 16/300, seasonal_1 Loss: 0.1279 | 0.1368
Epoch 17/300, seasonal_1 Loss: 0.1251 | 0.1295
Epoch 18/300, seasonal_1 Loss: 0.1225 | 0.1234
Epoch 19/300, seasonal_1 Loss: 0.1203 | 0.1175
Epoch 20/300, seasonal_1 Loss: 0.1183 | 0.1121
Epoch 21/300, seasonal_1 Loss: 0.1165 | 0.1066
Epoch 22/300, seasonal_1 Loss: 0.1149 | 0.1020
Epoch 23/300, seasonal_1 Loss: 0.1135 | 0.0979
Epoch 24/300, seasonal_1 Loss: 0.1122 | 0.0946
Epoch 25/300, seasonal_1 Loss: 0.1109 | 0.0924
Epoch 26/300, seasonal_1 Loss: 0.1097 | 0.0911
Epoch 27/300, seasonal_1 Loss: 0.1089 | 0.0905
Epoch 28/300, seasonal_1 Loss: 0.1084 | 0.0899
Epoch 29/300, seasonal_1 Loss: 0.1075 | 0.0894
Epoch 30/300, seasonal_1 Loss: 0.1062 | 0.0870
Epoch 31/300, seasonal_1 Loss: 0.1052 | 0.0839
Epoch 32/300, seasonal_1 Loss: 0.1056 | 0.0817
Epoch 33/300, seasonal_1 Loss: 0.1065 | 0.0827
Epoch 34/300, seasonal_1 Loss: 0.1058 | 0.0813
Epoch 35/300, seasonal_1 Loss: 0.1035 | 0.0787
Epoch 36/300, seasonal_1 Loss: 0.1059 | 0.0884
Epoch 37/300, seasonal_1 Loss: 0.1095 | 0.0925
Epoch 38/300, seasonal_1 Loss: 0.1037 | 0.0811
Epoch 39/300, seasonal_1 Loss: 0.1028 | 0.0761
Epoch 40/300, seasonal_1 Loss: 0.1051 | 0.0782
Epoch 41/300, seasonal_1 Loss: 0.1005 | 0.0747
Epoch 42/300, seasonal_1 Loss: 0.1016 | 0.0792
Epoch 43/300, seasonal_1 Loss: 0.1000 | 0.0742
Epoch 44/300, seasonal_1 Loss: 0.0985 | 0.0722
Epoch 45/300, seasonal_1 Loss: 0.0988 | 0.0718
Epoch 46/300, seasonal_1 Loss: 0.0974 | 0.0718
Epoch 47/300, seasonal_1 Loss: 0.0975 | 0.0719
Epoch 48/300, seasonal_1 Loss: 0.0965 | 0.0702
Epoch 49/300, seasonal_1 Loss: 0.0963 | 0.0697
Epoch 50/300, seasonal_1 Loss: 0.0958 | 0.0693
Epoch 51/300, seasonal_1 Loss: 0.0955 | 0.0693
Epoch 52/300, seasonal_1 Loss: 0.0951 | 0.0686
Epoch 53/300, seasonal_1 Loss: 0.0948 | 0.0681
Epoch 54/300, seasonal_1 Loss: 0.0945 | 0.0678
Epoch 55/300, seasonal_1 Loss: 0.0942 | 0.0676
Epoch 56/300, seasonal_1 Loss: 0.0939 | 0.0672
Epoch 57/300, seasonal_1 Loss: 0.0936 | 0.0668
Epoch 58/300, seasonal_1 Loss: 0.0934 | 0.0665
Epoch 59/300, seasonal_1 Loss: 0.0931 | 0.0663
Epoch 60/300, seasonal_1 Loss: 0.0929 | 0.0660
Epoch 61/300, seasonal_1 Loss: 0.0926 | 0.0657
Epoch 62/300, seasonal_1 Loss: 0.0924 | 0.0655
Epoch 63/300, seasonal_1 Loss: 0.0922 | 0.0652
Epoch 64/300, seasonal_1 Loss: 0.0920 | 0.0650
Epoch 65/300, seasonal_1 Loss: 0.0918 | 0.0648
Epoch 66/300, seasonal_1 Loss: 0.0916 | 0.0646
Epoch 67/300, seasonal_1 Loss: 0.0914 | 0.0644
Epoch 68/300, seasonal_1 Loss: 0.0912 | 0.0642
Epoch 69/300, seasonal_1 Loss: 0.0910 | 0.0640
Epoch 70/300, seasonal_1 Loss: 0.0908 | 0.0638
Epoch 71/300, seasonal_1 Loss: 0.0907 | 0.0636
Epoch 72/300, seasonal_1 Loss: 0.0905 | 0.0634
Epoch 73/300, seasonal_1 Loss: 0.0903 | 0.0632
Epoch 74/300, seasonal_1 Loss: 0.0902 | 0.0631
Epoch 75/300, seasonal_1 Loss: 0.0900 | 0.0629
Epoch 76/300, seasonal_1 Loss: 0.0899 | 0.0628
Epoch 77/300, seasonal_1 Loss: 0.0897 | 0.0626
Epoch 78/300, seasonal_1 Loss: 0.0896 | 0.0625
Epoch 79/300, seasonal_1 Loss: 0.0895 | 0.0623
Epoch 80/300, seasonal_1 Loss: 0.0893 | 0.0622
Epoch 81/300, seasonal_1 Loss: 0.0892 | 0.0621
Epoch 82/300, seasonal_1 Loss: 0.0891 | 0.0619
Epoch 83/300, seasonal_1 Loss: 0.0890 | 0.0618
Epoch 84/300, seasonal_1 Loss: 0.0889 | 0.0617
Epoch 85/300, seasonal_1 Loss: 0.0887 | 0.0616
Epoch 86/300, seasonal_1 Loss: 0.0886 | 0.0615
Epoch 87/300, seasonal_1 Loss: 0.0885 | 0.0614
Epoch 88/300, seasonal_1 Loss: 0.0884 | 0.0612
Epoch 89/300, seasonal_1 Loss: 0.0883 | 0.0611
Epoch 90/300, seasonal_1 Loss: 0.0882 | 0.0610
Epoch 91/300, seasonal_1 Loss: 0.0881 | 0.0609
Epoch 92/300, seasonal_1 Loss: 0.0880 | 0.0609
Epoch 93/300, seasonal_1 Loss: 0.0880 | 0.0608
Epoch 94/300, seasonal_1 Loss: 0.0879 | 0.0607
Epoch 95/300, seasonal_1 Loss: 0.0878 | 0.0606
Epoch 96/300, seasonal_1 Loss: 0.0877 | 0.0605
Epoch 97/300, seasonal_1 Loss: 0.0876 | 0.0604
Epoch 98/300, seasonal_1 Loss: 0.0875 | 0.0603
Epoch 99/300, seasonal_1 Loss: 0.0875 | 0.0603
Epoch 100/300, seasonal_1 Loss: 0.0874 | 0.0602
Epoch 101/300, seasonal_1 Loss: 0.0873 | 0.0601
Epoch 102/300, seasonal_1 Loss: 0.0873 | 0.0600
Epoch 103/300, seasonal_1 Loss: 0.0872 | 0.0600
Epoch 104/300, seasonal_1 Loss: 0.0871 | 0.0599
Epoch 105/300, seasonal_1 Loss: 0.0871 | 0.0598
Epoch 106/300, seasonal_1 Loss: 0.0870 | 0.0597
Epoch 107/300, seasonal_1 Loss: 0.0870 | 0.0597
Epoch 108/300, seasonal_1 Loss: 0.0869 | 0.0596
Epoch 109/300, seasonal_1 Loss: 0.0869 | 0.0597
Epoch 110/300, seasonal_1 Loss: 0.0868 | 0.0595
Epoch 111/300, seasonal_1 Loss: 0.0868 | 0.0596
Epoch 112/300, seasonal_1 Loss: 0.0867 | 0.0593
Epoch 113/300, seasonal_1 Loss: 0.0867 | 0.0595
Epoch 114/300, seasonal_1 Loss: 0.0866 | 0.0592
Epoch 115/300, seasonal_1 Loss: 0.0866 | 0.0594
Epoch 116/300, seasonal_1 Loss: 0.0865 | 0.0591
Epoch 117/300, seasonal_1 Loss: 0.0865 | 0.0593
Epoch 118/300, seasonal_1 Loss: 0.0864 | 0.0590
Epoch 119/300, seasonal_1 Loss: 0.0864 | 0.0592
Epoch 120/300, seasonal_1 Loss: 0.0863 | 0.0589
Epoch 121/300, seasonal_1 Loss: 0.0863 | 0.0591
Epoch 122/300, seasonal_1 Loss: 0.0862 | 0.0589
Epoch 123/300, seasonal_1 Loss: 0.0862 | 0.0590
Epoch 124/300, seasonal_1 Loss: 0.0862 | 0.0588
Epoch 125/300, seasonal_1 Loss: 0.0861 | 0.0588
Epoch 126/300, seasonal_1 Loss: 0.0861 | 0.0588
Epoch 127/300, seasonal_1 Loss: 0.0861 | 0.0588
Epoch 128/300, seasonal_1 Loss: 0.0860 | 0.0587
Epoch 129/300, seasonal_1 Loss: 0.0860 | 0.0587
Epoch 130/300, seasonal_1 Loss: 0.0860 | 0.0586
Epoch 131/300, seasonal_1 Loss: 0.0859 | 0.0586
Epoch 132/300, seasonal_1 Loss: 0.0859 | 0.0586
Epoch 133/300, seasonal_1 Loss: 0.0859 | 0.0585
Epoch 134/300, seasonal_1 Loss: 0.0858 | 0.0585
Epoch 135/300, seasonal_1 Loss: 0.0858 | 0.0585
Epoch 136/300, seasonal_1 Loss: 0.0858 | 0.0584
Epoch 137/300, seasonal_1 Loss: 0.0858 | 0.0584
Epoch 138/300, seasonal_1 Loss: 0.0857 | 0.0584
Epoch 139/300, seasonal_1 Loss: 0.0857 | 0.0584
Epoch 140/300, seasonal_1 Loss: 0.0857 | 0.0583
Epoch 141/300, seasonal_1 Loss: 0.0857 | 0.0583
Epoch 142/300, seasonal_1 Loss: 0.0856 | 0.0583
Epoch 143/300, seasonal_1 Loss: 0.0856 | 0.0583
Epoch 144/300, seasonal_1 Loss: 0.0856 | 0.0582
Epoch 145/300, seasonal_1 Loss: 0.0856 | 0.0582
Epoch 146/300, seasonal_1 Loss: 0.0856 | 0.0582
Epoch 147/300, seasonal_1 Loss: 0.0855 | 0.0582
Epoch 148/300, seasonal_1 Loss: 0.0855 | 0.0582
Epoch 149/300, seasonal_1 Loss: 0.0855 | 0.0581
Epoch 150/300, seasonal_1 Loss: 0.0855 | 0.0581
Epoch 151/300, seasonal_1 Loss: 0.0855 | 0.0581
Epoch 152/300, seasonal_1 Loss: 0.0854 | 0.0581
Epoch 153/300, seasonal_1 Loss: 0.0854 | 0.0581
Epoch 154/300, seasonal_1 Loss: 0.0854 | 0.0580
Epoch 155/300, seasonal_1 Loss: 0.0854 | 0.0580
Epoch 156/300, seasonal_1 Loss: 0.0854 | 0.0580
Epoch 157/300, seasonal_1 Loss: 0.0854 | 0.0580
Epoch 158/300, seasonal_1 Loss: 0.0853 | 0.0580
Epoch 159/300, seasonal_1 Loss: 0.0853 | 0.0580
Epoch 160/300, seasonal_1 Loss: 0.0853 | 0.0579
Epoch 161/300, seasonal_1 Loss: 0.0853 | 0.0579
Epoch 162/300, seasonal_1 Loss: 0.0853 | 0.0579
Epoch 163/300, seasonal_1 Loss: 0.0853 | 0.0579
Epoch 164/300, seasonal_1 Loss: 0.0853 | 0.0579
Epoch 165/300, seasonal_1 Loss: 0.0853 | 0.0579
Epoch 166/300, seasonal_1 Loss: 0.0852 | 0.0578
Epoch 167/300, seasonal_1 Loss: 0.0852 | 0.0578
Epoch 168/300, seasonal_1 Loss: 0.0852 | 0.0578
Epoch 169/300, seasonal_1 Loss: 0.0852 | 0.0578
Epoch 170/300, seasonal_1 Loss: 0.0852 | 0.0578
Epoch 171/300, seasonal_1 Loss: 0.0852 | 0.0578
Epoch 172/300, seasonal_1 Loss: 0.0852 | 0.0578
Epoch 173/300, seasonal_1 Loss: 0.0852 | 0.0578
Epoch 174/300, seasonal_1 Loss: 0.0852 | 0.0578
Epoch 175/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 176/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 177/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 178/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 179/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 180/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 181/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 182/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 183/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 184/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 185/300, seasonal_1 Loss: 0.0851 | 0.0577
Epoch 186/300, seasonal_1 Loss: 0.0851 | 0.0576
Epoch 187/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 188/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 189/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 190/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 191/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 192/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 193/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 194/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 195/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 196/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 197/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 198/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 199/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 200/300, seasonal_1 Loss: 0.0850 | 0.0576
Epoch 201/300, seasonal_1 Loss: 0.0850 | 0.0575
Epoch 202/300, seasonal_1 Loss: 0.0850 | 0.0575
Epoch 203/300, seasonal_1 Loss: 0.0850 | 0.0575
Epoch 204/300, seasonal_1 Loss: 0.0850 | 0.0575
Epoch 205/300, seasonal_1 Loss: 0.0850 | 0.0575
Epoch 206/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 207/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 208/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 209/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 210/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 211/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 212/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 213/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 214/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 215/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 216/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 217/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 218/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 219/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 220/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 221/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 222/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 223/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 224/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 225/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 226/300, seasonal_1 Loss: 0.0849 | 0.0575
Epoch 227/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 228/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 229/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 230/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 231/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 232/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 233/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 234/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 235/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 236/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 237/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 238/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 239/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 240/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 241/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 242/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 243/300, seasonal_1 Loss: 0.0849 | 0.0574
Epoch 244/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 245/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 246/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 247/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 248/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 249/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 250/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 251/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 252/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 253/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 254/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 255/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 256/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 257/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 258/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 259/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 260/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 261/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 262/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 263/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 264/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 265/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 266/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 267/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 268/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 269/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 270/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 271/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 272/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 273/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 274/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 275/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 276/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 277/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 278/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 279/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 280/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 281/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 282/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 283/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 284/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 285/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 286/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 287/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 288/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 289/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 290/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 291/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 292/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 293/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 294/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 295/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 296/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 297/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 298/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 299/300, seasonal_1 Loss: 0.0848 | 0.0574
Epoch 300/300, seasonal_1 Loss: 0.0848 | 0.0574
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.8839016787321132, 'learning_rate': 0.000674165147135357, 'batch_size': 160, 'step_size': 15, 'gamma': 0.9358357495305698}
Epoch 1/300, seasonal_2 Loss: 0.4626 | 0.2977
Epoch 2/300, seasonal_2 Loss: 0.1820 | 0.1756
Epoch 3/300, seasonal_2 Loss: 0.1996 | 0.1906
Epoch 4/300, seasonal_2 Loss: 0.1808 | 0.1148
Epoch 5/300, seasonal_2 Loss: 0.1298 | 0.1235
Epoch 6/300, seasonal_2 Loss: 0.1228 | 0.1063
Epoch 7/300, seasonal_2 Loss: 0.1140 | 0.0772
Epoch 8/300, seasonal_2 Loss: 0.1057 | 0.0691
Epoch 9/300, seasonal_2 Loss: 0.1053 | 0.0671
Epoch 10/300, seasonal_2 Loss: 0.1136 | 0.0768
Epoch 11/300, seasonal_2 Loss: 0.1113 | 0.0686
Epoch 12/300, seasonal_2 Loss: 0.1034 | 0.0670
Epoch 13/300, seasonal_2 Loss: 0.1014 | 0.0616
Epoch 14/300, seasonal_2 Loss: 0.1114 | 0.0783
Epoch 15/300, seasonal_2 Loss: 0.1157 | 0.0746
Epoch 16/300, seasonal_2 Loss: 0.1041 | 0.0656
Epoch 17/300, seasonal_2 Loss: 0.0974 | 0.0558
Epoch 18/300, seasonal_2 Loss: 0.0940 | 0.0585
Epoch 19/300, seasonal_2 Loss: 0.1001 | 0.0770
Epoch 20/300, seasonal_2 Loss: 0.1014 | 0.1037
Epoch 21/300, seasonal_2 Loss: 0.1019 | 0.0775
Epoch 22/300, seasonal_2 Loss: 0.0994 | 0.0619
Epoch 23/300, seasonal_2 Loss: 0.0903 | 0.0510
Epoch 24/300, seasonal_2 Loss: 0.0879 | 0.0475
Epoch 25/300, seasonal_2 Loss: 0.0879 | 0.0546
Epoch 26/300, seasonal_2 Loss: 0.0828 | 0.0449
Epoch 27/300, seasonal_2 Loss: 0.0825 | 0.0449
Epoch 28/300, seasonal_2 Loss: 0.0842 | 0.0465
Epoch 29/300, seasonal_2 Loss: 0.0850 | 0.0477
Epoch 30/300, seasonal_2 Loss: 0.0842 | 0.0492
Epoch 31/300, seasonal_2 Loss: 0.0827 | 0.0488
Epoch 32/300, seasonal_2 Loss: 0.0832 | 0.0423
Epoch 33/300, seasonal_2 Loss: 0.0807 | 0.0446
Epoch 34/300, seasonal_2 Loss: 0.0789 | 0.0466
Epoch 35/300, seasonal_2 Loss: 0.0787 | 0.0435
Epoch 36/300, seasonal_2 Loss: 0.0785 | 0.0439
Epoch 37/300, seasonal_2 Loss: 0.0777 | 0.0440
Epoch 38/300, seasonal_2 Loss: 0.0775 | 0.0437
Epoch 39/300, seasonal_2 Loss: 0.0772 | 0.0424
Epoch 40/300, seasonal_2 Loss: 0.0772 | 0.0436
Epoch 41/300, seasonal_2 Loss: 0.0768 | 0.0412
Epoch 42/300, seasonal_2 Loss: 0.0761 | 0.0408
Epoch 43/300, seasonal_2 Loss: 0.0758 | 0.0387
Epoch 44/300, seasonal_2 Loss: 0.0782 | 0.0389
Epoch 45/300, seasonal_2 Loss: 0.0811 | 0.0444
Epoch 46/300, seasonal_2 Loss: 0.0789 | 0.0419
Epoch 47/300, seasonal_2 Loss: 0.0760 | 0.0417
Epoch 48/300, seasonal_2 Loss: 0.0748 | 0.0417
Epoch 49/300, seasonal_2 Loss: 0.0733 | 0.0360
Epoch 50/300, seasonal_2 Loss: 0.0741 | 0.0378
Epoch 51/300, seasonal_2 Loss: 0.0729 | 0.0363
Epoch 52/300, seasonal_2 Loss: 0.0722 | 0.0362
Epoch 53/300, seasonal_2 Loss: 0.0728 | 0.0367
Epoch 54/300, seasonal_2 Loss: 0.0737 | 0.0380
Epoch 55/300, seasonal_2 Loss: 0.0736 | 0.0381
Epoch 56/300, seasonal_2 Loss: 0.0723 | 0.0368
Epoch 57/300, seasonal_2 Loss: 0.0730 | 0.0355
Epoch 58/300, seasonal_2 Loss: 0.0727 | 0.0358
Epoch 59/300, seasonal_2 Loss: 0.0709 | 0.0340
Epoch 60/300, seasonal_2 Loss: 0.0702 | 0.0346
Epoch 61/300, seasonal_2 Loss: 0.0697 | 0.0362
Epoch 62/300, seasonal_2 Loss: 0.0697 | 0.0383
Epoch 63/300, seasonal_2 Loss: 0.0696 | 0.0360
Epoch 64/300, seasonal_2 Loss: 0.0701 | 0.0371
Epoch 65/300, seasonal_2 Loss: 0.0691 | 0.0345
Epoch 66/300, seasonal_2 Loss: 0.0682 | 0.0346
Epoch 67/300, seasonal_2 Loss: 0.0679 | 0.0346
Epoch 68/300, seasonal_2 Loss: 0.0688 | 0.0348
Epoch 69/300, seasonal_2 Loss: 0.0709 | 0.0354
Epoch 70/300, seasonal_2 Loss: 0.0720 | 0.0398
Epoch 71/300, seasonal_2 Loss: 0.0712 | 0.0436
Epoch 72/300, seasonal_2 Loss: 0.0697 | 0.0401
Epoch 73/300, seasonal_2 Loss: 0.0702 | 0.0354
Epoch 74/300, seasonal_2 Loss: 0.0711 | 0.0364
Epoch 75/300, seasonal_2 Loss: 0.0718 | 0.0357
Epoch 76/300, seasonal_2 Loss: 0.0711 | 0.0365
Epoch 77/300, seasonal_2 Loss: 0.0709 | 0.0360
Epoch 78/300, seasonal_2 Loss: 0.0704 | 0.0349
Epoch 79/300, seasonal_2 Loss: 0.0704 | 0.0383
Epoch 80/300, seasonal_2 Loss: 0.0699 | 0.0378
Epoch 81/300, seasonal_2 Loss: 0.0684 | 0.0361
Epoch 82/300, seasonal_2 Loss: 0.0658 | 0.0325
Epoch 83/300, seasonal_2 Loss: 0.0676 | 0.0356
Epoch 84/300, seasonal_2 Loss: 0.0682 | 0.0336
Epoch 85/300, seasonal_2 Loss: 0.0671 | 0.0331
Epoch 86/300, seasonal_2 Loss: 0.0657 | 0.0331
Epoch 87/300, seasonal_2 Loss: 0.0646 | 0.0325
Epoch 88/300, seasonal_2 Loss: 0.0641 | 0.0328
Epoch 89/300, seasonal_2 Loss: 0.0642 | 0.0340
Epoch 90/300, seasonal_2 Loss: 0.0642 | 0.0347
Epoch 91/300, seasonal_2 Loss: 0.0644 | 0.0345
Epoch 92/300, seasonal_2 Loss: 0.0643 | 0.0342
Epoch 93/300, seasonal_2 Loss: 0.0637 | 0.0334
Epoch 94/300, seasonal_2 Loss: 0.0635 | 0.0320
Epoch 95/300, seasonal_2 Loss: 0.0635 | 0.0312
Epoch 96/300, seasonal_2 Loss: 0.0631 | 0.0297
Epoch 97/300, seasonal_2 Loss: 0.0642 | 0.0300
Epoch 98/300, seasonal_2 Loss: 0.0661 | 0.0325
Epoch 99/300, seasonal_2 Loss: 0.0651 | 0.0321
Epoch 100/300, seasonal_2 Loss: 0.0644 | 0.0315
Epoch 101/300, seasonal_2 Loss: 0.0640 | 0.0317
Epoch 102/300, seasonal_2 Loss: 0.0636 | 0.0321
Epoch 103/300, seasonal_2 Loss: 0.0629 | 0.0347
Epoch 104/300, seasonal_2 Loss: 0.0625 | 0.0330
Epoch 105/300, seasonal_2 Loss: 0.0621 | 0.0299
Epoch 106/300, seasonal_2 Loss: 0.0612 | 0.0291
Epoch 107/300, seasonal_2 Loss: 0.0600 | 0.0274
Epoch 108/300, seasonal_2 Loss: 0.0602 | 0.0280
Epoch 109/300, seasonal_2 Loss: 0.0603 | 0.0272
Epoch 110/300, seasonal_2 Loss: 0.0601 | 0.0273
Epoch 111/300, seasonal_2 Loss: 0.0637 | 0.0336
Epoch 112/300, seasonal_2 Loss: 0.0634 | 0.0290
Epoch 113/300, seasonal_2 Loss: 0.0605 | 0.0266
Epoch 114/300, seasonal_2 Loss: 0.0589 | 0.0262
Epoch 115/300, seasonal_2 Loss: 0.0587 | 0.0274
Epoch 116/300, seasonal_2 Loss: 0.0589 | 0.0277
Epoch 117/300, seasonal_2 Loss: 0.0599 | 0.0274
Epoch 118/300, seasonal_2 Loss: 0.0634 | 0.0339
Epoch 119/300, seasonal_2 Loss: 0.0613 | 0.0334
Epoch 120/300, seasonal_2 Loss: 0.0619 | 0.0275
Epoch 121/300, seasonal_2 Loss: 0.0607 | 0.0281
Epoch 122/300, seasonal_2 Loss: 0.0613 | 0.0290
Epoch 123/300, seasonal_2 Loss: 0.0609 | 0.0320
Epoch 124/300, seasonal_2 Loss: 0.0600 | 0.0292
Epoch 125/300, seasonal_2 Loss: 0.0593 | 0.0272
Epoch 126/300, seasonal_2 Loss: 0.0587 | 0.0264
Epoch 127/300, seasonal_2 Loss: 0.0580 | 0.0254
Epoch 128/300, seasonal_2 Loss: 0.0579 | 0.0257
Epoch 129/300, seasonal_2 Loss: 0.0578 | 0.0268
Epoch 130/300, seasonal_2 Loss: 0.0580 | 0.0266
Epoch 131/300, seasonal_2 Loss: 0.0580 | 0.0261
Epoch 132/300, seasonal_2 Loss: 0.0571 | 0.0267
Epoch 133/300, seasonal_2 Loss: 0.0572 | 0.0275
Epoch 134/300, seasonal_2 Loss: 0.0572 | 0.0278
Epoch 135/300, seasonal_2 Loss: 0.0575 | 0.0274
Epoch 136/300, seasonal_2 Loss: 0.0584 | 0.0279
Epoch 137/300, seasonal_2 Loss: 0.0609 | 0.0351
Epoch 138/300, seasonal_2 Loss: 0.0643 | 0.0318
Epoch 139/300, seasonal_2 Loss: 0.0636 | 0.0328
Epoch 140/300, seasonal_2 Loss: 0.0633 | 0.0301
Epoch 141/300, seasonal_2 Loss: 0.0610 | 0.0268
Epoch 142/300, seasonal_2 Loss: 0.0670 | 0.0316
Epoch 143/300, seasonal_2 Loss: 0.0666 | 0.0330
Epoch 144/300, seasonal_2 Loss: 0.0623 | 0.0458
Epoch 145/300, seasonal_2 Loss: 0.0602 | 0.0308
Epoch 146/300, seasonal_2 Loss: 0.0588 | 0.0266
Epoch 147/300, seasonal_2 Loss: 0.0585 | 0.0259
Epoch 148/300, seasonal_2 Loss: 0.0595 | 0.0282
Epoch 149/300, seasonal_2 Loss: 0.0621 | 0.0285
Epoch 150/300, seasonal_2 Loss: 0.0616 | 0.0281
Epoch 151/300, seasonal_2 Loss: 0.0629 | 0.0300
Epoch 152/300, seasonal_2 Loss: 0.0655 | 0.0349
Epoch 153/300, seasonal_2 Loss: 0.0658 | 0.0304
Epoch 154/300, seasonal_2 Loss: 0.0654 | 0.0310
Epoch 155/300, seasonal_2 Loss: 0.0630 | 0.0269
Epoch 156/300, seasonal_2 Loss: 0.0625 | 0.0302
Epoch 157/300, seasonal_2 Loss: 0.0634 | 0.0302
Epoch 158/300, seasonal_2 Loss: 0.0614 | 0.0292
Epoch 159/300, seasonal_2 Loss: 0.0691 | 0.0356
Epoch 160/300, seasonal_2 Loss: 0.0658 | 0.0331
Epoch 161/300, seasonal_2 Loss: 0.0645 | 0.0404
Epoch 162/300, seasonal_2 Loss: 0.0584 | 0.0283
Epoch 163/300, seasonal_2 Loss: 0.0562 | 0.0256
Epoch 164/300, seasonal_2 Loss: 0.0566 | 0.0251
Epoch 165/300, seasonal_2 Loss: 0.0557 | 0.0252
Epoch 166/300, seasonal_2 Loss: 0.0553 | 0.0244
Epoch 167/300, seasonal_2 Loss: 0.0549 | 0.0246
Epoch 168/300, seasonal_2 Loss: 0.0548 | 0.0264
Epoch 169/300, seasonal_2 Loss: 0.0544 | 0.0243
Epoch 170/300, seasonal_2 Loss: 0.0542 | 0.0239
Epoch 171/300, seasonal_2 Loss: 0.0542 | 0.0248
Epoch 172/300, seasonal_2 Loss: 0.0540 | 0.0252
Epoch 173/300, seasonal_2 Loss: 0.0541 | 0.0254
Epoch 174/300, seasonal_2 Loss: 0.0543 | 0.0256
Epoch 175/300, seasonal_2 Loss: 0.0542 | 0.0252
Epoch 176/300, seasonal_2 Loss: 0.0540 | 0.0247
Epoch 177/300, seasonal_2 Loss: 0.0538 | 0.0247
Epoch 178/300, seasonal_2 Loss: 0.0536 | 0.0246
Epoch 179/300, seasonal_2 Loss: 0.0536 | 0.0244
Epoch 180/300, seasonal_2 Loss: 0.0540 | 0.0246
Epoch 181/300, seasonal_2 Loss: 0.0542 | 0.0246
Epoch 182/300, seasonal_2 Loss: 0.0537 | 0.0253
Epoch 183/300, seasonal_2 Loss: 0.0533 | 0.0256
Epoch 184/300, seasonal_2 Loss: 0.0535 | 0.0252
Epoch 185/300, seasonal_2 Loss: 0.0535 | 0.0248
Epoch 186/300, seasonal_2 Loss: 0.0532 | 0.0243
Epoch 187/300, seasonal_2 Loss: 0.0529 | 0.0241
Epoch 188/300, seasonal_2 Loss: 0.0528 | 0.0243
Epoch 189/300, seasonal_2 Loss: 0.0529 | 0.0247
Epoch 190/300, seasonal_2 Loss: 0.0531 | 0.0252
Epoch 191/300, seasonal_2 Loss: 0.0529 | 0.0253
Epoch 192/300, seasonal_2 Loss: 0.0525 | 0.0254
Epoch 193/300, seasonal_2 Loss: 0.0526 | 0.0254
Epoch 194/300, seasonal_2 Loss: 0.0528 | 0.0256
Epoch 195/300, seasonal_2 Loss: 0.0529 | 0.0254
Epoch 196/300, seasonal_2 Loss: 0.0527 | 0.0246
Epoch 197/300, seasonal_2 Loss: 0.0524 | 0.0244
Epoch 198/300, seasonal_2 Loss: 0.0523 | 0.0248
Epoch 199/300, seasonal_2 Loss: 0.0525 | 0.0253
Epoch 200/300, seasonal_2 Loss: 0.0528 | 0.0255
Epoch 201/300, seasonal_2 Loss: 0.0530 | 0.0253
Epoch 202/300, seasonal_2 Loss: 0.0525 | 0.0257
Epoch 203/300, seasonal_2 Loss: 0.0524 | 0.0254
Epoch 204/300, seasonal_2 Loss: 0.0526 | 0.0253
Epoch 205/300, seasonal_2 Loss: 0.0526 | 0.0250
Epoch 206/300, seasonal_2 Loss: 0.0522 | 0.0247
Epoch 207/300, seasonal_2 Loss: 0.0520 | 0.0245
Epoch 208/300, seasonal_2 Loss: 0.0520 | 0.0248
Epoch 209/300, seasonal_2 Loss: 0.0518 | 0.0255
Epoch 210/300, seasonal_2 Loss: 0.0519 | 0.0253
Epoch 211/300, seasonal_2 Loss: 0.0519 | 0.0254
Epoch 212/300, seasonal_2 Loss: 0.0516 | 0.0260
Epoch 213/300, seasonal_2 Loss: 0.0514 | 0.0258
Epoch 214/300, seasonal_2 Loss: 0.0513 | 0.0254
Epoch 215/300, seasonal_2 Loss: 0.0514 | 0.0251
Epoch 216/300, seasonal_2 Loss: 0.0514 | 0.0250
Epoch 217/300, seasonal_2 Loss: 0.0513 | 0.0249
Epoch 218/300, seasonal_2 Loss: 0.0512 | 0.0250
Epoch 219/300, seasonal_2 Loss: 0.0511 | 0.0251
Epoch 220/300, seasonal_2 Loss: 0.0510 | 0.0253
Epoch 221/300, seasonal_2 Loss: 0.0511 | 0.0256
Epoch 222/300, seasonal_2 Loss: 0.0514 | 0.0262
Epoch 223/300, seasonal_2 Loss: 0.0518 | 0.0268
Epoch 224/300, seasonal_2 Loss: 0.0519 | 0.0268
Epoch 225/300, seasonal_2 Loss: 0.0515 | 0.0265
Epoch 226/300, seasonal_2 Loss: 0.0515 | 0.0256
Epoch 227/300, seasonal_2 Loss: 0.0517 | 0.0256
Epoch 228/300, seasonal_2 Loss: 0.0517 | 0.0255
Epoch 229/300, seasonal_2 Loss: 0.0516 | 0.0254
Epoch 230/300, seasonal_2 Loss: 0.0514 | 0.0250
Epoch 231/300, seasonal_2 Loss: 0.0511 | 0.0251
Epoch 232/300, seasonal_2 Loss: 0.0509 | 0.0248
Epoch 233/300, seasonal_2 Loss: 0.0509 | 0.0258
Epoch 234/300, seasonal_2 Loss: 0.0511 | 0.0273
Epoch 235/300, seasonal_2 Loss: 0.0510 | 0.0279
Epoch 236/300, seasonal_2 Loss: 0.0506 | 0.0273
Epoch 237/300, seasonal_2 Loss: 0.0504 | 0.0260
Epoch 238/300, seasonal_2 Loss: 0.0504 | 0.0254
Epoch 239/300, seasonal_2 Loss: 0.0505 | 0.0251
Epoch 240/300, seasonal_2 Loss: 0.0505 | 0.0254
Epoch 241/300, seasonal_2 Loss: 0.0504 | 0.0251
Epoch 242/300, seasonal_2 Loss: 0.0503 | 0.0255
Epoch 243/300, seasonal_2 Loss: 0.0503 | 0.0258
Epoch 244/300, seasonal_2 Loss: 0.0504 | 0.0264
Epoch 245/300, seasonal_2 Loss: 0.0504 | 0.0266
Epoch 246/300, seasonal_2 Loss: 0.0502 | 0.0270
Epoch 247/300, seasonal_2 Loss: 0.0500 | 0.0265
Epoch 248/300, seasonal_2 Loss: 0.0500 | 0.0263
Epoch 249/300, seasonal_2 Loss: 0.0502 | 0.0260
Epoch 250/300, seasonal_2 Loss: 0.0502 | 0.0253
Epoch 251/300, seasonal_2 Loss: 0.0499 | 0.0255
Epoch 252/300, seasonal_2 Loss: 0.0499 | 0.0258
Epoch 253/300, seasonal_2 Loss: 0.0501 | 0.0261
Epoch 254/300, seasonal_2 Loss: 0.0500 | 0.0264
Epoch 255/300, seasonal_2 Loss: 0.0499 | 0.0265
Epoch 256/300, seasonal_2 Loss: 0.0498 | 0.0268
Epoch 257/300, seasonal_2 Loss: 0.0496 | 0.0266
Epoch 258/300, seasonal_2 Loss: 0.0495 | 0.0258
Epoch 259/300, seasonal_2 Loss: 0.0495 | 0.0257
Epoch 260/300, seasonal_2 Loss: 0.0495 | 0.0257
Epoch 261/300, seasonal_2 Loss: 0.0493 | 0.0256
Epoch 262/300, seasonal_2 Loss: 0.0491 | 0.0260
Epoch 263/300, seasonal_2 Loss: 0.0491 | 0.0262
Epoch 264/300, seasonal_2 Loss: 0.0491 | 0.0266
Epoch 265/300, seasonal_2 Loss: 0.0491 | 0.0269
Epoch 266/300, seasonal_2 Loss: 0.0491 | 0.0267
Epoch 267/300, seasonal_2 Loss: 0.0490 | 0.0265
Epoch 268/300, seasonal_2 Loss: 0.0490 | 0.0263
Epoch 269/300, seasonal_2 Loss: 0.0491 | 0.0262
Epoch 270/300, seasonal_2 Loss: 0.0492 | 0.0259
Epoch 271/300, seasonal_2 Loss: 0.0491 | 0.0263
Epoch 272/300, seasonal_2 Loss: 0.0493 | 0.0266
Epoch 273/300, seasonal_2 Loss: 0.0493 | 0.0267
Epoch 274/300, seasonal_2 Loss: 0.0490 | 0.0265
Epoch 275/300, seasonal_2 Loss: 0.0491 | 0.0271
Epoch 276/300, seasonal_2 Loss: 0.0491 | 0.0271
Epoch 277/300, seasonal_2 Loss: 0.0488 | 0.0270
Epoch 278/300, seasonal_2 Loss: 0.0486 | 0.0264
Epoch 279/300, seasonal_2 Loss: 0.0486 | 0.0264
Epoch 280/300, seasonal_2 Loss: 0.0486 | 0.0263
Epoch 281/300, seasonal_2 Loss: 0.0485 | 0.0265
Epoch 282/300, seasonal_2 Loss: 0.0484 | 0.0266
Epoch 283/300, seasonal_2 Loss: 0.0483 | 0.0265
Epoch 284/300, seasonal_2 Loss: 0.0482 | 0.0265
Epoch 285/300, seasonal_2 Loss: 0.0482 | 0.0267
Epoch 286/300, seasonal_2 Loss: 0.0483 | 0.0271
Epoch 287/300, seasonal_2 Loss: 0.0483 | 0.0273
Epoch 288/300, seasonal_2 Loss: 0.0481 | 0.0273
Epoch 289/300, seasonal_2 Loss: 0.0480 | 0.0273
Epoch 290/300, seasonal_2 Loss: 0.0482 | 0.0273
Epoch 291/300, seasonal_2 Loss: 0.0484 | 0.0274
Epoch 292/300, seasonal_2 Loss: 0.0485 | 0.0272
Epoch 293/300, seasonal_2 Loss: 0.0484 | 0.0272
Epoch 294/300, seasonal_2 Loss: 0.0481 | 0.0272
Epoch 295/300, seasonal_2 Loss: 0.0484 | 0.0280
Epoch 296/300, seasonal_2 Loss: 0.0485 | 0.0284
Epoch 297/300, seasonal_2 Loss: 0.0483 | 0.0276
Epoch 298/300, seasonal_2 Loss: 0.0480 | 0.0278
Epoch 299/300, seasonal_2 Loss: 0.0477 | 0.0279
Epoch 300/300, seasonal_2 Loss: 0.0476 | 0.0275
Training seasonal_3 component with params: {'observation_period_num': 11, 'train_rates': 0.9842128583282893, 'learning_rate': 0.0008405298478883172, 'batch_size': 162, 'step_size': 15, 'gamma': 0.8374829207376403}
Epoch 1/300, seasonal_3 Loss: 0.6182 | 0.2621
Epoch 2/300, seasonal_3 Loss: 0.2174 | 0.1420
Epoch 3/300, seasonal_3 Loss: 0.1644 | 0.1311
Epoch 4/300, seasonal_3 Loss: 0.1730 | 0.1312
Epoch 5/300, seasonal_3 Loss: 0.2418 | 0.3035
Epoch 6/300, seasonal_3 Loss: 0.2562 | 0.3697
Epoch 7/300, seasonal_3 Loss: 0.1885 | 0.3207
Epoch 8/300, seasonal_3 Loss: 0.1667 | 0.0989
Epoch 9/300, seasonal_3 Loss: 0.1267 | 0.1377
Epoch 10/300, seasonal_3 Loss: 0.1136 | 0.0829
Epoch 11/300, seasonal_3 Loss: 0.1109 | 0.0737
Epoch 12/300, seasonal_3 Loss: 0.1067 | 0.0676
Epoch 13/300, seasonal_3 Loss: 0.0991 | 0.0745
Epoch 14/300, seasonal_3 Loss: 0.0953 | 0.0708
Epoch 15/300, seasonal_3 Loss: 0.1019 | 0.0769
Epoch 16/300, seasonal_3 Loss: 0.0995 | 0.0717
Epoch 17/300, seasonal_3 Loss: 0.1040 | 0.0724
Epoch 18/300, seasonal_3 Loss: 0.1057 | 0.0631
Epoch 19/300, seasonal_3 Loss: 0.0993 | 0.0617
Epoch 20/300, seasonal_3 Loss: 0.0918 | 0.0615
Epoch 21/300, seasonal_3 Loss: 0.0860 | 0.0640
Epoch 22/300, seasonal_3 Loss: 0.0853 | 0.0661
Epoch 23/300, seasonal_3 Loss: 0.0857 | 0.0700
Epoch 24/300, seasonal_3 Loss: 0.0813 | 0.0640
Epoch 25/300, seasonal_3 Loss: 0.0822 | 0.0547
Epoch 26/300, seasonal_3 Loss: 0.0876 | 0.0511
Epoch 27/300, seasonal_3 Loss: 0.0869 | 0.0510
Epoch 28/300, seasonal_3 Loss: 0.0869 | 0.0503
Epoch 29/300, seasonal_3 Loss: 0.0884 | 0.0550
Epoch 30/300, seasonal_3 Loss: 0.1053 | 0.0614
Epoch 31/300, seasonal_3 Loss: 0.1096 | 0.0612
Epoch 32/300, seasonal_3 Loss: 0.0989 | 0.0764
Epoch 33/300, seasonal_3 Loss: 0.0972 | 0.0585
Epoch 34/300, seasonal_3 Loss: 0.0907 | 0.0582
Epoch 35/300, seasonal_3 Loss: 0.0937 | 0.0520
Epoch 36/300, seasonal_3 Loss: 0.0810 | 0.0451
Epoch 37/300, seasonal_3 Loss: 0.0847 | 0.0645
Epoch 38/300, seasonal_3 Loss: 0.0993 | 0.0866
Epoch 39/300, seasonal_3 Loss: 0.0971 | 0.0571
Epoch 40/300, seasonal_3 Loss: 0.0837 | 0.0478
Epoch 41/300, seasonal_3 Loss: 0.0747 | 0.0430
Epoch 42/300, seasonal_3 Loss: 0.0784 | 0.0444
Epoch 43/300, seasonal_3 Loss: 0.0754 | 0.0433
Epoch 44/300, seasonal_3 Loss: 0.0736 | 0.0417
Epoch 45/300, seasonal_3 Loss: 0.0715 | 0.0424
Epoch 46/300, seasonal_3 Loss: 0.0717 | 0.0448
Epoch 47/300, seasonal_3 Loss: 0.0713 | 0.0433
Epoch 48/300, seasonal_3 Loss: 0.0705 | 0.0421
Epoch 49/300, seasonal_3 Loss: 0.0695 | 0.0416
Epoch 50/300, seasonal_3 Loss: 0.0690 | 0.0411
Epoch 51/300, seasonal_3 Loss: 0.0685 | 0.0405
Epoch 52/300, seasonal_3 Loss: 0.0682 | 0.0401
Epoch 53/300, seasonal_3 Loss: 0.0680 | 0.0398
Epoch 54/300, seasonal_3 Loss: 0.0677 | 0.0394
Epoch 55/300, seasonal_3 Loss: 0.0677 | 0.0391
Epoch 56/300, seasonal_3 Loss: 0.0675 | 0.0391
Epoch 57/300, seasonal_3 Loss: 0.0673 | 0.0392
Epoch 58/300, seasonal_3 Loss: 0.0670 | 0.0391
Epoch 59/300, seasonal_3 Loss: 0.0668 | 0.0389
Epoch 60/300, seasonal_3 Loss: 0.0665 | 0.0385
Epoch 61/300, seasonal_3 Loss: 0.0662 | 0.0378
Epoch 62/300, seasonal_3 Loss: 0.0659 | 0.0376
Epoch 63/300, seasonal_3 Loss: 0.0654 | 0.0372
Epoch 64/300, seasonal_3 Loss: 0.0650 | 0.0369
Epoch 65/300, seasonal_3 Loss: 0.0647 | 0.0366
Epoch 66/300, seasonal_3 Loss: 0.0645 | 0.0364
Epoch 67/300, seasonal_3 Loss: 0.0643 | 0.0363
Epoch 68/300, seasonal_3 Loss: 0.0641 | 0.0361
Epoch 69/300, seasonal_3 Loss: 0.0639 | 0.0359
Epoch 70/300, seasonal_3 Loss: 0.0639 | 0.0358
Epoch 71/300, seasonal_3 Loss: 0.0637 | 0.0357
Epoch 72/300, seasonal_3 Loss: 0.0637 | 0.0356
Epoch 73/300, seasonal_3 Loss: 0.0636 | 0.0355
Epoch 74/300, seasonal_3 Loss: 0.0635 | 0.0354
Epoch 75/300, seasonal_3 Loss: 0.0634 | 0.0353
Epoch 76/300, seasonal_3 Loss: 0.0634 | 0.0354
Epoch 77/300, seasonal_3 Loss: 0.0636 | 0.0356
Epoch 78/300, seasonal_3 Loss: 0.0636 | 0.0357
Epoch 79/300, seasonal_3 Loss: 0.0636 | 0.0360
Epoch 80/300, seasonal_3 Loss: 0.0636 | 0.0362
Epoch 81/300, seasonal_3 Loss: 0.0634 | 0.0359
Epoch 82/300, seasonal_3 Loss: 0.0632 | 0.0355
Epoch 83/300, seasonal_3 Loss: 0.0631 | 0.0351
Epoch 84/300, seasonal_3 Loss: 0.0628 | 0.0348
Epoch 85/300, seasonal_3 Loss: 0.0626 | 0.0346
Epoch 86/300, seasonal_3 Loss: 0.0628 | 0.0349
Epoch 87/300, seasonal_3 Loss: 0.0637 | 0.0357
Epoch 88/300, seasonal_3 Loss: 0.0646 | 0.0354
Epoch 89/300, seasonal_3 Loss: 0.0638 | 0.0347
Epoch 90/300, seasonal_3 Loss: 0.0630 | 0.0343
Epoch 91/300, seasonal_3 Loss: 0.0627 | 0.0346
Epoch 92/300, seasonal_3 Loss: 0.0621 | 0.0343
Epoch 93/300, seasonal_3 Loss: 0.0616 | 0.0340
Epoch 94/300, seasonal_3 Loss: 0.0613 | 0.0335
Epoch 95/300, seasonal_3 Loss: 0.0612 | 0.0335
Epoch 96/300, seasonal_3 Loss: 0.0611 | 0.0335
Epoch 97/300, seasonal_3 Loss: 0.0610 | 0.0337
Epoch 98/300, seasonal_3 Loss: 0.0610 | 0.0339
Epoch 99/300, seasonal_3 Loss: 0.0613 | 0.0347
Epoch 100/300, seasonal_3 Loss: 0.0620 | 0.0344
Epoch 101/300, seasonal_3 Loss: 0.0622 | 0.0339
Epoch 102/300, seasonal_3 Loss: 0.0618 | 0.0332
Epoch 103/300, seasonal_3 Loss: 0.0615 | 0.0326
Epoch 104/300, seasonal_3 Loss: 0.0611 | 0.0324
Epoch 105/300, seasonal_3 Loss: 0.0609 | 0.0324
Epoch 106/300, seasonal_3 Loss: 0.0608 | 0.0332
Epoch 107/300, seasonal_3 Loss: 0.0612 | 0.0337
Epoch 108/300, seasonal_3 Loss: 0.0612 | 0.0334
Epoch 109/300, seasonal_3 Loss: 0.0608 | 0.0329
Epoch 110/300, seasonal_3 Loss: 0.0604 | 0.0325
Epoch 111/300, seasonal_3 Loss: 0.0603 | 0.0321
Epoch 112/300, seasonal_3 Loss: 0.0605 | 0.0320
Epoch 113/300, seasonal_3 Loss: 0.0613 | 0.0327
Epoch 114/300, seasonal_3 Loss: 0.0627 | 0.0330
Epoch 115/300, seasonal_3 Loss: 0.0621 | 0.0318
Epoch 116/300, seasonal_3 Loss: 0.0605 | 0.0316
Epoch 117/300, seasonal_3 Loss: 0.0597 | 0.0320
Epoch 118/300, seasonal_3 Loss: 0.0595 | 0.0316
Epoch 119/300, seasonal_3 Loss: 0.0593 | 0.0313
Epoch 120/300, seasonal_3 Loss: 0.0592 | 0.0314
Epoch 121/300, seasonal_3 Loss: 0.0592 | 0.0316
Epoch 122/300, seasonal_3 Loss: 0.0594 | 0.0316
Epoch 123/300, seasonal_3 Loss: 0.0594 | 0.0315
Epoch 124/300, seasonal_3 Loss: 0.0592 | 0.0313
Epoch 125/300, seasonal_3 Loss: 0.0589 | 0.0311
Epoch 126/300, seasonal_3 Loss: 0.0588 | 0.0309
Epoch 127/300, seasonal_3 Loss: 0.0587 | 0.0308
Epoch 128/300, seasonal_3 Loss: 0.0586 | 0.0307
Epoch 129/300, seasonal_3 Loss: 0.0585 | 0.0307
Epoch 130/300, seasonal_3 Loss: 0.0584 | 0.0306
Epoch 131/300, seasonal_3 Loss: 0.0584 | 0.0306
Epoch 132/300, seasonal_3 Loss: 0.0583 | 0.0306
Epoch 133/300, seasonal_3 Loss: 0.0583 | 0.0306
Epoch 134/300, seasonal_3 Loss: 0.0583 | 0.0306
Epoch 135/300, seasonal_3 Loss: 0.0583 | 0.0307
Epoch 136/300, seasonal_3 Loss: 0.0583 | 0.0307
Epoch 137/300, seasonal_3 Loss: 0.0583 | 0.0306
Epoch 138/300, seasonal_3 Loss: 0.0582 | 0.0304
Epoch 139/300, seasonal_3 Loss: 0.0580 | 0.0302
Epoch 140/300, seasonal_3 Loss: 0.0579 | 0.0301
Epoch 141/300, seasonal_3 Loss: 0.0579 | 0.0301
Epoch 142/300, seasonal_3 Loss: 0.0579 | 0.0301
Epoch 143/300, seasonal_3 Loss: 0.0580 | 0.0302
Epoch 144/300, seasonal_3 Loss: 0.0580 | 0.0302
Epoch 145/300, seasonal_3 Loss: 0.0579 | 0.0301
Epoch 146/300, seasonal_3 Loss: 0.0577 | 0.0300
Epoch 147/300, seasonal_3 Loss: 0.0577 | 0.0301
Epoch 148/300, seasonal_3 Loss: 0.0577 | 0.0302
Epoch 149/300, seasonal_3 Loss: 0.0578 | 0.0302
Epoch 150/300, seasonal_3 Loss: 0.0577 | 0.0301
Epoch 151/300, seasonal_3 Loss: 0.0576 | 0.0298
Epoch 152/300, seasonal_3 Loss: 0.0575 | 0.0297
Epoch 153/300, seasonal_3 Loss: 0.0574 | 0.0296
Epoch 154/300, seasonal_3 Loss: 0.0574 | 0.0296
Epoch 155/300, seasonal_3 Loss: 0.0574 | 0.0296
Epoch 156/300, seasonal_3 Loss: 0.0573 | 0.0296
Epoch 157/300, seasonal_3 Loss: 0.0573 | 0.0296
Epoch 158/300, seasonal_3 Loss: 0.0572 | 0.0295
Epoch 159/300, seasonal_3 Loss: 0.0572 | 0.0295
Epoch 160/300, seasonal_3 Loss: 0.0572 | 0.0295
Epoch 161/300, seasonal_3 Loss: 0.0571 | 0.0294
Epoch 162/300, seasonal_3 Loss: 0.0571 | 0.0294
Epoch 163/300, seasonal_3 Loss: 0.0571 | 0.0293
Epoch 164/300, seasonal_3 Loss: 0.0570 | 0.0293
Epoch 165/300, seasonal_3 Loss: 0.0570 | 0.0293
Epoch 166/300, seasonal_3 Loss: 0.0570 | 0.0293
Epoch 167/300, seasonal_3 Loss: 0.0569 | 0.0292
Epoch 168/300, seasonal_3 Loss: 0.0569 | 0.0292
Epoch 169/300, seasonal_3 Loss: 0.0569 | 0.0292
Epoch 170/300, seasonal_3 Loss: 0.0568 | 0.0292
Epoch 171/300, seasonal_3 Loss: 0.0568 | 0.0291
Epoch 172/300, seasonal_3 Loss: 0.0568 | 0.0291
Epoch 173/300, seasonal_3 Loss: 0.0568 | 0.0291
Epoch 174/300, seasonal_3 Loss: 0.0567 | 0.0290
Epoch 175/300, seasonal_3 Loss: 0.0567 | 0.0290
Epoch 176/300, seasonal_3 Loss: 0.0567 | 0.0290
Epoch 177/300, seasonal_3 Loss: 0.0566 | 0.0290
Epoch 178/300, seasonal_3 Loss: 0.0566 | 0.0289
Epoch 179/300, seasonal_3 Loss: 0.0566 | 0.0289
Epoch 180/300, seasonal_3 Loss: 0.0566 | 0.0289
Epoch 181/300, seasonal_3 Loss: 0.0565 | 0.0289
Epoch 182/300, seasonal_3 Loss: 0.0565 | 0.0289
Epoch 183/300, seasonal_3 Loss: 0.0565 | 0.0288
Epoch 184/300, seasonal_3 Loss: 0.0565 | 0.0288
Epoch 185/300, seasonal_3 Loss: 0.0565 | 0.0288
Epoch 186/300, seasonal_3 Loss: 0.0564 | 0.0288
Epoch 187/300, seasonal_3 Loss: 0.0564 | 0.0287
Epoch 188/300, seasonal_3 Loss: 0.0564 | 0.0287
Epoch 189/300, seasonal_3 Loss: 0.0564 | 0.0287
Epoch 190/300, seasonal_3 Loss: 0.0563 | 0.0287
Epoch 191/300, seasonal_3 Loss: 0.0563 | 0.0286
Epoch 192/300, seasonal_3 Loss: 0.0563 | 0.0286
Epoch 193/300, seasonal_3 Loss: 0.0563 | 0.0286
Epoch 194/300, seasonal_3 Loss: 0.0563 | 0.0286
Epoch 195/300, seasonal_3 Loss: 0.0562 | 0.0286
Epoch 196/300, seasonal_3 Loss: 0.0562 | 0.0286
Epoch 197/300, seasonal_3 Loss: 0.0562 | 0.0285
Epoch 198/300, seasonal_3 Loss: 0.0562 | 0.0285
Epoch 199/300, seasonal_3 Loss: 0.0562 | 0.0285
Epoch 200/300, seasonal_3 Loss: 0.0561 | 0.0285
Epoch 201/300, seasonal_3 Loss: 0.0561 | 0.0284
Epoch 202/300, seasonal_3 Loss: 0.0561 | 0.0284
Epoch 203/300, seasonal_3 Loss: 0.0561 | 0.0284
Epoch 204/300, seasonal_3 Loss: 0.0561 | 0.0284
Epoch 205/300, seasonal_3 Loss: 0.0561 | 0.0284
Epoch 206/300, seasonal_3 Loss: 0.0560 | 0.0284
Epoch 207/300, seasonal_3 Loss: 0.0560 | 0.0283
Epoch 208/300, seasonal_3 Loss: 0.0560 | 0.0283
Epoch 209/300, seasonal_3 Loss: 0.0560 | 0.0283
Epoch 210/300, seasonal_3 Loss: 0.0560 | 0.0283
Epoch 211/300, seasonal_3 Loss: 0.0560 | 0.0283
Epoch 212/300, seasonal_3 Loss: 0.0559 | 0.0283
Epoch 213/300, seasonal_3 Loss: 0.0559 | 0.0283
Epoch 214/300, seasonal_3 Loss: 0.0559 | 0.0282
Epoch 215/300, seasonal_3 Loss: 0.0559 | 0.0282
Epoch 216/300, seasonal_3 Loss: 0.0559 | 0.0282
Epoch 217/300, seasonal_3 Loss: 0.0559 | 0.0282
Epoch 218/300, seasonal_3 Loss: 0.0559 | 0.0282
Epoch 219/300, seasonal_3 Loss: 0.0558 | 0.0282
Epoch 220/300, seasonal_3 Loss: 0.0558 | 0.0282
Epoch 221/300, seasonal_3 Loss: 0.0558 | 0.0281
Epoch 222/300, seasonal_3 Loss: 0.0558 | 0.0281
Epoch 223/300, seasonal_3 Loss: 0.0558 | 0.0281
Epoch 224/300, seasonal_3 Loss: 0.0558 | 0.0281
Epoch 225/300, seasonal_3 Loss: 0.0558 | 0.0281
Epoch 226/300, seasonal_3 Loss: 0.0558 | 0.0281
Epoch 227/300, seasonal_3 Loss: 0.0557 | 0.0281
Epoch 228/300, seasonal_3 Loss: 0.0557 | 0.0281
Epoch 229/300, seasonal_3 Loss: 0.0557 | 0.0280
Epoch 230/300, seasonal_3 Loss: 0.0557 | 0.0280
Epoch 231/300, seasonal_3 Loss: 0.0557 | 0.0280
Epoch 232/300, seasonal_3 Loss: 0.0557 | 0.0280
Epoch 233/300, seasonal_3 Loss: 0.0557 | 0.0280
Epoch 234/300, seasonal_3 Loss: 0.0557 | 0.0280
Epoch 235/300, seasonal_3 Loss: 0.0557 | 0.0280
Epoch 236/300, seasonal_3 Loss: 0.0556 | 0.0280
Epoch 237/300, seasonal_3 Loss: 0.0556 | 0.0280
Epoch 238/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 239/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 240/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 241/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 242/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 243/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 244/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 245/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 246/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 247/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 248/300, seasonal_3 Loss: 0.0555 | 0.0279
Epoch 249/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 250/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 251/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 252/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 253/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 254/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 255/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 256/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 257/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 258/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 259/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 260/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 261/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 262/300, seasonal_3 Loss: 0.0554 | 0.0278
Epoch 263/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 264/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 265/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 266/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 267/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 268/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 269/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 270/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 271/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 272/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 273/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 274/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 275/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 276/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 277/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 278/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 279/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 280/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 281/300, seasonal_3 Loss: 0.0554 | 0.0277
Epoch 282/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 283/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 284/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 285/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 286/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 287/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 288/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 289/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 290/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 291/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 292/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 293/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 294/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 295/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 296/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 297/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 298/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 299/300, seasonal_3 Loss: 0.0553 | 0.0276
Epoch 300/300, seasonal_3 Loss: 0.0553 | 0.0276
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.9845217270587265, 'learning_rate': 0.00046638797923573706, 'batch_size': 80, 'step_size': 6, 'gamma': 0.8863868985436688}
Epoch 1/300, resid Loss: 0.2322 | 0.1238
Epoch 2/300, resid Loss: 0.1363 | 0.1141
Epoch 3/300, resid Loss: 0.1235 | 0.0800
Epoch 4/300, resid Loss: 0.1125 | 0.0746
Epoch 5/300, resid Loss: 0.1073 | 0.0739
Epoch 6/300, resid Loss: 0.1026 | 0.0739
Epoch 7/300, resid Loss: 0.0986 | 0.0827
Epoch 8/300, resid Loss: 0.0957 | 0.0861
Epoch 9/300, resid Loss: 0.0904 | 0.0840
Epoch 10/300, resid Loss: 0.0898 | 0.0714
Epoch 11/300, resid Loss: 0.0972 | 0.0643
Epoch 12/300, resid Loss: 0.1006 | 0.0656
Epoch 13/300, resid Loss: 0.0889 | 0.0514
Epoch 14/300, resid Loss: 0.0823 | 0.0520
Epoch 15/300, resid Loss: 0.0859 | 0.0527
Epoch 16/300, resid Loss: 0.0843 | 0.0519
Epoch 17/300, resid Loss: 0.0824 | 0.0509
Epoch 18/300, resid Loss: 0.0801 | 0.0491
Epoch 19/300, resid Loss: 0.0781 | 0.0488
Epoch 20/300, resid Loss: 0.0772 | 0.0480
Epoch 21/300, resid Loss: 0.0770 | 0.0488
Epoch 22/300, resid Loss: 0.0764 | 0.0466
Epoch 23/300, resid Loss: 0.0774 | 0.0464
Epoch 24/300, resid Loss: 0.0768 | 0.0450
Epoch 25/300, resid Loss: 0.0758 | 0.0440
Epoch 26/300, resid Loss: 0.0746 | 0.0432
Epoch 27/300, resid Loss: 0.0724 | 0.0428
Epoch 28/300, resid Loss: 0.0718 | 0.0431
Epoch 29/300, resid Loss: 0.0712 | 0.0430
Epoch 30/300, resid Loss: 0.0702 | 0.0422
Epoch 31/300, resid Loss: 0.0695 | 0.0416
Epoch 32/300, resid Loss: 0.0690 | 0.0411
Epoch 33/300, resid Loss: 0.0686 | 0.0409
Epoch 34/300, resid Loss: 0.0683 | 0.0407
Epoch 35/300, resid Loss: 0.0685 | 0.0409
Epoch 36/300, resid Loss: 0.0685 | 0.0411
Epoch 37/300, resid Loss: 0.0682 | 0.0415
Epoch 38/300, resid Loss: 0.0679 | 0.0421
Epoch 39/300, resid Loss: 0.0673 | 0.0425
Epoch 40/300, resid Loss: 0.0670 | 0.0435
Epoch 41/300, resid Loss: 0.0671 | 0.0436
Epoch 42/300, resid Loss: 0.0669 | 0.0432
Epoch 43/300, resid Loss: 0.0669 | 0.0423
Epoch 44/300, resid Loss: 0.0677 | 0.0423
Epoch 45/300, resid Loss: 0.0691 | 0.0426
Epoch 46/300, resid Loss: 0.0710 | 0.0461
Epoch 47/300, resid Loss: 0.0716 | 0.0471
Epoch 48/300, resid Loss: 0.0705 | 0.0462
Epoch 49/300, resid Loss: 0.0703 | 0.0425
Epoch 50/300, resid Loss: 0.0688 | 0.0416
Epoch 51/300, resid Loss: 0.0676 | 0.0406
Epoch 52/300, resid Loss: 0.0675 | 0.0401
Epoch 53/300, resid Loss: 0.0671 | 0.0398
Epoch 54/300, resid Loss: 0.0665 | 0.0396
Epoch 55/300, resid Loss: 0.0661 | 0.0401
Epoch 56/300, resid Loss: 0.0664 | 0.0402
Epoch 57/300, resid Loss: 0.0680 | 0.0406
Epoch 58/300, resid Loss: 0.0705 | 0.0432
Epoch 59/300, resid Loss: 0.0708 | 0.0380
Epoch 60/300, resid Loss: 0.0681 | 0.0393
Epoch 61/300, resid Loss: 0.0681 | 0.0395
Epoch 62/300, resid Loss: 0.0667 | 0.0391
Epoch 63/300, resid Loss: 0.0655 | 0.0385
Epoch 64/300, resid Loss: 0.0651 | 0.0385
Epoch 65/300, resid Loss: 0.0649 | 0.0389
Epoch 66/300, resid Loss: 0.0647 | 0.0395
Epoch 67/300, resid Loss: 0.0648 | 0.0402
Epoch 68/300, resid Loss: 0.0651 | 0.0399
Epoch 69/300, resid Loss: 0.0662 | 0.0397
Epoch 70/300, resid Loss: 0.0687 | 0.0397
Epoch 71/300, resid Loss: 0.0723 | 0.0411
Epoch 72/300, resid Loss: 0.0754 | 0.0463
Epoch 73/300, resid Loss: 0.0755 | 0.0508
Epoch 74/300, resid Loss: 0.0699 | 0.0420
Epoch 75/300, resid Loss: 0.0657 | 0.0387
Epoch 76/300, resid Loss: 0.0656 | 0.0388
Epoch 77/300, resid Loss: 0.0670 | 0.0377
Epoch 78/300, resid Loss: 0.0692 | 0.0387
Epoch 79/300, resid Loss: 0.0705 | 0.0380
Epoch 80/300, resid Loss: 0.0666 | 0.0378
Epoch 81/300, resid Loss: 0.0641 | 0.0368
Epoch 82/300, resid Loss: 0.0625 | 0.0369
Epoch 83/300, resid Loss: 0.0621 | 0.0366
Epoch 84/300, resid Loss: 0.0619 | 0.0364
Epoch 85/300, resid Loss: 0.0619 | 0.0366
Epoch 86/300, resid Loss: 0.0621 | 0.0371
Epoch 87/300, resid Loss: 0.0623 | 0.0375
Epoch 88/300, resid Loss: 0.0624 | 0.0373
Epoch 89/300, resid Loss: 0.0619 | 0.0371
Epoch 90/300, resid Loss: 0.0614 | 0.0367
Epoch 91/300, resid Loss: 0.0612 | 0.0365
Epoch 92/300, resid Loss: 0.0611 | 0.0363
Epoch 93/300, resid Loss: 0.0611 | 0.0362
Epoch 94/300, resid Loss: 0.0611 | 0.0357
Epoch 95/300, resid Loss: 0.0611 | 0.0354
Epoch 96/300, resid Loss: 0.0613 | 0.0352
Epoch 97/300, resid Loss: 0.0614 | 0.0354
Epoch 98/300, resid Loss: 0.0614 | 0.0352
Epoch 99/300, resid Loss: 0.0611 | 0.0351
Epoch 100/300, resid Loss: 0.0609 | 0.0351
Epoch 101/300, resid Loss: 0.0608 | 0.0350
Epoch 102/300, resid Loss: 0.0607 | 0.0350
Epoch 103/300, resid Loss: 0.0607 | 0.0349
Epoch 104/300, resid Loss: 0.0606 | 0.0349
Epoch 105/300, resid Loss: 0.0606 | 0.0348
Epoch 106/300, resid Loss: 0.0605 | 0.0348
Epoch 107/300, resid Loss: 0.0604 | 0.0348
Epoch 108/300, resid Loss: 0.0604 | 0.0347
Epoch 109/300, resid Loss: 0.0604 | 0.0347
Epoch 110/300, resid Loss: 0.0603 | 0.0347
Epoch 111/300, resid Loss: 0.0603 | 0.0346
Epoch 112/300, resid Loss: 0.0602 | 0.0346
Epoch 113/300, resid Loss: 0.0602 | 0.0346
Epoch 114/300, resid Loss: 0.0602 | 0.0346
Epoch 115/300, resid Loss: 0.0601 | 0.0345
Epoch 116/300, resid Loss: 0.0601 | 0.0345
Epoch 117/300, resid Loss: 0.0601 | 0.0345
Epoch 118/300, resid Loss: 0.0601 | 0.0344
Epoch 119/300, resid Loss: 0.0600 | 0.0344
Epoch 120/300, resid Loss: 0.0600 | 0.0344
Epoch 121/300, resid Loss: 0.0600 | 0.0343
Epoch 122/300, resid Loss: 0.0600 | 0.0343
Epoch 123/300, resid Loss: 0.0600 | 0.0343
Epoch 124/300, resid Loss: 0.0599 | 0.0342
Epoch 125/300, resid Loss: 0.0599 | 0.0342
Epoch 126/300, resid Loss: 0.0599 | 0.0342
Epoch 127/300, resid Loss: 0.0599 | 0.0342
Epoch 128/300, resid Loss: 0.0599 | 0.0342
Epoch 129/300, resid Loss: 0.0598 | 0.0341
Epoch 130/300, resid Loss: 0.0598 | 0.0341
Epoch 131/300, resid Loss: 0.0598 | 0.0341
Epoch 132/300, resid Loss: 0.0598 | 0.0341
Epoch 133/300, resid Loss: 0.0598 | 0.0341
Epoch 134/300, resid Loss: 0.0598 | 0.0340
Epoch 135/300, resid Loss: 0.0598 | 0.0340
Epoch 136/300, resid Loss: 0.0597 | 0.0340
Epoch 137/300, resid Loss: 0.0597 | 0.0340
Epoch 138/300, resid Loss: 0.0597 | 0.0340
Epoch 139/300, resid Loss: 0.0597 | 0.0340
Epoch 140/300, resid Loss: 0.0597 | 0.0340
Epoch 141/300, resid Loss: 0.0597 | 0.0339
Epoch 142/300, resid Loss: 0.0597 | 0.0339
Epoch 143/300, resid Loss: 0.0597 | 0.0339
Epoch 144/300, resid Loss: 0.0597 | 0.0339
Epoch 145/300, resid Loss: 0.0596 | 0.0339
Epoch 146/300, resid Loss: 0.0596 | 0.0339
Epoch 147/300, resid Loss: 0.0596 | 0.0339
Epoch 148/300, resid Loss: 0.0596 | 0.0339
Epoch 149/300, resid Loss: 0.0596 | 0.0338
Epoch 150/300, resid Loss: 0.0596 | 0.0338
Epoch 151/300, resid Loss: 0.0596 | 0.0338
Epoch 152/300, resid Loss: 0.0596 | 0.0338
Epoch 153/300, resid Loss: 0.0596 | 0.0338
Epoch 154/300, resid Loss: 0.0596 | 0.0338
Epoch 155/300, resid Loss: 0.0596 | 0.0338
Epoch 156/300, resid Loss: 0.0596 | 0.0338
Epoch 157/300, resid Loss: 0.0596 | 0.0338
Epoch 158/300, resid Loss: 0.0595 | 0.0338
Epoch 159/300, resid Loss: 0.0595 | 0.0338
Epoch 160/300, resid Loss: 0.0595 | 0.0338
Epoch 161/300, resid Loss: 0.0595 | 0.0338
Epoch 162/300, resid Loss: 0.0595 | 0.0337
Epoch 163/300, resid Loss: 0.0595 | 0.0337
Epoch 164/300, resid Loss: 0.0595 | 0.0337
Epoch 165/300, resid Loss: 0.0595 | 0.0337
Epoch 166/300, resid Loss: 0.0595 | 0.0337
Epoch 167/300, resid Loss: 0.0595 | 0.0337
Epoch 168/300, resid Loss: 0.0595 | 0.0337
Epoch 169/300, resid Loss: 0.0595 | 0.0337
Epoch 170/300, resid Loss: 0.0595 | 0.0337
Epoch 171/300, resid Loss: 0.0595 | 0.0337
Epoch 172/300, resid Loss: 0.0595 | 0.0337
Epoch 173/300, resid Loss: 0.0595 | 0.0337
Epoch 174/300, resid Loss: 0.0595 | 0.0337
Epoch 175/300, resid Loss: 0.0595 | 0.0337
Epoch 176/300, resid Loss: 0.0595 | 0.0337
Epoch 177/300, resid Loss: 0.0595 | 0.0337
Epoch 178/300, resid Loss: 0.0595 | 0.0337
Epoch 179/300, resid Loss: 0.0595 | 0.0337
Epoch 180/300, resid Loss: 0.0595 | 0.0337
Epoch 181/300, resid Loss: 0.0595 | 0.0337
Epoch 182/300, resid Loss: 0.0595 | 0.0337
Epoch 183/300, resid Loss: 0.0595 | 0.0337
Epoch 184/300, resid Loss: 0.0595 | 0.0337
Epoch 185/300, resid Loss: 0.0594 | 0.0337
Epoch 186/300, resid Loss: 0.0594 | 0.0337
Epoch 187/300, resid Loss: 0.0594 | 0.0336
Epoch 188/300, resid Loss: 0.0594 | 0.0336
Epoch 189/300, resid Loss: 0.0594 | 0.0336
Epoch 190/300, resid Loss: 0.0594 | 0.0336
Epoch 191/300, resid Loss: 0.0594 | 0.0336
Epoch 192/300, resid Loss: 0.0594 | 0.0336
Epoch 193/300, resid Loss: 0.0594 | 0.0336
Epoch 194/300, resid Loss: 0.0594 | 0.0336
Epoch 195/300, resid Loss: 0.0594 | 0.0336
Epoch 196/300, resid Loss: 0.0594 | 0.0336
Epoch 197/300, resid Loss: 0.0594 | 0.0336
Epoch 198/300, resid Loss: 0.0594 | 0.0336
Epoch 199/300, resid Loss: 0.0594 | 0.0336
Epoch 200/300, resid Loss: 0.0594 | 0.0336
Epoch 201/300, resid Loss: 0.0594 | 0.0336
Epoch 202/300, resid Loss: 0.0594 | 0.0336
Epoch 203/300, resid Loss: 0.0594 | 0.0336
Epoch 204/300, resid Loss: 0.0594 | 0.0336
Epoch 205/300, resid Loss: 0.0594 | 0.0336
Epoch 206/300, resid Loss: 0.0594 | 0.0336
Epoch 207/300, resid Loss: 0.0594 | 0.0336
Epoch 208/300, resid Loss: 0.0594 | 0.0336
Epoch 209/300, resid Loss: 0.0594 | 0.0336
Epoch 210/300, resid Loss: 0.0594 | 0.0336
Epoch 211/300, resid Loss: 0.0594 | 0.0336
Epoch 212/300, resid Loss: 0.0594 | 0.0336
Epoch 213/300, resid Loss: 0.0594 | 0.0336
Epoch 214/300, resid Loss: 0.0594 | 0.0336
Epoch 215/300, resid Loss: 0.0594 | 0.0336
Epoch 216/300, resid Loss: 0.0594 | 0.0336
Epoch 217/300, resid Loss: 0.0594 | 0.0336
Epoch 218/300, resid Loss: 0.0594 | 0.0336
Epoch 219/300, resid Loss: 0.0594 | 0.0336
Epoch 220/300, resid Loss: 0.0594 | 0.0336
Epoch 221/300, resid Loss: 0.0594 | 0.0336
Epoch 222/300, resid Loss: 0.0594 | 0.0336
Epoch 223/300, resid Loss: 0.0594 | 0.0336
Epoch 224/300, resid Loss: 0.0594 | 0.0336
Epoch 225/300, resid Loss: 0.0594 | 0.0336
Epoch 226/300, resid Loss: 0.0594 | 0.0336
Epoch 227/300, resid Loss: 0.0594 | 0.0336
Epoch 228/300, resid Loss: 0.0594 | 0.0336
Epoch 229/300, resid Loss: 0.0594 | 0.0336
Epoch 230/300, resid Loss: 0.0594 | 0.0336
Epoch 231/300, resid Loss: 0.0594 | 0.0336
Epoch 232/300, resid Loss: 0.0594 | 0.0336
Epoch 233/300, resid Loss: 0.0594 | 0.0336
Epoch 234/300, resid Loss: 0.0594 | 0.0336
Epoch 235/300, resid Loss: 0.0594 | 0.0336
Epoch 236/300, resid Loss: 0.0594 | 0.0336
Epoch 237/300, resid Loss: 0.0594 | 0.0336
Epoch 238/300, resid Loss: 0.0594 | 0.0336
Epoch 239/300, resid Loss: 0.0594 | 0.0336
Epoch 240/300, resid Loss: 0.0594 | 0.0336
Epoch 241/300, resid Loss: 0.0594 | 0.0336
Epoch 242/300, resid Loss: 0.0594 | 0.0336
Epoch 243/300, resid Loss: 0.0594 | 0.0336
Epoch 244/300, resid Loss: 0.0594 | 0.0336
Epoch 245/300, resid Loss: 0.0594 | 0.0336
Epoch 246/300, resid Loss: 0.0594 | 0.0336
Epoch 247/300, resid Loss: 0.0594 | 0.0336
Epoch 248/300, resid Loss: 0.0594 | 0.0336
Epoch 249/300, resid Loss: 0.0594 | 0.0336
Epoch 250/300, resid Loss: 0.0594 | 0.0336
Epoch 251/300, resid Loss: 0.0594 | 0.0336
Epoch 252/300, resid Loss: 0.0594 | 0.0336
Epoch 253/300, resid Loss: 0.0594 | 0.0336
Epoch 254/300, resid Loss: 0.0594 | 0.0336
Epoch 255/300, resid Loss: 0.0594 | 0.0336
Epoch 256/300, resid Loss: 0.0594 | 0.0336
Epoch 257/300, resid Loss: 0.0594 | 0.0336
Epoch 258/300, resid Loss: 0.0594 | 0.0336
Epoch 259/300, resid Loss: 0.0594 | 0.0336
Epoch 260/300, resid Loss: 0.0594 | 0.0336
Epoch 261/300, resid Loss: 0.0594 | 0.0336
Epoch 262/300, resid Loss: 0.0594 | 0.0336
Epoch 263/300, resid Loss: 0.0594 | 0.0336
Epoch 264/300, resid Loss: 0.0594 | 0.0336
Epoch 265/300, resid Loss: 0.0594 | 0.0336
Epoch 266/300, resid Loss: 0.0594 | 0.0336
Epoch 267/300, resid Loss: 0.0594 | 0.0336
Epoch 268/300, resid Loss: 0.0594 | 0.0336
Epoch 269/300, resid Loss: 0.0594 | 0.0336
Epoch 270/300, resid Loss: 0.0594 | 0.0336
Epoch 271/300, resid Loss: 0.0594 | 0.0336
Epoch 272/300, resid Loss: 0.0594 | 0.0336
Epoch 273/300, resid Loss: 0.0594 | 0.0336
Epoch 274/300, resid Loss: 0.0594 | 0.0336
Epoch 275/300, resid Loss: 0.0594 | 0.0336
Epoch 276/300, resid Loss: 0.0594 | 0.0336
Epoch 277/300, resid Loss: 0.0594 | 0.0336
Epoch 278/300, resid Loss: 0.0594 | 0.0336
Epoch 279/300, resid Loss: 0.0594 | 0.0336
Epoch 280/300, resid Loss: 0.0594 | 0.0336
Epoch 281/300, resid Loss: 0.0594 | 0.0336
Epoch 282/300, resid Loss: 0.0594 | 0.0336
Epoch 283/300, resid Loss: 0.0594 | 0.0336
Epoch 284/300, resid Loss: 0.0594 | 0.0336
Epoch 285/300, resid Loss: 0.0594 | 0.0336
Epoch 286/300, resid Loss: 0.0594 | 0.0336
Epoch 287/300, resid Loss: 0.0594 | 0.0336
Epoch 288/300, resid Loss: 0.0594 | 0.0336
Epoch 289/300, resid Loss: 0.0594 | 0.0336
Epoch 290/300, resid Loss: 0.0594 | 0.0336
Epoch 291/300, resid Loss: 0.0594 | 0.0336
Epoch 292/300, resid Loss: 0.0594 | 0.0336
Epoch 293/300, resid Loss: 0.0594 | 0.0336
Epoch 294/300, resid Loss: 0.0594 | 0.0336
Epoch 295/300, resid Loss: 0.0594 | 0.0336
Epoch 296/300, resid Loss: 0.0594 | 0.0336
Epoch 297/300, resid Loss: 0.0594 | 0.0336
Epoch 298/300, resid Loss: 0.0594 | 0.0336
Epoch 299/300, resid Loss: 0.0594 | 0.0336
Epoch 300/300, resid Loss: 0.0594 | 0.0336
Runtime (seconds): 1514.2280278205872
4.668291486710994e-05
[156.01239]
[-1.6285608]
[-5.1490455]
[10.020376]
[1.7988689]
[7.287986]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 3.821196938166395
RMSE: 1.9547882080078125
MAE: 1.9547882080078125
R-squared: nan
[168.34201]
