[32m[I 2025-02-06 09:41:30,444][0m A new study created in memory with name: no-name-717d1fec-e35d-4a37-98d8-27541c5fa101[0m
[32m[I 2025-02-06 09:42:05,308][0m Trial 0 finished with value: 0.17666935626870003 and parameters: {'observation_period_num': 99, 'train_rates': 0.6295633766706676, 'learning_rate': 0.0005628039924668223, 'batch_size': 133, 'step_size': 15, 'gamma': 0.8691266718233239}. Best is trial 0 with value: 0.17666935626870003.[0m
[32m[I 2025-02-06 09:42:35,524][0m Trial 1 finished with value: 0.2691390105491675 and parameters: {'observation_period_num': 231, 'train_rates': 0.77494016844602, 'learning_rate': 1.7544230822979095e-05, 'batch_size': 174, 'step_size': 10, 'gamma': 0.9804384961062649}. Best is trial 0 with value: 0.17666935626870003.[0m
[32m[I 2025-02-06 09:43:27,757][0m Trial 2 finished with value: 0.4128799357294609 and parameters: {'observation_period_num': 36, 'train_rates': 0.9056179700128582, 'learning_rate': 3.290597049806816e-06, 'batch_size': 116, 'step_size': 14, 'gamma': 0.7843551007456787}. Best is trial 0 with value: 0.17666935626870003.[0m
[32m[I 2025-02-06 09:44:43,477][0m Trial 3 finished with value: 0.3062552511692047 and parameters: {'observation_period_num': 173, 'train_rates': 0.9832566029542873, 'learning_rate': 0.0005183482665396811, 'batch_size': 78, 'step_size': 11, 'gamma': 0.8697083732617306}. Best is trial 0 with value: 0.17666935626870003.[0m
[32m[I 2025-02-06 09:46:53,636][0m Trial 4 finished with value: 0.26601483578130297 and parameters: {'observation_period_num': 46, 'train_rates': 0.6028720847297663, 'learning_rate': 0.0003514148320646654, 'batch_size': 33, 'step_size': 9, 'gamma': 0.9522089396632927}. Best is trial 0 with value: 0.17666935626870003.[0m
[32m[I 2025-02-06 09:47:14,809][0m Trial 5 finished with value: 0.9139445738497324 and parameters: {'observation_period_num': 85, 'train_rates': 0.6494172223198467, 'learning_rate': 2.144846318200474e-06, 'batch_size': 238, 'step_size': 11, 'gamma': 0.945341897999423}. Best is trial 0 with value: 0.17666935626870003.[0m
[32m[I 2025-02-06 09:47:45,412][0m Trial 6 finished with value: 0.25752808039378966 and parameters: {'observation_period_num': 226, 'train_rates': 0.7780169587828235, 'learning_rate': 0.0006437472353971765, 'batch_size': 177, 'step_size': 4, 'gamma': 0.9891127279158112}. Best is trial 0 with value: 0.17666935626870003.[0m
[32m[I 2025-02-06 09:48:38,835][0m Trial 7 finished with value: 0.4859619589517836 and parameters: {'observation_period_num': 185, 'train_rates': 0.8571719429993487, 'learning_rate': 1.7774926525313448e-06, 'batch_size': 100, 'step_size': 3, 'gamma': 0.9899080597953568}. Best is trial 0 with value: 0.17666935626870003.[0m
[32m[I 2025-02-06 09:49:02,227][0m Trial 8 finished with value: 0.378161831042313 and parameters: {'observation_period_num': 88, 'train_rates': 0.6319746874895613, 'learning_rate': 9.797915594791825e-05, 'batch_size': 215, 'step_size': 2, 'gamma': 0.9042241609446837}. Best is trial 0 with value: 0.17666935626870003.[0m
[32m[I 2025-02-06 09:49:36,444][0m Trial 9 finished with value: 0.09075154137806357 and parameters: {'observation_period_num': 50, 'train_rates': 0.9272532172173603, 'learning_rate': 8.396848285534348e-05, 'batch_size': 185, 'step_size': 5, 'gamma': 0.9378423562348223}. Best is trial 9 with value: 0.09075154137806357.[0m
[32m[I 2025-02-06 09:50:14,775][0m Trial 10 finished with value: 0.1606629192829132 and parameters: {'observation_period_num': 5, 'train_rates': 0.9855025521566548, 'learning_rate': 2.740823210744279e-05, 'batch_size': 183, 'step_size': 5, 'gamma': 0.8185320467719531}. Best is trial 9 with value: 0.09075154137806357.[0m
[32m[I 2025-02-06 09:50:49,845][0m Trial 11 finished with value: 0.21401679515838623 and parameters: {'observation_period_num': 9, 'train_rates': 0.9837574043532277, 'learning_rate': 2.7572634494938038e-05, 'batch_size': 186, 'step_size': 6, 'gamma': 0.8097939167516862}. Best is trial 9 with value: 0.09075154137806357.[0m
[32m[I 2025-02-06 09:51:19,050][0m Trial 12 finished with value: 0.07178942463197908 and parameters: {'observation_period_num': 11, 'train_rates': 0.9041781503950737, 'learning_rate': 7.932841478839136e-05, 'batch_size': 210, 'step_size': 6, 'gamma': 0.8223147332432321}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:51:44,389][0m Trial 13 finished with value: 0.10944622457027435 and parameters: {'observation_period_num': 55, 'train_rates': 0.8910543050913636, 'learning_rate': 0.00012042720565677022, 'batch_size': 256, 'step_size': 7, 'gamma': 0.8385198398716563}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:52:12,549][0m Trial 14 finished with value: 0.3133841178750479 and parameters: {'observation_period_num': 135, 'train_rates': 0.8371929447259011, 'learning_rate': 0.00010527665088538579, 'batch_size': 214, 'step_size': 1, 'gamma': 0.9044542354742524}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:52:51,847][0m Trial 15 finished with value: 0.45304197591284046 and parameters: {'observation_period_num': 58, 'train_rates': 0.9216867066943536, 'learning_rate': 8.068162508923637e-06, 'batch_size': 152, 'step_size': 8, 'gamma': 0.7630720921737686}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:53:16,617][0m Trial 16 finished with value: 0.25275201682731435 and parameters: {'observation_period_num': 119, 'train_rates': 0.7225046669580752, 'learning_rate': 0.0001495781677411778, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9089816061095563}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:53:56,489][0m Trial 17 finished with value: 0.10399280425528405 and parameters: {'observation_period_num': 32, 'train_rates': 0.9280488741899493, 'learning_rate': 5.697528403230722e-05, 'batch_size': 156, 'step_size': 7, 'gamma': 0.8299948526314986}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:54:26,314][0m Trial 18 finished with value: 0.7014878498240821 and parameters: {'observation_period_num': 74, 'train_rates': 0.8418812998387687, 'learning_rate': 1.024875883207091e-05, 'batch_size': 203, 'step_size': 3, 'gamma': 0.8566954857660308}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:54:48,853][0m Trial 19 finished with value: 0.1703059709503123 and parameters: {'observation_period_num': 25, 'train_rates': 0.7269068461193134, 'learning_rate': 0.00020455764449315853, 'batch_size': 242, 'step_size': 13, 'gamma': 0.7940045219664797}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:56:24,828][0m Trial 20 finished with value: 0.1369006335735321 and parameters: {'observation_period_num': 141, 'train_rates': 0.9349694198565985, 'learning_rate': 5.2037261198327856e-05, 'batch_size': 60, 'step_size': 6, 'gamma': 0.9306256221370284}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:57:10,348][0m Trial 21 finished with value: 0.07820104946322898 and parameters: {'observation_period_num': 17, 'train_rates': 0.9525366346466578, 'learning_rate': 5.40092105377681e-05, 'batch_size': 138, 'step_size': 8, 'gamma': 0.8424295576804859}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:57:54,365][0m Trial 22 finished with value: 0.08217498070883508 and parameters: {'observation_period_num': 20, 'train_rates': 0.9505506587122828, 'learning_rate': 5.365824694838945e-05, 'batch_size': 146, 'step_size': 8, 'gamma': 0.8441599453872752}. Best is trial 12 with value: 0.07178942463197908.[0m
[32m[I 2025-02-06 09:58:41,239][0m Trial 23 finished with value: 0.03991686594660215 and parameters: {'observation_period_num': 20, 'train_rates': 0.8724772089996855, 'learning_rate': 0.0002394716988300176, 'batch_size': 126, 'step_size': 9, 'gamma': 0.8436183538932935}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 09:59:29,898][0m Trial 24 finished with value: 0.05822865099940631 and parameters: {'observation_period_num': 71, 'train_rates': 0.8667668267730863, 'learning_rate': 0.0002543641081854061, 'batch_size': 116, 'step_size': 10, 'gamma': 0.8022450705068812}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:00:24,574][0m Trial 25 finished with value: 0.0723105861368228 and parameters: {'observation_period_num': 69, 'train_rates': 0.8658268293158714, 'learning_rate': 0.00028133841654579076, 'batch_size': 104, 'step_size': 12, 'gamma': 0.7533898832983279}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:01:31,672][0m Trial 26 finished with value: 0.15603146044475458 and parameters: {'observation_period_num': 107, 'train_rates': 0.8228123083215103, 'learning_rate': 0.0008807416978532136, 'batch_size': 80, 'step_size': 9, 'gamma': 0.796607628830979}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:02:22,079][0m Trial 27 finished with value: 0.046277383879079655 and parameters: {'observation_period_num': 35, 'train_rates': 0.8803399964674422, 'learning_rate': 0.0002646593081226536, 'batch_size': 117, 'step_size': 11, 'gamma': 0.7722953650360156}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:03:07,038][0m Trial 28 finished with value: 0.08920646035450104 and parameters: {'observation_period_num': 69, 'train_rates': 0.7970219016568927, 'learning_rate': 0.0002895161225457716, 'batch_size': 121, 'step_size': 12, 'gamma': 0.7723286646873102}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:04:12,492][0m Trial 29 finished with value: 0.1851104138602673 and parameters: {'observation_period_num': 95, 'train_rates': 0.8773137657056169, 'learning_rate': 0.0009871125636633498, 'batch_size': 86, 'step_size': 15, 'gamma': 0.8884195070289383}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:04:58,332][0m Trial 30 finished with value: 0.048047368932854044 and parameters: {'observation_period_num': 39, 'train_rates': 0.8136260032059107, 'learning_rate': 0.00046104053592864935, 'batch_size': 126, 'step_size': 10, 'gamma': 0.8019817034267717}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:05:43,673][0m Trial 31 finished with value: 0.04583232970132182 and parameters: {'observation_period_num': 33, 'train_rates': 0.8052514100982682, 'learning_rate': 0.000416446534150543, 'batch_size': 122, 'step_size': 10, 'gamma': 0.7802319523527267}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:06:25,623][0m Trial 32 finished with value: 0.0715781864594971 and parameters: {'observation_period_num': 41, 'train_rates': 0.8078633348599563, 'learning_rate': 0.0004608207667706963, 'batch_size': 133, 'step_size': 10, 'gamma': 0.7777009092414509}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:06:59,015][0m Trial 33 finished with value: 0.17679164555816143 and parameters: {'observation_period_num': 34, 'train_rates': 0.7327237030103567, 'learning_rate': 0.00016629381343071924, 'batch_size': 162, 'step_size': 11, 'gamma': 0.7527438748118345}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:07:53,358][0m Trial 34 finished with value: 0.16409492354702065 and parameters: {'observation_period_num': 25, 'train_rates': 0.7540518090698518, 'learning_rate': 0.0004493956457708804, 'batch_size': 93, 'step_size': 9, 'gamma': 0.7871852577801532}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:09:12,807][0m Trial 35 finished with value: 0.16411488054704246 and parameters: {'observation_period_num': 43, 'train_rates': 0.6918898994867041, 'learning_rate': 0.0006648114019304355, 'batch_size': 60, 'step_size': 13, 'gamma': 0.7691088247152102}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:09:56,659][0m Trial 36 finished with value: 0.18646713951113877 and parameters: {'observation_period_num': 57, 'train_rates': 0.7725783198999532, 'learning_rate': 0.0003580732040195017, 'batch_size': 125, 'step_size': 11, 'gamma': 0.811283885244812}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:10:45,951][0m Trial 37 finished with value: 0.1939869067000766 and parameters: {'observation_period_num': 172, 'train_rates': 0.8239985003094649, 'learning_rate': 0.0006432128623111978, 'batch_size': 110, 'step_size': 12, 'gamma': 0.8596970917623414}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:11:58,804][0m Trial 38 finished with value: 0.6832831499149215 and parameters: {'observation_period_num': 242, 'train_rates': 0.8456423149269402, 'learning_rate': 1.1546690222630994e-06, 'batch_size': 71, 'step_size': 10, 'gamma': 0.7845268925677265}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:12:43,330][0m Trial 39 finished with value: 0.046199150844808703 and parameters: {'observation_period_num': 34, 'train_rates': 0.8909137837887375, 'learning_rate': 0.00021173281435311092, 'batch_size': 134, 'step_size': 9, 'gamma': 0.880151200425815}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:13:26,179][0m Trial 40 finished with value: 0.1919033987696763 and parameters: {'observation_period_num': 152, 'train_rates': 0.8893883517355534, 'learning_rate': 0.00019949988533867296, 'batch_size': 137, 'step_size': 9, 'gamma': 0.8857196135043142}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:14:01,135][0m Trial 41 finished with value: 0.06290852892621655 and parameters: {'observation_period_num': 31, 'train_rates': 0.8191697420296389, 'learning_rate': 0.00034882802774293726, 'batch_size': 162, 'step_size': 10, 'gamma': 0.8825809008555593}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:14:45,686][0m Trial 42 finished with value: 0.1915290202427457 and parameters: {'observation_period_num': 43, 'train_rates': 0.7706089474333966, 'learning_rate': 0.0005107521837878483, 'batch_size': 125, 'step_size': 11, 'gamma': 0.8653084406282129}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:15:37,325][0m Trial 43 finished with value: 0.07201547571262921 and parameters: {'observation_period_num': 83, 'train_rates': 0.7946456492697648, 'learning_rate': 0.00021216489046124894, 'batch_size': 103, 'step_size': 8, 'gamma': 0.8021039355101263}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:20:05,671][0m Trial 44 finished with value: 0.046539623485725434 and parameters: {'observation_period_num': 16, 'train_rates': 0.8974779919901256, 'learning_rate': 0.0003546693661711719, 'batch_size': 21, 'step_size': 9, 'gamma': 0.9699088230192183}. Best is trial 23 with value: 0.03991686594660215.[0m
[32m[I 2025-02-06 10:25:36,090][0m Trial 45 finished with value: 0.02995698685538135 and parameters: {'observation_period_num': 6, 'train_rates': 0.8990730084569691, 'learning_rate': 0.0001364669867612119, 'batch_size': 17, 'step_size': 7, 'gamma': 0.9626673591092519}. Best is trial 45 with value: 0.02995698685538135.[0m
[32m[I 2025-02-06 10:27:44,954][0m Trial 46 finished with value: 0.028473223225688033 and parameters: {'observation_period_num': 8, 'train_rates': 0.9647105656807625, 'learning_rate': 0.0001356142909157797, 'batch_size': 47, 'step_size': 7, 'gamma': 0.963152082332493}. Best is trial 46 with value: 0.028473223225688033.[0m
[32m[I 2025-02-06 10:30:19,696][0m Trial 47 finished with value: 0.03227592790025776 and parameters: {'observation_period_num': 6, 'train_rates': 0.9607021473967322, 'learning_rate': 0.0001309632658393622, 'batch_size': 39, 'step_size': 7, 'gamma': 0.9705211371497837}. Best is trial 46 with value: 0.028473223225688033.[0m
[32m[I 2025-02-06 10:32:51,467][0m Trial 48 finished with value: 0.03514136884834117 and parameters: {'observation_period_num': 7, 'train_rates': 0.9552475797818692, 'learning_rate': 0.00013697733535584708, 'batch_size': 39, 'step_size': 7, 'gamma': 0.9690757312113376}. Best is trial 46 with value: 0.028473223225688033.[0m
[32m[I 2025-02-06 10:35:13,096][0m Trial 49 finished with value: 0.03445278757828332 and parameters: {'observation_period_num': 7, 'train_rates': 0.9720967340549558, 'learning_rate': 0.00013397564162520445, 'batch_size': 43, 'step_size': 7, 'gamma': 0.9737497526331091}. Best is trial 46 with value: 0.028473223225688033.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2322 | 0.1570
Epoch 2/300, Loss: 0.1393 | 0.1400
Epoch 3/300, Loss: 0.1277 | 0.1230
Epoch 4/300, Loss: 0.1210 | 0.0957
Epoch 5/300, Loss: 0.1191 | 0.1048
Epoch 6/300, Loss: 0.1146 | 0.1322
Epoch 7/300, Loss: 0.1095 | 0.1246
Epoch 8/300, Loss: 0.1037 | 0.1043
Epoch 9/300, Loss: 0.0989 | 0.0895
Epoch 10/300, Loss: 0.0959 | 0.0747
Epoch 11/300, Loss: 0.0943 | 0.0651
Epoch 12/300, Loss: 0.0921 | 0.0598
Epoch 13/300, Loss: 0.0894 | 0.0576
Epoch 14/300, Loss: 0.0867 | 0.0575
Epoch 15/300, Loss: 0.0845 | 0.0580
Epoch 16/300, Loss: 0.0833 | 0.0588
Epoch 17/300, Loss: 0.0823 | 0.0600
Epoch 18/300, Loss: 0.0814 | 0.0596
Epoch 19/300, Loss: 0.0788 | 0.0591
Epoch 20/300, Loss: 0.0761 | 0.0583
Epoch 21/300, Loss: 0.0745 | 0.0562
Epoch 22/300, Loss: 0.0738 | 0.0538
Epoch 23/300, Loss: 0.0740 | 0.0510
Epoch 24/300, Loss: 0.0738 | 0.0471
Epoch 25/300, Loss: 0.0731 | 0.0452
Epoch 26/300, Loss: 0.0720 | 0.0441
Epoch 27/300, Loss: 0.0708 | 0.0431
Epoch 28/300, Loss: 0.0697 | 0.0420
Epoch 29/300, Loss: 0.0686 | 0.0410
Epoch 30/300, Loss: 0.0678 | 0.0404
Epoch 31/300, Loss: 0.0676 | 0.0402
Epoch 32/300, Loss: 0.0680 | 0.0402
Epoch 33/300, Loss: 0.0687 | 0.0386
Epoch 34/300, Loss: 0.0673 | 0.0371
Epoch 35/300, Loss: 0.0656 | 0.0367
Epoch 36/300, Loss: 0.0648 | 0.0365
Epoch 37/300, Loss: 0.0647 | 0.0364
Epoch 38/300, Loss: 0.0641 | 0.0363
Epoch 39/300, Loss: 0.0633 | 0.0365
Epoch 40/300, Loss: 0.0626 | 0.0369
Epoch 41/300, Loss: 0.0620 | 0.0370
Epoch 42/300, Loss: 0.0614 | 0.0364
Epoch 43/300, Loss: 0.0608 | 0.0354
Epoch 44/300, Loss: 0.0605 | 0.0347
Epoch 45/300, Loss: 0.0605 | 0.0340
Epoch 46/300, Loss: 0.0604 | 0.0335
Epoch 47/300, Loss: 0.0600 | 0.0337
Epoch 48/300, Loss: 0.0596 | 0.0341
Epoch 49/300, Loss: 0.0592 | 0.0347
Epoch 50/300, Loss: 0.0590 | 0.0356
Epoch 51/300, Loss: 0.0588 | 0.0369
Epoch 52/300, Loss: 0.0590 | 0.0381
Epoch 53/300, Loss: 0.0592 | 0.0367
Epoch 54/300, Loss: 0.0592 | 0.0351
Epoch 55/300, Loss: 0.0594 | 0.0362
Epoch 56/300, Loss: 0.0591 | 0.0408
Epoch 57/300, Loss: 0.0586 | 0.0364
Epoch 58/300, Loss: 0.0581 | 0.0347
Epoch 59/300, Loss: 0.0581 | 0.0337
Epoch 60/300, Loss: 0.0585 | 0.0334
Epoch 61/300, Loss: 0.0584 | 0.0326
Epoch 62/300, Loss: 0.0581 | 0.0319
Epoch 63/300, Loss: 0.0577 | 0.0318
Epoch 64/300, Loss: 0.0576 | 0.0312
Epoch 65/300, Loss: 0.0577 | 0.0306
Epoch 66/300, Loss: 0.0575 | 0.0299
Epoch 67/300, Loss: 0.0568 | 0.0296
Epoch 68/300, Loss: 0.0561 | 0.0298
Epoch 69/300, Loss: 0.0556 | 0.0299
Epoch 70/300, Loss: 0.0555 | 0.0299
Epoch 71/300, Loss: 0.0571 | 0.0315
Epoch 72/300, Loss: 0.0601 | 0.0351
Epoch 73/300, Loss: 0.0595 | 0.0327
Epoch 74/300, Loss: 0.0567 | 0.0301
Epoch 75/300, Loss: 0.0590 | 0.0421
Epoch 76/300, Loss: 0.0583 | 0.0367
Epoch 77/300, Loss: 0.0568 | 0.0349
Epoch 78/300, Loss: 0.0567 | 0.0325
Epoch 79/300, Loss: 0.0567 | 0.0303
Epoch 80/300, Loss: 0.0567 | 0.0288
Epoch 81/300, Loss: 0.0561 | 0.0280
Epoch 82/300, Loss: 0.0550 | 0.0278
Epoch 83/300, Loss: 0.0543 | 0.0278
Epoch 84/300, Loss: 0.0538 | 0.0281
Epoch 85/300, Loss: 0.0534 | 0.0284
Epoch 86/300, Loss: 0.0533 | 0.0286
Epoch 87/300, Loss: 0.0535 | 0.0285
Epoch 88/300, Loss: 0.0543 | 0.0289
Epoch 89/300, Loss: 0.0577 | 0.0363
Epoch 90/300, Loss: 0.0601 | 0.0427
Epoch 91/300, Loss: 0.0567 | 0.0473
Epoch 92/300, Loss: 0.0597 | 0.0501
Epoch 93/300, Loss: 0.0608 | 0.0316
Epoch 94/300, Loss: 0.0578 | 0.0308
Epoch 95/300, Loss: 0.0576 | 0.0302
Epoch 96/300, Loss: 0.0555 | 0.0299
Epoch 97/300, Loss: 0.0543 | 0.0302
Epoch 98/300, Loss: 0.0539 | 0.0306
Epoch 99/300, Loss: 0.0541 | 0.0304
Epoch 100/300, Loss: 0.0539 | 0.0293
Epoch 101/300, Loss: 0.0535 | 0.0284
Epoch 102/300, Loss: 0.0574 | 0.0316
Epoch 103/300, Loss: 0.0553 | 0.0285
Epoch 104/300, Loss: 0.0541 | 0.0280
Epoch 105/300, Loss: 0.0538 | 0.0278
Epoch 106/300, Loss: 0.0534 | 0.0277
Epoch 107/300, Loss: 0.0531 | 0.0276
Epoch 108/300, Loss: 0.0528 | 0.0275
Epoch 109/300, Loss: 0.0527 | 0.0273
Epoch 110/300, Loss: 0.0526 | 0.0272
Epoch 111/300, Loss: 0.0525 | 0.0271
Epoch 112/300, Loss: 0.0524 | 0.0270
Epoch 113/300, Loss: 0.0523 | 0.0271
Epoch 114/300, Loss: 0.0522 | 0.0272
Epoch 115/300, Loss: 0.0522 | 0.0272
Epoch 116/300, Loss: 0.0521 | 0.0272
Epoch 117/300, Loss: 0.0520 | 0.0273
Epoch 118/300, Loss: 0.0518 | 0.0273
Epoch 119/300, Loss: 0.0517 | 0.0272
Epoch 120/300, Loss: 0.0516 | 0.0272
Epoch 121/300, Loss: 0.0514 | 0.0270
Epoch 122/300, Loss: 0.0513 | 0.0268
Epoch 123/300, Loss: 0.0511 | 0.0267
Epoch 124/300, Loss: 0.0510 | 0.0267
Epoch 125/300, Loss: 0.0508 | 0.0266
Epoch 126/300, Loss: 0.0507 | 0.0265
Epoch 127/300, Loss: 0.0505 | 0.0266
Epoch 128/300, Loss: 0.0504 | 0.0266
Epoch 129/300, Loss: 0.0503 | 0.0266
Epoch 130/300, Loss: 0.0502 | 0.0266
Epoch 131/300, Loss: 0.0501 | 0.0267
Epoch 132/300, Loss: 0.0500 | 0.0267
Epoch 133/300, Loss: 0.0500 | 0.0265
Epoch 134/300, Loss: 0.0499 | 0.0264
Epoch 135/300, Loss: 0.0499 | 0.0263
Epoch 136/300, Loss: 0.0498 | 0.0263
Epoch 137/300, Loss: 0.0497 | 0.0263
Epoch 138/300, Loss: 0.0497 | 0.0263
Epoch 139/300, Loss: 0.0496 | 0.0264
Epoch 140/300, Loss: 0.0497 | 0.0264
Epoch 141/300, Loss: 0.0498 | 0.0266
Epoch 142/300, Loss: 0.0499 | 0.0270
Epoch 143/300, Loss: 0.0500 | 0.0273
Epoch 144/300, Loss: 0.0500 | 0.0273
Epoch 145/300, Loss: 0.0501 | 0.0271
Epoch 146/300, Loss: 0.0502 | 0.0270
Epoch 147/300, Loss: 0.0502 | 0.0272
Epoch 148/300, Loss: 0.0499 | 0.0278
Epoch 149/300, Loss: 0.0497 | 0.0283
Epoch 150/300, Loss: 0.0496 | 0.0286
Epoch 151/300, Loss: 0.0495 | 0.0286
Epoch 152/300, Loss: 0.0496 | 0.0282
Epoch 153/300, Loss: 0.0497 | 0.0282
Epoch 154/300, Loss: 0.0497 | 0.0295
Epoch 155/300, Loss: 0.0502 | 0.0289
Epoch 156/300, Loss: 0.0505 | 0.0280
Epoch 157/300, Loss: 0.0512 | 0.0284
Epoch 158/300, Loss: 0.0513 | 0.0279
Epoch 159/300, Loss: 0.0502 | 0.0280
Epoch 160/300, Loss: 0.0493 | 0.0276
Epoch 161/300, Loss: 0.0488 | 0.0272
Epoch 162/300, Loss: 0.0487 | 0.0271
Epoch 163/300, Loss: 0.0487 | 0.0269
Epoch 164/300, Loss: 0.0487 | 0.0270
Epoch 165/300, Loss: 0.0488 | 0.0271
Epoch 166/300, Loss: 0.0488 | 0.0275
Epoch 167/300, Loss: 0.0487 | 0.0275
Epoch 168/300, Loss: 0.0487 | 0.0274
Epoch 169/300, Loss: 0.0486 | 0.0278
Epoch 170/300, Loss: 0.0484 | 0.0277
Epoch 171/300, Loss: 0.0483 | 0.0276
Epoch 172/300, Loss: 0.0482 | 0.0276
Epoch 173/300, Loss: 0.0481 | 0.0281
Epoch 174/300, Loss: 0.0480 | 0.0280
Epoch 175/300, Loss: 0.0480 | 0.0274
Epoch 176/300, Loss: 0.0480 | 0.0269
Epoch 177/300, Loss: 0.0480 | 0.0266
Epoch 178/300, Loss: 0.0481 | 0.0265
Epoch 179/300, Loss: 0.0481 | 0.0265
Epoch 180/300, Loss: 0.0481 | 0.0265
Epoch 181/300, Loss: 0.0480 | 0.0266
Epoch 182/300, Loss: 0.0480 | 0.0269
Epoch 183/300, Loss: 0.0479 | 0.0273
Epoch 184/300, Loss: 0.0479 | 0.0278
Epoch 185/300, Loss: 0.0482 | 0.0278
Epoch 186/300, Loss: 0.0483 | 0.0267
Epoch 187/300, Loss: 0.0482 | 0.0263
Epoch 188/300, Loss: 0.0481 | 0.0264
Epoch 189/300, Loss: 0.0480 | 0.0264
Epoch 190/300, Loss: 0.0479 | 0.0264
Epoch 191/300, Loss: 0.0479 | 0.0264
Epoch 192/300, Loss: 0.0478 | 0.0264
Epoch 193/300, Loss: 0.0478 | 0.0265
Epoch 194/300, Loss: 0.0477 | 0.0266
Epoch 195/300, Loss: 0.0477 | 0.0266
Epoch 196/300, Loss: 0.0476 | 0.0267
Epoch 197/300, Loss: 0.0476 | 0.0268
Epoch 198/300, Loss: 0.0475 | 0.0268
Epoch 199/300, Loss: 0.0475 | 0.0268
Epoch 200/300, Loss: 0.0474 | 0.0268
Epoch 201/300, Loss: 0.0474 | 0.0270
Epoch 202/300, Loss: 0.0474 | 0.0270
Epoch 203/300, Loss: 0.0474 | 0.0270
Epoch 204/300, Loss: 0.0473 | 0.0272
Epoch 205/300, Loss: 0.0472 | 0.0271
Epoch 206/300, Loss: 0.0472 | 0.0271
Epoch 207/300, Loss: 0.0471 | 0.0271
Epoch 208/300, Loss: 0.0470 | 0.0273
Epoch 209/300, Loss: 0.0470 | 0.0273
Epoch 210/300, Loss: 0.0470 | 0.0274
Epoch 211/300, Loss: 0.0471 | 0.0274
Epoch 212/300, Loss: 0.0471 | 0.0273
Epoch 213/300, Loss: 0.0471 | 0.0272
Epoch 214/300, Loss: 0.0472 | 0.0271
Epoch 215/300, Loss: 0.0472 | 0.0270
Epoch 216/300, Loss: 0.0472 | 0.0270
Epoch 217/300, Loss: 0.0471 | 0.0269
Epoch 218/300, Loss: 0.0470 | 0.0270
Epoch 219/300, Loss: 0.0469 | 0.0270
Epoch 220/300, Loss: 0.0468 | 0.0270
Epoch 221/300, Loss: 0.0467 | 0.0271
Epoch 222/300, Loss: 0.0466 | 0.0273
Epoch 223/300, Loss: 0.0467 | 0.0274
Epoch 224/300, Loss: 0.0467 | 0.0275
Epoch 225/300, Loss: 0.0467 | 0.0276
Epoch 226/300, Loss: 0.0466 | 0.0276
Epoch 227/300, Loss: 0.0465 | 0.0276
Epoch 228/300, Loss: 0.0464 | 0.0275
Epoch 229/300, Loss: 0.0463 | 0.0274
Epoch 230/300, Loss: 0.0463 | 0.0273
Epoch 231/300, Loss: 0.0463 | 0.0272
Epoch 232/300, Loss: 0.0463 | 0.0271
Epoch 233/300, Loss: 0.0463 | 0.0271
Epoch 234/300, Loss: 0.0463 | 0.0270
Epoch 235/300, Loss: 0.0462 | 0.0270
Epoch 236/300, Loss: 0.0462 | 0.0270
Epoch 237/300, Loss: 0.0461 | 0.0270
Epoch 238/300, Loss: 0.0460 | 0.0270
Epoch 239/300, Loss: 0.0459 | 0.0271
Epoch 240/300, Loss: 0.0458 | 0.0272
Epoch 241/300, Loss: 0.0458 | 0.0273
Epoch 242/300, Loss: 0.0457 | 0.0274
Epoch 243/300, Loss: 0.0457 | 0.0275
Epoch 244/300, Loss: 0.0456 | 0.0276
Epoch 245/300, Loss: 0.0456 | 0.0277
Epoch 246/300, Loss: 0.0456 | 0.0278
Epoch 247/300, Loss: 0.0455 | 0.0279
Epoch 248/300, Loss: 0.0455 | 0.0280
Epoch 249/300, Loss: 0.0454 | 0.0281
Epoch 250/300, Loss: 0.0454 | 0.0282
Epoch 251/300, Loss: 0.0454 | 0.0283
Epoch 252/300, Loss: 0.0453 | 0.0284
Epoch 253/300, Loss: 0.0452 | 0.0285
Epoch 254/300, Loss: 0.0452 | 0.0285
Epoch 255/300, Loss: 0.0451 | 0.0285
Epoch 256/300, Loss: 0.0450 | 0.0285
Epoch 257/300, Loss: 0.0449 | 0.0286
Epoch 258/300, Loss: 0.0448 | 0.0287
Epoch 259/300, Loss: 0.0447 | 0.0288
Epoch 260/300, Loss: 0.0446 | 0.0289
Epoch 261/300, Loss: 0.0445 | 0.0290
Epoch 262/300, Loss: 0.0444 | 0.0292
Epoch 263/300, Loss: 0.0443 | 0.0293
Epoch 264/300, Loss: 0.0442 | 0.0295
Epoch 265/300, Loss: 0.0441 | 0.0296
Epoch 266/300, Loss: 0.0440 | 0.0298
Epoch 267/300, Loss: 0.0439 | 0.0300
Epoch 268/300, Loss: 0.0438 | 0.0301
Epoch 269/300, Loss: 0.0437 | 0.0303
Epoch 270/300, Loss: 0.0436 | 0.0305
Epoch 271/300, Loss: 0.0435 | 0.0307
Epoch 272/300, Loss: 0.0434 | 0.0309
Epoch 273/300, Loss: 0.0433 | 0.0311
Epoch 274/300, Loss: 0.0431 | 0.0313
Epoch 275/300, Loss: 0.0430 | 0.0315
Epoch 276/300, Loss: 0.0429 | 0.0317
Epoch 277/300, Loss: 0.0428 | 0.0319
Epoch 278/300, Loss: 0.0427 | 0.0320
Epoch 279/300, Loss: 0.0425 | 0.0322
Epoch 280/300, Loss: 0.0424 | 0.0324
Epoch 281/300, Loss: 0.0423 | 0.0325
Epoch 282/300, Loss: 0.0422 | 0.0326
Epoch 283/300, Loss: 0.0421 | 0.0327
Epoch 284/300, Loss: 0.0420 | 0.0328
Epoch 285/300, Loss: 0.0419 | 0.0329
Epoch 286/300, Loss: 0.0418 | 0.0330
Epoch 287/300, Loss: 0.0417 | 0.0331
Epoch 288/300, Loss: 0.0416 | 0.0332
Epoch 289/300, Loss: 0.0415 | 0.0333
Epoch 290/300, Loss: 0.0414 | 0.0334
Epoch 291/300, Loss: 0.0413 | 0.0335
Epoch 292/300, Loss: 0.0412 | 0.0336
Epoch 293/300, Loss: 0.0411 | 0.0336
Epoch 294/300, Loss: 0.0411 | 0.0337
Epoch 295/300, Loss: 0.0410 | 0.0338
Epoch 296/300, Loss: 0.0409 | 0.0339
Epoch 297/300, Loss: 0.0408 | 0.0340
Epoch 298/300, Loss: 0.0407 | 0.0341
Epoch 299/300, Loss: 0.0407 | 0.0342
Epoch 300/300, Loss: 0.0406 | 0.0343
Runtime (seconds): 387.34168124198914
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 34.32351406686939
RMSE: 5.8586273193359375
MAE: 5.8586273193359375
R-squared: nan
[191.69138]
