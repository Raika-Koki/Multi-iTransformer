[32m[I 2025-02-07 18:51:01,367][0m A new study created in memory with name: no-name-cce99a41-c35b-42c6-a540-d8f69f1f2743[0m
[32m[I 2025-02-07 18:51:28,296][0m Trial 0 finished with value: 0.4956054217512275 and parameters: {'observation_period_num': 60, 'train_rates': 0.7218173012487971, 'learning_rate': 1.0228312811474241e-05, 'batch_size': 209, 'step_size': 11, 'gamma': 0.7694277476251573}. Best is trial 0 with value: 0.4956054217512275.[0m
[32m[I 2025-02-07 18:51:56,689][0m Trial 1 finished with value: 0.25105889246714097 and parameters: {'observation_period_num': 125, 'train_rates': 0.7979541836435295, 'learning_rate': 2.595479422477375e-05, 'batch_size': 208, 'step_size': 11, 'gamma': 0.9136847527657377}. Best is trial 1 with value: 0.25105889246714097.[0m
[32m[I 2025-02-07 18:52:26,488][0m Trial 2 finished with value: 2.0243701934814453 and parameters: {'observation_period_num': 121, 'train_rates': 0.9601737926218976, 'learning_rate': 1.5610296027508797e-06, 'batch_size': 226, 'step_size': 2, 'gamma': 0.8416076232143541}. Best is trial 1 with value: 0.25105889246714097.[0m
[32m[I 2025-02-07 18:53:12,783][0m Trial 3 finished with value: 0.08486886091624238 and parameters: {'observation_period_num': 120, 'train_rates': 0.8347042683440798, 'learning_rate': 0.0003110545873607014, 'batch_size': 123, 'step_size': 14, 'gamma': 0.8083199160167625}. Best is trial 3 with value: 0.08486886091624238.[0m
[32m[I 2025-02-07 18:53:49,948][0m Trial 4 finished with value: 0.8616748836593352 and parameters: {'observation_period_num': 184, 'train_rates': 0.8531179725918159, 'learning_rate': 1.0628888087164637e-06, 'batch_size': 151, 'step_size': 9, 'gamma': 0.9158970239659773}. Best is trial 3 with value: 0.08486886091624238.[0m
[32m[I 2025-02-07 18:54:25,635][0m Trial 5 finished with value: 0.24299763856587683 and parameters: {'observation_period_num': 128, 'train_rates': 0.6464284155206965, 'learning_rate': 0.000150957535834999, 'batch_size': 137, 'step_size': 7, 'gamma': 0.9477073131128313}. Best is trial 3 with value: 0.08486886091624238.[0m
[32m[I 2025-02-07 18:59:21,925][0m Trial 6 finished with value: 0.11595507965672135 and parameters: {'observation_period_num': 52, 'train_rates': 0.8974011754967024, 'learning_rate': 0.00012500473680572983, 'batch_size': 19, 'step_size': 5, 'gamma': 0.9598690677147241}. Best is trial 3 with value: 0.08486886091624238.[0m
[32m[I 2025-02-07 19:00:39,655][0m Trial 7 finished with value: 0.46286193797321135 and parameters: {'observation_period_num': 10, 'train_rates': 0.6152026072827417, 'learning_rate': 1.5373946727651738e-06, 'batch_size': 58, 'step_size': 12, 'gamma': 0.8779619688518308}. Best is trial 3 with value: 0.08486886091624238.[0m
[32m[I 2025-02-07 19:01:22,809][0m Trial 8 finished with value: 0.7168152281570555 and parameters: {'observation_period_num': 53, 'train_rates': 0.7309399995916648, 'learning_rate': 1.0166917126144063e-06, 'batch_size': 125, 'step_size': 10, 'gamma': 0.8595346878059936}. Best is trial 3 with value: 0.08486886091624238.[0m
[32m[I 2025-02-07 19:01:53,698][0m Trial 9 finished with value: 0.4990517047348318 and parameters: {'observation_period_num': 222, 'train_rates': 0.639658330170426, 'learning_rate': 1.684503696807874e-05, 'batch_size': 159, 'step_size': 11, 'gamma': 0.7524946325889608}. Best is trial 3 with value: 0.08486886091624238.[0m
[32m[I 2025-02-07 19:03:05,521][0m Trial 10 finished with value: 0.22249696217477322 and parameters: {'observation_period_num': 176, 'train_rates': 0.9691611902071908, 'learning_rate': 0.0007411018189850256, 'batch_size': 85, 'step_size': 15, 'gamma': 0.8052627809240481}. Best is trial 3 with value: 0.08486886091624238.[0m
[32m[I 2025-02-07 19:04:41,899][0m Trial 11 finished with value: 0.17791761019139526 and parameters: {'observation_period_num': 75, 'train_rates': 0.8830249567706939, 'learning_rate': 0.0001830323664931131, 'batch_size': 58, 'step_size': 5, 'gamma': 0.9803986157861144}. Best is trial 3 with value: 0.08486886091624238.[0m
[32m[I 2025-02-07 19:08:27,601][0m Trial 12 finished with value: 0.03684761902035842 and parameters: {'observation_period_num': 19, 'train_rates': 0.884812696534404, 'learning_rate': 0.00014146465208298806, 'batch_size': 25, 'step_size': 4, 'gamma': 0.819942617057621}. Best is trial 12 with value: 0.03684761902035842.[0m
[32m[I 2025-02-07 19:11:40,414][0m Trial 13 finished with value: 0.04330203449442954 and parameters: {'observation_period_num': 13, 'train_rates': 0.8256165392661482, 'learning_rate': 0.0009531914418291985, 'batch_size': 28, 'step_size': 15, 'gamma': 0.821295273278918}. Best is trial 12 with value: 0.03684761902035842.[0m
Early stopping at epoch 99
[32m[I 2025-02-07 19:17:21,597][0m Trial 14 finished with value: 0.03444351116195321 and parameters: {'observation_period_num': 5, 'train_rates': 0.920630777981981, 'learning_rate': 0.0008570542032317673, 'batch_size': 17, 'step_size': 1, 'gamma': 0.8039037682692219}. Best is trial 14 with value: 0.03444351116195321.[0m
Early stopping at epoch 68
[32m[I 2025-02-07 19:18:20,496][0m Trial 15 finished with value: 0.1340478805359453 and parameters: {'observation_period_num': 5, 'train_rates': 0.9145748008379828, 'learning_rate': 6.394999605526932e-05, 'batch_size': 72, 'step_size': 1, 'gamma': 0.788518898900653}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:20:56,219][0m Trial 16 finished with value: 0.12764775100271258 and parameters: {'observation_period_num': 87, 'train_rates': 0.9226490608353812, 'learning_rate': 0.00036626884296348823, 'batch_size': 37, 'step_size': 3, 'gamma': 0.8356280417477261}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:21:47,995][0m Trial 17 finished with value: 0.20497604070529754 and parameters: {'observation_period_num': 32, 'train_rates': 0.7560255617673926, 'learning_rate': 5.919057083435837e-05, 'batch_size': 102, 'step_size': 4, 'gamma': 0.8827885406417665}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:24:09,851][0m Trial 18 finished with value: 0.574567437171936 and parameters: {'observation_period_num': 88, 'train_rates': 0.9860840941409383, 'learning_rate': 5.963836250365239e-06, 'batch_size': 43, 'step_size': 7, 'gamma': 0.7820848650324326}. Best is trial 14 with value: 0.03444351116195321.[0m
Early stopping at epoch 79
[32m[I 2025-02-07 19:24:30,469][0m Trial 19 finished with value: 0.2908957600593567 and parameters: {'observation_period_num': 30, 'train_rates': 0.9328255977092819, 'learning_rate': 0.0004350248273542926, 'batch_size': 254, 'step_size': 1, 'gamma': 0.8455740855005219}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:25:27,825][0m Trial 20 finished with value: 0.23349615208784305 and parameters: {'observation_period_num': 166, 'train_rates': 0.8706972657879203, 'learning_rate': 8.954706960796843e-05, 'batch_size': 95, 'step_size': 3, 'gamma': 0.8044498696912872}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:28:51,915][0m Trial 21 finished with value: 0.04966799260231016 and parameters: {'observation_period_num': 29, 'train_rates': 0.8130062845371391, 'learning_rate': 0.0009532114226234844, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8228450889466218}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:33:51,743][0m Trial 22 finished with value: 0.16853878385831364 and parameters: {'observation_period_num': 6, 'train_rates': 0.7702178678231201, 'learning_rate': 0.0006833660929421061, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8217705029379117}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:35:40,926][0m Trial 23 finished with value: 0.15620381894564211 and parameters: {'observation_period_num': 34, 'train_rates': 0.8460953508678184, 'learning_rate': 0.00030131478581149433, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8587077784041439}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:36:57,769][0m Trial 24 finished with value: 0.20675101202644713 and parameters: {'observation_period_num': 251, 'train_rates': 0.9468806050821077, 'learning_rate': 0.0006194057783562529, 'batch_size': 74, 'step_size': 2, 'gamma': 0.785769750398423}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:39:46,849][0m Trial 25 finished with value: 0.17289935579558588 and parameters: {'observation_period_num': 100, 'train_rates': 0.8903837672099083, 'learning_rate': 0.0002402591450844461, 'batch_size': 33, 'step_size': 8, 'gamma': 0.7597657036594737}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:41:42,491][0m Trial 26 finished with value: 0.06880394224134556 and parameters: {'observation_period_num': 24, 'train_rates': 0.8246038099981671, 'learning_rate': 0.00047885128394387766, 'batch_size': 47, 'step_size': 3, 'gamma': 0.8965988795094524}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:42:57,439][0m Trial 27 finished with value: 0.20824393775404954 and parameters: {'observation_period_num': 62, 'train_rates': 0.7766559324638933, 'learning_rate': 0.000999701478246963, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8242571825702389}. Best is trial 14 with value: 0.03444351116195321.[0m
Early stopping at epoch 56
[32m[I 2025-02-07 19:43:17,934][0m Trial 28 finished with value: 0.7328000454704996 and parameters: {'observation_period_num': 156, 'train_rates': 0.864401537686516, 'learning_rate': 4.465739635137097e-05, 'batch_size': 174, 'step_size': 1, 'gamma': 0.7947827657598856}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:44:05,961][0m Trial 29 finished with value: 0.1645033697385206 and parameters: {'observation_period_num': 47, 'train_rates': 0.6823874663090351, 'learning_rate': 9.954841373556464e-05, 'batch_size': 103, 'step_size': 9, 'gamma': 0.7733884789256597}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:47:14,869][0m Trial 30 finished with value: 0.11883121898726505 and parameters: {'observation_period_num': 71, 'train_rates': 0.9071381510923843, 'learning_rate': 7.983352970372709e-06, 'batch_size': 30, 'step_size': 15, 'gamma': 0.8571787927061607}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:51:40,853][0m Trial 31 finished with value: 0.050128022116989315 and parameters: {'observation_period_num': 20, 'train_rates': 0.8094940140600629, 'learning_rate': 0.0008692811162728835, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8233057694407878}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:54:34,272][0m Trial 32 finished with value: 0.09092601557300325 and parameters: {'observation_period_num': 40, 'train_rates': 0.8182365850942295, 'learning_rate': 0.0005432692093129555, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8223646249466121}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:56:21,324][0m Trial 33 finished with value: 0.04621562466164616 and parameters: {'observation_period_num': 19, 'train_rates': 0.7848610049059874, 'learning_rate': 0.0002061800281476609, 'batch_size': 49, 'step_size': 4, 'gamma': 0.7697453988603666}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 19:57:43,660][0m Trial 34 finished with value: 0.1625141689496837 and parameters: {'observation_period_num': 17, 'train_rates': 0.7372544950592654, 'learning_rate': 0.00018480916526832496, 'batch_size': 62, 'step_size': 4, 'gamma': 0.7659146863896926}. Best is trial 14 with value: 0.03444351116195321.[0m
Early stopping at epoch 88
[32m[I 2025-02-07 19:58:11,181][0m Trial 35 finished with value: 0.5746993084107676 and parameters: {'observation_period_num': 45, 'train_rates': 0.7902850417189013, 'learning_rate': 2.3289608783530508e-05, 'batch_size': 192, 'step_size': 2, 'gamma': 0.7722840564689661}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:00:21,232][0m Trial 36 finished with value: 0.0947838828433305 and parameters: {'observation_period_num': 63, 'train_rates': 0.8475274908596536, 'learning_rate': 0.0002502106876515367, 'batch_size': 42, 'step_size': 4, 'gamma': 0.807880965034932}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:01:18,442][0m Trial 37 finished with value: 0.8268498244328736 and parameters: {'observation_period_num': 18, 'train_rates': 0.7036498377887335, 'learning_rate': 4.116522733693251e-06, 'batch_size': 88, 'step_size': 2, 'gamma': 0.8424029384512977}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:05:58,760][0m Trial 38 finished with value: 0.18976273614773775 and parameters: {'observation_period_num': 143, 'train_rates': 0.8692551733787363, 'learning_rate': 0.0003538469866238416, 'batch_size': 19, 'step_size': 13, 'gamma': 0.7978292757962271}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:06:51,532][0m Trial 39 finished with value: 0.1343885131362008 and parameters: {'observation_period_num': 106, 'train_rates': 0.9434600881280031, 'learning_rate': 0.00015777438800136965, 'batch_size': 116, 'step_size': 10, 'gamma': 0.779332312006771}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:08:18,208][0m Trial 40 finished with value: 0.06119906841321321 and parameters: {'observation_period_num': 6, 'train_rates': 0.7919792033064319, 'learning_rate': 3.7860581809315244e-05, 'batch_size': 61, 'step_size': 6, 'gamma': 0.7504020933605416}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:11:29,832][0m Trial 41 finished with value: 0.12221479346233655 and parameters: {'observation_period_num': 28, 'train_rates': 0.8346041923789611, 'learning_rate': 0.0005949559132550645, 'batch_size': 28, 'step_size': 8, 'gamma': 0.8149308318959061}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:16:32,520][0m Trial 42 finished with value: 0.054030687307470106 and parameters: {'observation_period_num': 40, 'train_rates': 0.8013655719181493, 'learning_rate': 0.0008656490687189552, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8307283152986322}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:18:19,621][0m Trial 43 finished with value: 0.1709723596974295 and parameters: {'observation_period_num': 17, 'train_rates': 0.7536412365335723, 'learning_rate': 0.00045152749026685355, 'batch_size': 47, 'step_size': 9, 'gamma': 0.848495893565548}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:21:14,635][0m Trial 44 finished with value: 0.06697442509492489 and parameters: {'observation_period_num': 54, 'train_rates': 0.8924023492375828, 'learning_rate': 0.0002745889429358121, 'batch_size': 32, 'step_size': 4, 'gamma': 0.7931663311333244}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:23:03,441][0m Trial 45 finished with value: 0.035374182717387945 and parameters: {'observation_period_num': 15, 'train_rates': 0.8296176009987651, 'learning_rate': 0.00011129406326209547, 'batch_size': 50, 'step_size': 11, 'gamma': 0.8077811047327556}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:24:46,072][0m Trial 46 finished with value: 0.03520722111799365 and parameters: {'observation_period_num': 13, 'train_rates': 0.8391612796840848, 'learning_rate': 0.00011632558529742019, 'batch_size': 54, 'step_size': 12, 'gamma': 0.8115901109187524}. Best is trial 14 with value: 0.03444351116195321.[0m
[32m[I 2025-02-07 20:25:59,726][0m Trial 47 finished with value: 0.03263381775252504 and parameters: {'observation_period_num': 13, 'train_rates': 0.8558338432142012, 'learning_rate': 0.0001242803082996049, 'batch_size': 76, 'step_size': 14, 'gamma': 0.936307196704028}. Best is trial 47 with value: 0.03263381775252504.[0m
[32m[I 2025-02-07 20:27:07,713][0m Trial 48 finished with value: 0.06796444830569354 and parameters: {'observation_period_num': 37, 'train_rates': 0.851584254128102, 'learning_rate': 0.00010724505370083593, 'batch_size': 82, 'step_size': 12, 'gamma': 0.9443639942712736}. Best is trial 47 with value: 0.03263381775252504.[0m
[32m[I 2025-02-07 20:28:32,103][0m Trial 49 finished with value: 0.22349163349661583 and parameters: {'observation_period_num': 211, 'train_rates': 0.8815929514337503, 'learning_rate': 7.440786428027885e-05, 'batch_size': 66, 'step_size': 13, 'gamma': 0.9210227553482235}. Best is trial 47 with value: 0.03263381775252504.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2708 | 0.2495
Epoch 2/300, Loss: 0.1828 | 0.2033
Epoch 3/300, Loss: 0.1441 | 0.1375
Epoch 4/300, Loss: 0.1407 | 0.1178
Epoch 5/300, Loss: 0.1322 | 0.1140
Epoch 6/300, Loss: 0.1320 | 0.1044
Epoch 7/300, Loss: 0.1423 | 0.1172
Epoch 8/300, Loss: 0.1497 | 0.2921
Epoch 9/300, Loss: 0.1407 | 0.1792
Epoch 10/300, Loss: 0.1203 | 0.1111
Epoch 11/300, Loss: 0.1168 | 0.0905
Epoch 12/300, Loss: 0.1223 | 0.0876
Epoch 13/300, Loss: 0.1161 | 0.0805
Epoch 14/300, Loss: 0.1080 | 0.0761
Epoch 15/300, Loss: 0.1042 | 0.0734
Epoch 16/300, Loss: 0.1032 | 0.0713
Epoch 17/300, Loss: 0.1032 | 0.0699
Epoch 18/300, Loss: 0.1027 | 0.0690
Epoch 19/300, Loss: 0.1017 | 0.0689
Epoch 20/300, Loss: 0.1008 | 0.0694
Epoch 21/300, Loss: 0.1001 | 0.0693
Epoch 22/300, Loss: 0.0992 | 0.0694
Epoch 23/300, Loss: 0.0972 | 0.0648
Epoch 24/300, Loss: 0.0936 | 0.0610
Epoch 25/300, Loss: 0.0918 | 0.0580
Epoch 26/300, Loss: 0.0928 | 0.0566
Epoch 27/300, Loss: 0.0967 | 0.0652
Epoch 28/300, Loss: 0.1019 | 0.0881
Epoch 29/300, Loss: 0.1024 | 0.1062
Epoch 30/300, Loss: 0.0953 | 0.0780
Epoch 31/300, Loss: 0.0878 | 0.0557
Epoch 32/300, Loss: 0.0882 | 0.0529
Epoch 33/300, Loss: 0.0951 | 0.0604
Epoch 34/300, Loss: 0.1016 | 0.0617
Epoch 35/300, Loss: 0.0926 | 0.0511
Epoch 36/300, Loss: 0.0862 | 0.0477
Epoch 37/300, Loss: 0.0832 | 0.0481
Epoch 38/300, Loss: 0.0826 | 0.0567
Epoch 39/300, Loss: 0.0831 | 0.0612
Epoch 40/300, Loss: 0.0828 | 0.0565
Epoch 41/300, Loss: 0.0828 | 0.0520
Epoch 42/300, Loss: 0.0813 | 0.0476
Epoch 43/300, Loss: 0.0788 | 0.0446
Epoch 44/300, Loss: 0.0781 | 0.0440
Epoch 45/300, Loss: 0.0786 | 0.0448
Epoch 46/300, Loss: 0.0805 | 0.0464
Epoch 47/300, Loss: 0.0843 | 0.0524
Epoch 48/300, Loss: 0.0867 | 0.0491
Epoch 49/300, Loss: 0.0796 | 0.0448
Epoch 50/300, Loss: 0.0790 | 0.0479
Epoch 51/300, Loss: 0.0796 | 0.0560
Epoch 52/300, Loss: 0.0797 | 0.0535
Epoch 53/300, Loss: 0.0776 | 0.0495
Epoch 54/300, Loss: 0.0819 | 0.0516
Epoch 55/300, Loss: 0.0810 | 0.0505
Epoch 56/300, Loss: 0.0761 | 0.0455
Epoch 57/300, Loss: 0.0751 | 0.0429
Epoch 58/300, Loss: 0.0744 | 0.0406
Epoch 59/300, Loss: 0.0739 | 0.0398
Epoch 60/300, Loss: 0.0740 | 0.0400
Epoch 61/300, Loss: 0.0739 | 0.0406
Epoch 62/300, Loss: 0.0735 | 0.0414
Epoch 63/300, Loss: 0.0733 | 0.0421
Epoch 64/300, Loss: 0.0734 | 0.0427
Epoch 65/300, Loss: 0.0734 | 0.0412
Epoch 66/300, Loss: 0.0735 | 0.0463
Epoch 67/300, Loss: 0.0731 | 0.0426
Epoch 68/300, Loss: 0.0720 | 0.0416
Epoch 69/300, Loss: 0.0712 | 0.0416
Epoch 70/300, Loss: 0.0714 | 0.0415
Epoch 71/300, Loss: 0.0717 | 0.0405
Epoch 72/300, Loss: 0.0716 | 0.0396
Epoch 73/300, Loss: 0.0708 | 0.0389
Epoch 74/300, Loss: 0.0706 | 0.0389
Epoch 75/300, Loss: 0.0711 | 0.0392
Epoch 76/300, Loss: 0.0717 | 0.0400
Epoch 77/300, Loss: 0.0715 | 0.0409
Epoch 78/300, Loss: 0.0703 | 0.0417
Epoch 79/300, Loss: 0.0693 | 0.0421
Epoch 80/300, Loss: 0.0696 | 0.0430
Epoch 81/300, Loss: 0.0713 | 0.0436
Epoch 82/300, Loss: 0.0742 | 0.0445
Epoch 83/300, Loss: 0.0784 | 0.0475
Epoch 84/300, Loss: 0.0812 | 0.0469
Epoch 85/300, Loss: 0.0761 | 0.0446
Epoch 86/300, Loss: 0.0785 | 0.0564
Epoch 87/300, Loss: 0.0760 | 0.0491
Epoch 88/300, Loss: 0.0690 | 0.0457
Epoch 89/300, Loss: 0.0691 | 0.0464
Epoch 90/300, Loss: 0.0676 | 0.0423
Epoch 91/300, Loss: 0.0673 | 0.0406
Epoch 92/300, Loss: 0.0668 | 0.0386
Epoch 93/300, Loss: 0.0664 | 0.0371
Epoch 94/300, Loss: 0.0660 | 0.0362
Epoch 95/300, Loss: 0.0658 | 0.0357
Epoch 96/300, Loss: 0.0655 | 0.0354
Epoch 97/300, Loss: 0.0653 | 0.0352
Epoch 98/300, Loss: 0.0651 | 0.0350
Epoch 99/300, Loss: 0.0649 | 0.0350
Epoch 100/300, Loss: 0.0647 | 0.0347
Epoch 101/300, Loss: 0.0645 | 0.0343
Epoch 102/300, Loss: 0.0643 | 0.0338
Epoch 103/300, Loss: 0.0641 | 0.0333
Epoch 104/300, Loss: 0.0639 | 0.0329
Epoch 105/300, Loss: 0.0636 | 0.0325
Epoch 106/300, Loss: 0.0634 | 0.0325
Epoch 107/300, Loss: 0.0632 | 0.0323
Epoch 108/300, Loss: 0.0630 | 0.0322
Epoch 109/300, Loss: 0.0627 | 0.0321
Epoch 110/300, Loss: 0.0625 | 0.0320
Epoch 111/300, Loss: 0.0623 | 0.0319
Epoch 112/300, Loss: 0.0621 | 0.0318
Epoch 113/300, Loss: 0.0619 | 0.0319
Epoch 114/300, Loss: 0.0618 | 0.0316
Epoch 115/300, Loss: 0.0617 | 0.0315
Epoch 116/300, Loss: 0.0617 | 0.0315
Epoch 117/300, Loss: 0.0618 | 0.0317
Epoch 118/300, Loss: 0.0618 | 0.0321
Epoch 119/300, Loss: 0.0617 | 0.0327
Epoch 120/300, Loss: 0.0616 | 0.0342
Epoch 121/300, Loss: 0.0614 | 0.0344
Epoch 122/300, Loss: 0.0614 | 0.0339
Epoch 123/300, Loss: 0.0619 | 0.0322
Epoch 124/300, Loss: 0.0624 | 0.0312
Epoch 125/300, Loss: 0.0630 | 0.0314
Epoch 126/300, Loss: 0.0635 | 0.0313
Epoch 127/300, Loss: 0.0635 | 0.0322
Epoch 128/300, Loss: 0.0627 | 0.0307
Epoch 129/300, Loss: 0.0628 | 0.0298
Epoch 130/300, Loss: 0.0623 | 0.0291
Epoch 131/300, Loss: 0.0614 | 0.0292
Epoch 132/300, Loss: 0.0613 | 0.0297
Epoch 133/300, Loss: 0.0614 | 0.0307
Epoch 134/300, Loss: 0.0617 | 0.0330
Epoch 135/300, Loss: 0.0618 | 0.0320
Epoch 136/300, Loss: 0.0617 | 0.0313
Epoch 137/300, Loss: 0.0614 | 0.0311
Epoch 138/300, Loss: 0.0611 | 0.0309
Epoch 139/300, Loss: 0.0613 | 0.0306
Epoch 140/300, Loss: 0.0619 | 0.0323
Epoch 141/300, Loss: 0.0629 | 0.0312
Epoch 142/300, Loss: 0.0620 | 0.0299
Epoch 143/300, Loss: 0.0613 | 0.0309
Epoch 144/300, Loss: 0.0611 | 0.0311
Epoch 145/300, Loss: 0.0610 | 0.0308
Epoch 146/300, Loss: 0.0609 | 0.0303
Epoch 147/300, Loss: 0.0608 | 0.0299
Epoch 148/300, Loss: 0.0609 | 0.0298
Epoch 149/300, Loss: 0.0608 | 0.0297
Epoch 150/300, Loss: 0.0606 | 0.0295
Epoch 151/300, Loss: 0.0604 | 0.0292
Epoch 152/300, Loss: 0.0602 | 0.0289
Epoch 153/300, Loss: 0.0604 | 0.0289
Epoch 154/300, Loss: 0.0609 | 0.0295
Epoch 155/300, Loss: 0.0617 | 0.0311
Epoch 156/300, Loss: 0.0617 | 0.0301
Epoch 157/300, Loss: 0.0625 | 0.0390
Epoch 158/300, Loss: 0.0632 | 0.0368
Epoch 159/300, Loss: 0.0623 | 0.0318
Epoch 160/300, Loss: 0.0616 | 0.0295
Epoch 161/300, Loss: 0.0613 | 0.0289
Epoch 162/300, Loss: 0.0601 | 0.0293
Epoch 163/300, Loss: 0.0599 | 0.0301
Epoch 164/300, Loss: 0.0590 | 0.0309
Epoch 165/300, Loss: 0.0589 | 0.0314
Epoch 166/300, Loss: 0.0587 | 0.0314
Epoch 167/300, Loss: 0.0587 | 0.0313
Epoch 168/300, Loss: 0.0587 | 0.0313
Epoch 169/300, Loss: 0.0586 | 0.0312
Epoch 170/300, Loss: 0.0584 | 0.0303
Epoch 171/300, Loss: 0.0583 | 0.0292
Epoch 172/300, Loss: 0.0583 | 0.0284
Epoch 173/300, Loss: 0.0584 | 0.0282
Epoch 174/300, Loss: 0.0585 | 0.0283
Epoch 175/300, Loss: 0.0586 | 0.0287
Epoch 176/300, Loss: 0.0587 | 0.0293
Epoch 177/300, Loss: 0.0587 | 0.0295
Epoch 178/300, Loss: 0.0588 | 0.0292
Epoch 179/300, Loss: 0.0589 | 0.0289
Epoch 180/300, Loss: 0.0589 | 0.0288
Epoch 181/300, Loss: 0.0588 | 0.0289
Epoch 182/300, Loss: 0.0585 | 0.0292
Epoch 183/300, Loss: 0.0582 | 0.0295
Epoch 184/300, Loss: 0.0579 | 0.0298
Epoch 185/300, Loss: 0.0579 | 0.0300
Epoch 186/300, Loss: 0.0590 | 0.0312
Epoch 187/300, Loss: 0.0628 | 0.0367
Epoch 188/300, Loss: 0.0604 | 0.0337
Epoch 189/300, Loss: 0.0584 | 0.0307
Epoch 190/300, Loss: 0.0576 | 0.0305
Epoch 191/300, Loss: 0.0576 | 0.0303
Epoch 192/300, Loss: 0.0575 | 0.0306
Epoch 193/300, Loss: 0.0571 | 0.0311
Epoch 194/300, Loss: 0.0567 | 0.0312
Epoch 195/300, Loss: 0.0566 | 0.0304
Epoch 196/300, Loss: 0.0564 | 0.0292
Epoch 197/300, Loss: 0.0564 | 0.0283
Epoch 198/300, Loss: 0.0564 | 0.0280
Epoch 199/300, Loss: 0.0563 | 0.0282
Epoch 200/300, Loss: 0.0563 | 0.0291
Epoch 201/300, Loss: 0.0563 | 0.0299
Epoch 202/300, Loss: 0.0564 | 0.0299
Epoch 203/300, Loss: 0.0563 | 0.0292
Epoch 204/300, Loss: 0.0563 | 0.0285
Epoch 205/300, Loss: 0.0563 | 0.0283
Epoch 206/300, Loss: 0.0562 | 0.0285
Epoch 207/300, Loss: 0.0560 | 0.0291
Epoch 208/300, Loss: 0.0559 | 0.0295
Epoch 209/300, Loss: 0.0558 | 0.0293
Epoch 210/300, Loss: 0.0558 | 0.0288
Epoch 211/300, Loss: 0.0559 | 0.0284
Epoch 212/300, Loss: 0.0560 | 0.0286
Epoch 213/300, Loss: 0.0560 | 0.0293
Epoch 214/300, Loss: 0.0560 | 0.0302
Epoch 215/300, Loss: 0.0561 | 0.0307
Epoch 216/300, Loss: 0.0561 | 0.0304
Epoch 217/300, Loss: 0.0560 | 0.0299
Epoch 218/300, Loss: 0.0560 | 0.0296
Epoch 219/300, Loss: 0.0560 | 0.0295
Epoch 220/300, Loss: 0.0561 | 0.0297
Epoch 221/300, Loss: 0.0564 | 0.0300
Epoch 222/300, Loss: 0.0566 | 0.0301
Epoch 223/300, Loss: 0.0567 | 0.0299
Epoch 224/300, Loss: 0.0567 | 0.0294
Epoch 225/300, Loss: 0.0568 | 0.0291
Epoch 226/300, Loss: 0.0568 | 0.0293
Epoch 227/300, Loss: 0.0565 | 0.0300
Epoch 228/300, Loss: 0.0559 | 0.0311
Epoch 229/300, Loss: 0.0556 | 0.0313
Epoch 230/300, Loss: 0.0556 | 0.0305
Epoch 231/300, Loss: 0.0561 | 0.0299
Epoch 232/300, Loss: 0.0568 | 0.0304
Epoch 233/300, Loss: 0.0570 | 0.0312
Epoch 234/300, Loss: 0.0562 | 0.0314
Epoch 235/300, Loss: 0.0556 | 0.0312
Epoch 236/300, Loss: 0.0552 | 0.0301
Epoch 237/300, Loss: 0.0550 | 0.0290
Epoch 238/300, Loss: 0.0550 | 0.0283
Epoch 239/300, Loss: 0.0552 | 0.0285
Epoch 240/300, Loss: 0.0553 | 0.0292
Epoch 241/300, Loss: 0.0550 | 0.0301
Epoch 242/300, Loss: 0.0547 | 0.0303
Epoch 243/300, Loss: 0.0545 | 0.0299
Epoch 244/300, Loss: 0.0544 | 0.0298
Epoch 245/300, Loss: 0.0543 | 0.0300
Epoch 246/300, Loss: 0.0543 | 0.0305
Epoch 247/300, Loss: 0.0543 | 0.0307
Epoch 248/300, Loss: 0.0542 | 0.0305
Epoch 249/300, Loss: 0.0542 | 0.0301
Epoch 250/300, Loss: 0.0541 | 0.0297
Epoch 251/300, Loss: 0.0540 | 0.0295
Epoch 252/300, Loss: 0.0540 | 0.0295
Epoch 253/300, Loss: 0.0540 | 0.0296
Epoch 254/300, Loss: 0.0540 | 0.0298
Epoch 255/300, Loss: 0.0539 | 0.0298
Epoch 256/300, Loss: 0.0538 | 0.0299
Epoch 257/300, Loss: 0.0538 | 0.0300
Epoch 258/300, Loss: 0.0537 | 0.0301
Epoch 259/300, Loss: 0.0537 | 0.0303
Epoch 260/300, Loss: 0.0536 | 0.0304
Epoch 261/300, Loss: 0.0536 | 0.0304
Epoch 262/300, Loss: 0.0536 | 0.0303
Epoch 263/300, Loss: 0.0536 | 0.0302
Epoch 264/300, Loss: 0.0536 | 0.0302
Epoch 265/300, Loss: 0.0535 | 0.0301
Epoch 266/300, Loss: 0.0535 | 0.0301
Epoch 267/300, Loss: 0.0535 | 0.0300
Epoch 268/300, Loss: 0.0535 | 0.0300
Epoch 269/300, Loss: 0.0534 | 0.0300
Epoch 270/300, Loss: 0.0534 | 0.0301
Epoch 271/300, Loss: 0.0534 | 0.0302
Epoch 272/300, Loss: 0.0534 | 0.0304
Epoch 273/300, Loss: 0.0533 | 0.0305
Epoch 274/300, Loss: 0.0533 | 0.0306
Epoch 275/300, Loss: 0.0533 | 0.0307
Epoch 276/300, Loss: 0.0532 | 0.0308
Epoch 277/300, Loss: 0.0532 | 0.0309
Epoch 278/300, Loss: 0.0532 | 0.0310
Epoch 279/300, Loss: 0.0531 | 0.0310
Epoch 280/300, Loss: 0.0531 | 0.0310
Epoch 281/300, Loss: 0.0531 | 0.0310
Epoch 282/300, Loss: 0.0531 | 0.0309
Epoch 283/300, Loss: 0.0530 | 0.0307
Epoch 284/300, Loss: 0.0530 | 0.0307
Epoch 285/300, Loss: 0.0530 | 0.0307
Epoch 286/300, Loss: 0.0530 | 0.0308
Epoch 287/300, Loss: 0.0530 | 0.0309
Epoch 288/300, Loss: 0.0530 | 0.0311
Epoch 289/300, Loss: 0.0530 | 0.0312
Epoch 290/300, Loss: 0.0530 | 0.0314
Epoch 291/300, Loss: 0.0530 | 0.0315
Epoch 292/300, Loss: 0.0529 | 0.0317
Epoch 293/300, Loss: 0.0529 | 0.0318
Epoch 294/300, Loss: 0.0529 | 0.0319
Epoch 295/300, Loss: 0.0528 | 0.0319
Epoch 296/300, Loss: 0.0528 | 0.0319
Epoch 297/300, Loss: 0.0528 | 0.0317
Epoch 298/300, Loss: 0.0527 | 0.0316
Epoch 299/300, Loss: 0.0527 | 0.0314
Epoch 300/300, Loss: 0.0527 | 0.0313
Runtime (seconds): 222.29917526245117
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 40.57003418938257
RMSE: 6.3694610595703125
MAE: 6.3694610595703125
R-squared: nan
[186.94054]
