[32m[I 2025-02-02 18:45:35,344][0m A new study created in memory with name: no-name-84dbd758-e5d8-473d-a564-67e0f15279e2[0m
[32m[I 2025-02-02 18:46:17,786][0m Trial 0 finished with value: 0.1044015196886489 and parameters: {'observation_period_num': 128, 'train_rates': 0.7390051948437851, 'learning_rate': 0.0003273251313620214, 'batch_size': 120, 'step_size': 9, 'gamma': 0.8539653751591965}. Best is trial 0 with value: 0.1044015196886489.[0m
[32m[I 2025-02-02 18:46:46,714][0m Trial 1 finished with value: 0.999522783216976 and parameters: {'observation_period_num': 188, 'train_rates': 0.7265468574795302, 'learning_rate': 3.590180772923873e-06, 'batch_size': 177, 'step_size': 4, 'gamma': 0.788431827127542}. Best is trial 0 with value: 0.1044015196886489.[0m
[32m[I 2025-02-02 18:48:15,285][0m Trial 2 finished with value: 0.31074416121611226 and parameters: {'observation_period_num': 118, 'train_rates': 0.8900800096167425, 'learning_rate': 1.5697817874953858e-06, 'batch_size': 62, 'step_size': 13, 'gamma': 0.8923877151876531}. Best is trial 0 with value: 0.1044015196886489.[0m
[32m[I 2025-02-02 18:48:46,948][0m Trial 3 finished with value: 0.7330821436504985 and parameters: {'observation_period_num': 129, 'train_rates': 0.6954125988492937, 'learning_rate': 3.5417615046363973e-06, 'batch_size': 162, 'step_size': 15, 'gamma': 0.9857683450213819}. Best is trial 0 with value: 0.1044015196886489.[0m
[32m[I 2025-02-02 18:50:12,742][0m Trial 4 finished with value: 0.09900843505473698 and parameters: {'observation_period_num': 237, 'train_rates': 0.91260265966045, 'learning_rate': 3.85148535547651e-05, 'batch_size': 63, 'step_size': 6, 'gamma': 0.9013464361839603}. Best is trial 4 with value: 0.09900843505473698.[0m
Early stopping at epoch 48
[32m[I 2025-02-02 18:50:25,821][0m Trial 5 finished with value: 0.5865375995635986 and parameters: {'observation_period_num': 212, 'train_rates': 0.9798759166157752, 'learning_rate': 2.8403162073453563e-06, 'batch_size': 248, 'step_size': 1, 'gamma': 0.8180067805905854}. Best is trial 4 with value: 0.09900843505473698.[0m
[32m[I 2025-02-02 18:50:51,197][0m Trial 6 finished with value: 0.08734428200102945 and parameters: {'observation_period_num': 14, 'train_rates': 0.9111284292060257, 'learning_rate': 1.4923955221177621e-05, 'batch_size': 252, 'step_size': 14, 'gamma': 0.9294285086659178}. Best is trial 6 with value: 0.08734428200102945.[0m
[32m[I 2025-02-02 18:51:22,316][0m Trial 7 finished with value: 0.14825618891946732 and parameters: {'observation_period_num': 147, 'train_rates': 0.7568927417748015, 'learning_rate': 3.9252902862166984e-05, 'batch_size': 165, 'step_size': 9, 'gamma': 0.9794334424641296}. Best is trial 6 with value: 0.08734428200102945.[0m
[32m[I 2025-02-02 18:51:49,569][0m Trial 8 finished with value: 0.2713195541053238 and parameters: {'observation_period_num': 192, 'train_rates': 0.7203957867636295, 'learning_rate': 0.00026568658743390713, 'batch_size': 188, 'step_size': 5, 'gamma': 0.7588382772837793}. Best is trial 6 with value: 0.08734428200102945.[0m
[32m[I 2025-02-02 18:53:07,215][0m Trial 9 finished with value: 0.09088496539262261 and parameters: {'observation_period_num': 188, 'train_rates': 0.9637862856006082, 'learning_rate': 5.1763701563071254e-05, 'batch_size': 72, 'step_size': 15, 'gamma': 0.8177930373037159}. Best is trial 6 with value: 0.08734428200102945.[0m
[32m[I 2025-02-02 18:53:28,165][0m Trial 10 finished with value: 0.27289178758678073 and parameters: {'observation_period_num': 7, 'train_rates': 0.609947650804371, 'learning_rate': 1.1910690106970499e-05, 'batch_size': 253, 'step_size': 12, 'gamma': 0.9369339927706575}. Best is trial 6 with value: 0.08734428200102945.[0m
[32m[I 2025-02-02 18:57:34,028][0m Trial 11 finished with value: 0.04604550379913007 and parameters: {'observation_period_num': 11, 'train_rates': 0.8576507330862354, 'learning_rate': 9.954161019968439e-05, 'batch_size': 22, 'step_size': 15, 'gamma': 0.8403685608474624}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:03:00,157][0m Trial 12 finished with value: 0.04848033546434213 and parameters: {'observation_period_num': 14, 'train_rates': 0.8324600025972393, 'learning_rate': 0.00012678928993898112, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9343150738814221}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:07:37,893][0m Trial 13 finished with value: 0.06365179394815484 and parameters: {'observation_period_num': 48, 'train_rates': 0.839646302628977, 'learning_rate': 0.0009018138392655677, 'batch_size': 19, 'step_size': 11, 'gamma': 0.8517038976256642}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:11:43,215][0m Trial 14 finished with value: 0.07306790940071407 and parameters: {'observation_period_num': 63, 'train_rates': 0.8159267811414745, 'learning_rate': 9.845224286076946e-05, 'batch_size': 21, 'step_size': 11, 'gamma': 0.9423835603975506}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:12:38,878][0m Trial 15 finished with value: 0.05695861216961913 and parameters: {'observation_period_num': 61, 'train_rates': 0.8546485908176013, 'learning_rate': 0.00014477591828105098, 'batch_size': 100, 'step_size': 13, 'gamma': 0.8765936379908812}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:14:43,279][0m Trial 16 finished with value: 0.09333719298042156 and parameters: {'observation_period_num': 85, 'train_rates': 0.7937003696894268, 'learning_rate': 0.0009959227007475615, 'batch_size': 41, 'step_size': 10, 'gamma': 0.8254921166070348}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:15:49,546][0m Trial 17 finished with value: 0.055008102482871 and parameters: {'observation_period_num': 33, 'train_rates': 0.8698989196282161, 'learning_rate': 9.606310974270016e-05, 'batch_size': 86, 'step_size': 7, 'gamma': 0.9038948800216906}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:17:59,900][0m Trial 18 finished with value: 0.08855074343153242 and parameters: {'observation_period_num': 90, 'train_rates': 0.7856204148403669, 'learning_rate': 1.2714827533869967e-05, 'batch_size': 40, 'step_size': 13, 'gamma': 0.961089350723825}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:18:48,696][0m Trial 19 finished with value: 0.05295294884614602 and parameters: {'observation_period_num': 34, 'train_rates': 0.9430785960464741, 'learning_rate': 0.00034984088820203733, 'batch_size': 127, 'step_size': 15, 'gamma': 0.8507911998125004}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:21:04,864][0m Trial 20 finished with value: 0.13305510421518643 and parameters: {'observation_period_num': 89, 'train_rates': 0.6779178425006338, 'learning_rate': 0.0001791730248610588, 'batch_size': 34, 'step_size': 11, 'gamma': 0.9163188189535526}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:21:53,567][0m Trial 21 finished with value: 0.051969475374964186 and parameters: {'observation_period_num': 32, 'train_rates': 0.9430954917358413, 'learning_rate': 0.00032900424299460614, 'batch_size': 130, 'step_size': 15, 'gamma': 0.8486582860641779}. Best is trial 11 with value: 0.04604550379913007.[0m
[32m[I 2025-02-02 19:22:22,370][0m Trial 22 finished with value: 0.042259031383426814 and parameters: {'observation_period_num': 7, 'train_rates': 0.8272806715953956, 'learning_rate': 0.0005058287593844437, 'batch_size': 215, 'step_size': 14, 'gamma': 0.8800497130597484}. Best is trial 22 with value: 0.042259031383426814.[0m
[32m[I 2025-02-02 19:22:51,142][0m Trial 23 finished with value: 0.04181942202754923 and parameters: {'observation_period_num': 6, 'train_rates': 0.8243971920644166, 'learning_rate': 0.0005303188630651956, 'batch_size': 216, 'step_size': 13, 'gamma': 0.8758523654495357}. Best is trial 23 with value: 0.04181942202754923.[0m
[32m[I 2025-02-02 19:23:18,626][0m Trial 24 finished with value: 0.038602150195810994 and parameters: {'observation_period_num': 6, 'train_rates': 0.7669791212843143, 'learning_rate': 0.000536874552235522, 'batch_size': 214, 'step_size': 14, 'gamma': 0.8763076834903126}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:23:45,579][0m Trial 25 finished with value: 0.07531918416629131 and parameters: {'observation_period_num': 55, 'train_rates': 0.7698073303930019, 'learning_rate': 0.0005494783769821368, 'batch_size': 217, 'step_size': 13, 'gamma': 0.876439662188029}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:24:14,390][0m Trial 26 finished with value: 0.054716397669504986 and parameters: {'observation_period_num': 33, 'train_rates': 0.819416873959012, 'learning_rate': 0.0006095538433494958, 'batch_size': 213, 'step_size': 14, 'gamma': 0.8849229375961057}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:24:38,361][0m Trial 27 finished with value: 0.1390722592243518 and parameters: {'observation_period_num': 5, 'train_rates': 0.6457766224311426, 'learning_rate': 0.000521034760932738, 'batch_size': 221, 'step_size': 12, 'gamma': 0.8653652792883788}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:25:06,940][0m Trial 28 finished with value: 0.06937373908216594 and parameters: {'observation_period_num': 73, 'train_rates': 0.8064145441827452, 'learning_rate': 0.00022882249462554443, 'batch_size': 201, 'step_size': 8, 'gamma': 0.911566364675017}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:25:31,464][0m Trial 29 finished with value: 0.10041628870463604 and parameters: {'observation_period_num': 106, 'train_rates': 0.7483655866856496, 'learning_rate': 0.00042311470442485193, 'batch_size': 233, 'step_size': 14, 'gamma': 0.8648203298878256}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:26:07,430][0m Trial 30 finished with value: 0.08238512540092835 and parameters: {'observation_period_num': 45, 'train_rates': 0.7767159272832476, 'learning_rate': 6.454988413398634e-05, 'batch_size': 151, 'step_size': 9, 'gamma': 0.8042278523311359}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:26:40,111][0m Trial 31 finished with value: 0.051872691686850846 and parameters: {'observation_period_num': 23, 'train_rates': 0.8675876849962695, 'learning_rate': 0.0006643930635090967, 'batch_size': 189, 'step_size': 14, 'gamma': 0.8399363038027828}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:27:35,638][0m Trial 32 finished with value: 0.046782030501460604 and parameters: {'observation_period_num': 19, 'train_rates': 0.846011664633507, 'learning_rate': 0.0002547441795456319, 'batch_size': 106, 'step_size': 14, 'gamma': 0.8316306934065171}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:28:04,105][0m Trial 33 finished with value: 0.07910202950239181 and parameters: {'observation_period_num': 7, 'train_rates': 0.8896654452251906, 'learning_rate': 1.8917809782546034e-05, 'batch_size': 233, 'step_size': 12, 'gamma': 0.8899931022454096}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:28:31,496][0m Trial 34 finished with value: 0.17535782854413298 and parameters: {'observation_period_num': 150, 'train_rates': 0.7279344780282887, 'learning_rate': 0.0002136644517104729, 'batch_size': 202, 'step_size': 3, 'gamma': 0.8608299156199254}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:29:14,918][0m Trial 35 finished with value: 0.04865294090799383 and parameters: {'observation_period_num': 40, 'train_rates': 0.8857536143398917, 'learning_rate': 0.0007630885833242173, 'batch_size': 141, 'step_size': 15, 'gamma': 0.7913082463826937}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:29:48,077][0m Trial 36 finished with value: 0.1332377948553489 and parameters: {'observation_period_num': 22, 'train_rates': 0.8000154142864148, 'learning_rate': 7.1286200898402365e-06, 'batch_size': 180, 'step_size': 13, 'gamma': 0.892095244432972}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:30:23,456][0m Trial 37 finished with value: 1.158458553827726 and parameters: {'observation_period_num': 22, 'train_rates': 0.8272896252775199, 'learning_rate': 1.1788757227530801e-06, 'batch_size': 166, 'step_size': 14, 'gamma': 0.839258646458527}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:30:51,971][0m Trial 38 finished with value: 0.07243879730543815 and parameters: {'observation_period_num': 75, 'train_rates': 0.9161326624310743, 'learning_rate': 0.0004127017669074642, 'batch_size': 230, 'step_size': 10, 'gamma': 0.8769940618399599}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:31:17,479][0m Trial 39 finished with value: 0.42141478163440055 and parameters: {'observation_period_num': 152, 'train_rates': 0.7074987551540882, 'learning_rate': 2.6233445900731653e-05, 'batch_size': 199, 'step_size': 13, 'gamma': 0.7677175938233538}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:31:53,600][0m Trial 40 finished with value: 0.09787156707362125 and parameters: {'observation_period_num': 48, 'train_rates': 0.7645170923423854, 'learning_rate': 6.638452392031771e-05, 'batch_size': 156, 'step_size': 15, 'gamma': 0.9177089119989452}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:32:50,568][0m Trial 41 finished with value: 0.053062539382845955 and parameters: {'observation_period_num': 15, 'train_rates': 0.8464411179885332, 'learning_rate': 0.00027454270794624185, 'batch_size': 103, 'step_size': 14, 'gamma': 0.8299439317350974}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:33:41,196][0m Trial 42 finished with value: 0.11948143911582453 and parameters: {'observation_period_num': 248, 'train_rates': 0.8604524806310107, 'learning_rate': 0.0001590122404095135, 'batch_size': 107, 'step_size': 14, 'gamma': 0.8384114123847175}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:34:07,043][0m Trial 43 finished with value: 0.05423079893426687 and parameters: {'observation_period_num': 24, 'train_rates': 0.835756814906751, 'learning_rate': 0.00044732185620832205, 'batch_size': 243, 'step_size': 15, 'gamma': 0.8124785371381544}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:35:43,754][0m Trial 44 finished with value: 0.04905049204826355 and parameters: {'observation_period_num': 6, 'train_rates': 0.8984711191155227, 'learning_rate': 9.744313719458137e-05, 'batch_size': 60, 'step_size': 12, 'gamma': 0.7973541423336244}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:36:16,925][0m Trial 45 finished with value: 0.04787233435228223 and parameters: {'observation_period_num': 17, 'train_rates': 0.813100677599619, 'learning_rate': 0.0002799973834253765, 'batch_size': 174, 'step_size': 14, 'gamma': 0.8594732198478203}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:37:21,961][0m Trial 46 finished with value: 0.05995403349736877 and parameters: {'observation_period_num': 26, 'train_rates': 0.743626153339241, 'learning_rate': 0.0007286890398161066, 'batch_size': 79, 'step_size': 13, 'gamma': 0.7755664862714331}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:37:53,487][0m Trial 47 finished with value: 0.06682129875874855 and parameters: {'observation_period_num': 43, 'train_rates': 0.877999272124748, 'learning_rate': 0.00020525778197435533, 'batch_size': 190, 'step_size': 3, 'gamma': 0.8829116679053507}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:39:20,329][0m Trial 48 finished with value: 0.1017262626622544 and parameters: {'observation_period_num': 131, 'train_rates': 0.7840722088826095, 'learning_rate': 0.0001239532883642813, 'batch_size': 57, 'step_size': 11, 'gamma': 0.9014455328253216}. Best is trial 24 with value: 0.038602150195810994.[0m
[32m[I 2025-02-02 19:40:10,169][0m Trial 49 finished with value: 0.05503176186649566 and parameters: {'observation_period_num': 58, 'train_rates': 0.8484275185780931, 'learning_rate': 0.0009802090527874297, 'batch_size': 116, 'step_size': 15, 'gamma': 0.8282508605135644}. Best is trial 24 with value: 0.038602150195810994.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_PFE_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.6629 | 0.2753
Epoch 2/300, Loss: 0.2677 | 0.1639
Epoch 3/300, Loss: 0.2379 | 0.1227
Epoch 4/300, Loss: 0.1719 | 0.1230
Epoch 5/300, Loss: 0.1832 | 0.1218
Epoch 6/300, Loss: 0.1836 | 0.1402
Epoch 7/300, Loss: 0.1500 | 0.0890
Epoch 8/300, Loss: 0.1536 | 0.1101
Epoch 9/300, Loss: 0.1636 | 0.1113
Epoch 10/300, Loss: 0.1543 | 0.1364
Epoch 11/300, Loss: 0.1548 | 0.1094
Epoch 12/300, Loss: 0.1473 | 0.1134
Epoch 13/300, Loss: 0.1423 | 0.0769
Epoch 14/300, Loss: 0.1394 | 0.1045
Epoch 15/300, Loss: 0.1583 | 0.1781
Epoch 16/300, Loss: 0.1458 | 0.1025
Epoch 17/300, Loss: 0.1251 | 0.0824
Epoch 18/300, Loss: 0.1197 | 0.1017
Epoch 19/300, Loss: 0.1237 | 0.0709
Epoch 20/300, Loss: 0.1198 | 0.0852
Epoch 21/300, Loss: 0.1334 | 0.1243
Epoch 22/300, Loss: 0.1200 | 0.1100
Epoch 23/300, Loss: 0.1139 | 0.0669
Epoch 24/300, Loss: 0.1088 | 0.0747
Epoch 25/300, Loss: 0.1078 | 0.0666
Epoch 26/300, Loss: 0.1066 | 0.0695
Epoch 27/300, Loss: 0.1126 | 0.0849
Epoch 28/300, Loss: 0.1110 | 0.0834
Epoch 29/300, Loss: 0.1133 | 0.0873
Epoch 30/300, Loss: 0.1066 | 0.0784
Epoch 31/300, Loss: 0.1043 | 0.0623
Epoch 32/300, Loss: 0.1014 | 0.0641
Epoch 33/300, Loss: 0.0987 | 0.0593
Epoch 34/300, Loss: 0.0977 | 0.0599
Epoch 35/300, Loss: 0.0975 | 0.0605
Epoch 36/300, Loss: 0.0972 | 0.0606
Epoch 37/300, Loss: 0.0996 | 0.0698
Epoch 38/300, Loss: 0.0951 | 0.0637
Epoch 39/300, Loss: 0.0938 | 0.0558
Epoch 40/300, Loss: 0.0923 | 0.0552
Epoch 41/300, Loss: 0.0927 | 0.0566
Epoch 42/300, Loss: 0.0929 | 0.0553
Epoch 43/300, Loss: 0.0928 | 0.0585
Epoch 44/300, Loss: 0.0913 | 0.0560
Epoch 45/300, Loss: 0.0904 | 0.0535
Epoch 46/300, Loss: 0.0892 | 0.0525
Epoch 47/300, Loss: 0.0889 | 0.0529
Epoch 48/300, Loss: 0.0882 | 0.0516
Epoch 49/300, Loss: 0.0884 | 0.0528
Epoch 50/300, Loss: 0.0873 | 0.0513
Epoch 51/300, Loss: 0.0867 | 0.0511
Epoch 52/300, Loss: 0.0859 | 0.0497
Epoch 53/300, Loss: 0.0853 | 0.0496
Epoch 54/300, Loss: 0.0849 | 0.0488
Epoch 55/300, Loss: 0.0845 | 0.0489
Epoch 56/300, Loss: 0.0842 | 0.0484
Epoch 57/300, Loss: 0.0838 | 0.0484
Epoch 58/300, Loss: 0.0834 | 0.0477
Epoch 59/300, Loss: 0.0830 | 0.0476
Epoch 60/300, Loss: 0.0827 | 0.0473
Epoch 61/300, Loss: 0.0823 | 0.0470
Epoch 62/300, Loss: 0.0821 | 0.0469
Epoch 63/300, Loss: 0.0817 | 0.0466
Epoch 64/300, Loss: 0.0815 | 0.0464
Epoch 65/300, Loss: 0.0812 | 0.0461
Epoch 66/300, Loss: 0.0810 | 0.0461
Epoch 67/300, Loss: 0.0808 | 0.0458
Epoch 68/300, Loss: 0.0806 | 0.0459
Epoch 69/300, Loss: 0.0804 | 0.0456
Epoch 70/300, Loss: 0.0802 | 0.0456
Epoch 71/300, Loss: 0.0799 | 0.0452
Epoch 72/300, Loss: 0.0799 | 0.0454
Epoch 73/300, Loss: 0.0798 | 0.0454
Epoch 74/300, Loss: 0.0798 | 0.0454
Epoch 75/300, Loss: 0.0798 | 0.0450
Epoch 76/300, Loss: 0.0797 | 0.0451
Epoch 77/300, Loss: 0.0796 | 0.0450
Epoch 78/300, Loss: 0.0797 | 0.0450
Epoch 79/300, Loss: 0.0801 | 0.0448
Epoch 80/300, Loss: 0.0807 | 0.0457
Epoch 81/300, Loss: 0.0808 | 0.0479
Epoch 82/300, Loss: 0.0821 | 0.0474
Epoch 83/300, Loss: 0.0827 | 0.0450
Epoch 84/300, Loss: 0.0805 | 0.0455
Epoch 85/300, Loss: 0.0793 | 0.0447
Epoch 86/300, Loss: 0.0790 | 0.0452
Epoch 87/300, Loss: 0.0785 | 0.0452
Epoch 88/300, Loss: 0.0785 | 0.0446
Epoch 89/300, Loss: 0.0787 | 0.0442
Epoch 90/300, Loss: 0.0791 | 0.0446
Epoch 91/300, Loss: 0.0802 | 0.0466
Epoch 92/300, Loss: 0.0827 | 0.0464
Epoch 93/300, Loss: 0.0834 | 0.0455
Epoch 94/300, Loss: 0.0799 | 0.0455
Epoch 95/300, Loss: 0.0790 | 0.0441
Epoch 96/300, Loss: 0.0784 | 0.0447
Epoch 97/300, Loss: 0.0774 | 0.0445
Epoch 98/300, Loss: 0.0772 | 0.0439
Epoch 99/300, Loss: 0.0770 | 0.0436
Epoch 100/300, Loss: 0.0766 | 0.0435
Epoch 101/300, Loss: 0.0765 | 0.0435
Epoch 102/300, Loss: 0.0764 | 0.0435
Epoch 103/300, Loss: 0.0762 | 0.0434
Epoch 104/300, Loss: 0.0761 | 0.0433
Epoch 105/300, Loss: 0.0760 | 0.0432
Epoch 106/300, Loss: 0.0759 | 0.0432
Epoch 107/300, Loss: 0.0758 | 0.0431
Epoch 108/300, Loss: 0.0757 | 0.0430
Epoch 109/300, Loss: 0.0756 | 0.0429
Epoch 110/300, Loss: 0.0755 | 0.0428
Epoch 111/300, Loss: 0.0754 | 0.0428
Epoch 112/300, Loss: 0.0753 | 0.0427
Epoch 113/300, Loss: 0.0752 | 0.0426
Epoch 114/300, Loss: 0.0751 | 0.0426
Epoch 115/300, Loss: 0.0750 | 0.0425
Epoch 116/300, Loss: 0.0749 | 0.0425
Epoch 117/300, Loss: 0.0748 | 0.0424
Epoch 118/300, Loss: 0.0747 | 0.0424
Epoch 119/300, Loss: 0.0746 | 0.0423
Epoch 120/300, Loss: 0.0745 | 0.0422
Epoch 121/300, Loss: 0.0744 | 0.0421
Epoch 122/300, Loss: 0.0743 | 0.0420
Epoch 123/300, Loss: 0.0742 | 0.0420
Epoch 124/300, Loss: 0.0742 | 0.0420
Epoch 125/300, Loss: 0.0741 | 0.0419
Epoch 126/300, Loss: 0.0741 | 0.0419
Epoch 127/300, Loss: 0.0741 | 0.0419
Epoch 128/300, Loss: 0.0741 | 0.0417
Epoch 129/300, Loss: 0.0741 | 0.0416
Epoch 130/300, Loss: 0.0739 | 0.0416
Epoch 131/300, Loss: 0.0739 | 0.0419
Epoch 132/300, Loss: 0.0740 | 0.0421
Epoch 133/300, Loss: 0.0743 | 0.0421
Epoch 134/300, Loss: 0.0744 | 0.0414
Epoch 135/300, Loss: 0.0739 | 0.0415
Epoch 136/300, Loss: 0.0735 | 0.0414
Epoch 137/300, Loss: 0.0735 | 0.0412
Epoch 138/300, Loss: 0.0734 | 0.0412
Epoch 139/300, Loss: 0.0732 | 0.0412
Epoch 140/300, Loss: 0.0730 | 0.0412
Epoch 141/300, Loss: 0.0730 | 0.0410
Epoch 142/300, Loss: 0.0729 | 0.0409
Epoch 143/300, Loss: 0.0728 | 0.0409
Epoch 144/300, Loss: 0.0727 | 0.0408
Epoch 145/300, Loss: 0.0726 | 0.0408
Epoch 146/300, Loss: 0.0726 | 0.0407
Epoch 147/300, Loss: 0.0725 | 0.0407
Epoch 148/300, Loss: 0.0724 | 0.0407
Epoch 149/300, Loss: 0.0724 | 0.0407
Epoch 150/300, Loss: 0.0723 | 0.0406
Epoch 151/300, Loss: 0.0722 | 0.0405
Epoch 152/300, Loss: 0.0721 | 0.0405
Epoch 153/300, Loss: 0.0721 | 0.0404
Epoch 154/300, Loss: 0.0720 | 0.0404
Epoch 155/300, Loss: 0.0720 | 0.0403
Epoch 156/300, Loss: 0.0719 | 0.0403
Epoch 157/300, Loss: 0.0718 | 0.0403
Epoch 158/300, Loss: 0.0718 | 0.0403
Epoch 159/300, Loss: 0.0717 | 0.0402
Epoch 160/300, Loss: 0.0717 | 0.0402
Epoch 161/300, Loss: 0.0716 | 0.0401
Epoch 162/300, Loss: 0.0715 | 0.0401
Epoch 163/300, Loss: 0.0715 | 0.0400
Epoch 164/300, Loss: 0.0715 | 0.0400
Epoch 165/300, Loss: 0.0714 | 0.0400
Epoch 166/300, Loss: 0.0714 | 0.0400
Epoch 167/300, Loss: 0.0713 | 0.0399
Epoch 168/300, Loss: 0.0713 | 0.0399
Epoch 169/300, Loss: 0.0712 | 0.0399
Epoch 170/300, Loss: 0.0712 | 0.0398
Epoch 171/300, Loss: 0.0711 | 0.0398
Epoch 172/300, Loss: 0.0711 | 0.0398
Epoch 173/300, Loss: 0.0711 | 0.0397
Epoch 174/300, Loss: 0.0710 | 0.0397
Epoch 175/300, Loss: 0.0710 | 0.0397
Epoch 176/300, Loss: 0.0709 | 0.0397
Epoch 177/300, Loss: 0.0709 | 0.0397
Epoch 178/300, Loss: 0.0709 | 0.0396
Epoch 179/300, Loss: 0.0708 | 0.0396
Epoch 180/300, Loss: 0.0708 | 0.0396
Epoch 181/300, Loss: 0.0708 | 0.0395
Epoch 182/300, Loss: 0.0707 | 0.0395
Epoch 183/300, Loss: 0.0707 | 0.0395
Epoch 184/300, Loss: 0.0706 | 0.0395
Epoch 185/300, Loss: 0.0706 | 0.0394
Epoch 186/300, Loss: 0.0706 | 0.0394
Epoch 187/300, Loss: 0.0705 | 0.0394
Epoch 188/300, Loss: 0.0705 | 0.0394
Epoch 189/300, Loss: 0.0705 | 0.0394
Epoch 190/300, Loss: 0.0705 | 0.0394
Epoch 191/300, Loss: 0.0704 | 0.0393
Epoch 192/300, Loss: 0.0704 | 0.0393
Epoch 193/300, Loss: 0.0704 | 0.0393
Epoch 194/300, Loss: 0.0703 | 0.0393
Epoch 195/300, Loss: 0.0703 | 0.0393
Epoch 196/300, Loss: 0.0703 | 0.0392
Epoch 197/300, Loss: 0.0703 | 0.0392
Epoch 198/300, Loss: 0.0702 | 0.0392
Epoch 199/300, Loss: 0.0702 | 0.0392
Epoch 200/300, Loss: 0.0702 | 0.0392
Epoch 201/300, Loss: 0.0702 | 0.0392
Epoch 202/300, Loss: 0.0701 | 0.0391
Epoch 203/300, Loss: 0.0701 | 0.0391
Epoch 204/300, Loss: 0.0701 | 0.0391
Epoch 205/300, Loss: 0.0701 | 0.0391
Epoch 206/300, Loss: 0.0700 | 0.0391
Epoch 207/300, Loss: 0.0700 | 0.0391
Epoch 208/300, Loss: 0.0700 | 0.0391
Epoch 209/300, Loss: 0.0700 | 0.0390
Epoch 210/300, Loss: 0.0700 | 0.0390
Epoch 211/300, Loss: 0.0699 | 0.0390
Epoch 212/300, Loss: 0.0699 | 0.0390
Epoch 213/300, Loss: 0.0699 | 0.0390
Epoch 214/300, Loss: 0.0699 | 0.0390
Epoch 215/300, Loss: 0.0699 | 0.0390
Epoch 216/300, Loss: 0.0699 | 0.0390
Epoch 217/300, Loss: 0.0698 | 0.0390
Epoch 218/300, Loss: 0.0698 | 0.0389
Epoch 219/300, Loss: 0.0698 | 0.0389
Epoch 220/300, Loss: 0.0698 | 0.0389
Epoch 221/300, Loss: 0.0698 | 0.0389
Epoch 222/300, Loss: 0.0697 | 0.0389
Epoch 223/300, Loss: 0.0697 | 0.0389
Epoch 224/300, Loss: 0.0697 | 0.0389
Epoch 225/300, Loss: 0.0697 | 0.0389
Epoch 226/300, Loss: 0.0697 | 0.0389
Epoch 227/300, Loss: 0.0697 | 0.0389
Epoch 228/300, Loss: 0.0697 | 0.0389
Epoch 229/300, Loss: 0.0696 | 0.0388
Epoch 230/300, Loss: 0.0696 | 0.0388
Epoch 231/300, Loss: 0.0696 | 0.0388
Epoch 232/300, Loss: 0.0696 | 0.0388
Epoch 233/300, Loss: 0.0696 | 0.0388
Epoch 234/300, Loss: 0.0696 | 0.0388
Epoch 235/300, Loss: 0.0696 | 0.0388
Epoch 236/300, Loss: 0.0695 | 0.0388
Epoch 237/300, Loss: 0.0695 | 0.0388
Epoch 238/300, Loss: 0.0695 | 0.0388
Epoch 239/300, Loss: 0.0695 | 0.0388
Epoch 240/300, Loss: 0.0695 | 0.0388
Epoch 241/300, Loss: 0.0695 | 0.0388
Epoch 242/300, Loss: 0.0695 | 0.0387
Epoch 243/300, Loss: 0.0695 | 0.0387
Epoch 244/300, Loss: 0.0695 | 0.0387
Epoch 245/300, Loss: 0.0694 | 0.0387
Epoch 246/300, Loss: 0.0694 | 0.0387
Epoch 247/300, Loss: 0.0694 | 0.0387
Epoch 248/300, Loss: 0.0694 | 0.0387
Epoch 249/300, Loss: 0.0694 | 0.0387
Epoch 250/300, Loss: 0.0694 | 0.0387
Epoch 251/300, Loss: 0.0694 | 0.0387
Epoch 252/300, Loss: 0.0694 | 0.0387
Epoch 253/300, Loss: 0.0694 | 0.0387
Epoch 254/300, Loss: 0.0694 | 0.0387
Epoch 255/300, Loss: 0.0693 | 0.0387
Epoch 256/300, Loss: 0.0693 | 0.0387
Epoch 257/300, Loss: 0.0693 | 0.0387
Epoch 258/300, Loss: 0.0693 | 0.0387
Epoch 259/300, Loss: 0.0693 | 0.0387
Epoch 260/300, Loss: 0.0693 | 0.0386
Epoch 261/300, Loss: 0.0693 | 0.0386
Epoch 262/300, Loss: 0.0693 | 0.0386
Epoch 263/300, Loss: 0.0693 | 0.0386
Epoch 264/300, Loss: 0.0693 | 0.0386
Epoch 265/300, Loss: 0.0693 | 0.0386
Epoch 266/300, Loss: 0.0693 | 0.0386
Epoch 267/300, Loss: 0.0693 | 0.0386
Epoch 268/300, Loss: 0.0692 | 0.0386
Epoch 269/300, Loss: 0.0692 | 0.0386
Epoch 270/300, Loss: 0.0692 | 0.0386
Epoch 271/300, Loss: 0.0692 | 0.0386
Epoch 272/300, Loss: 0.0692 | 0.0386
Epoch 273/300, Loss: 0.0692 | 0.0386
Epoch 274/300, Loss: 0.0692 | 0.0386
Epoch 275/300, Loss: 0.0692 | 0.0386
Epoch 276/300, Loss: 0.0692 | 0.0386
Epoch 277/300, Loss: 0.0692 | 0.0386
Epoch 278/300, Loss: 0.0692 | 0.0386
Epoch 279/300, Loss: 0.0692 | 0.0386
Epoch 280/300, Loss: 0.0692 | 0.0386
Epoch 281/300, Loss: 0.0692 | 0.0386
Epoch 282/300, Loss: 0.0692 | 0.0386
Epoch 283/300, Loss: 0.0692 | 0.0386
Epoch 284/300, Loss: 0.0691 | 0.0386
Epoch 285/300, Loss: 0.0691 | 0.0386
Epoch 286/300, Loss: 0.0691 | 0.0386
Epoch 287/300, Loss: 0.0691 | 0.0386
Epoch 288/300, Loss: 0.0691 | 0.0385
Epoch 289/300, Loss: 0.0691 | 0.0385
Epoch 290/300, Loss: 0.0691 | 0.0385
Epoch 291/300, Loss: 0.0691 | 0.0385
Epoch 292/300, Loss: 0.0691 | 0.0385
Epoch 293/300, Loss: 0.0691 | 0.0385
Epoch 294/300, Loss: 0.0691 | 0.0385
Epoch 295/300, Loss: 0.0691 | 0.0385
Epoch 296/300, Loss: 0.0691 | 0.0385
Epoch 297/300, Loss: 0.0691 | 0.0385
Epoch 298/300, Loss: 0.0691 | 0.0385
Epoch 299/300, Loss: 0.0691 | 0.0385
Epoch 300/300, Loss: 0.0691 | 0.0385
Runtime (seconds): 78.02805852890015
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.05894801815520623
RMSE: 0.24279212951660156
MAE: 0.24279212951660156
R-squared: nan
[25.108105]
