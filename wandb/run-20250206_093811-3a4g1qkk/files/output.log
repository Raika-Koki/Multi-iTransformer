ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-06 09:38:16,805][0m A new study created in memory with name: no-name-3e9acc69-5a86-4a69-bb2d-b57744103763[0m
[32m[I 2025-02-06 09:39:06,601][0m Trial 0 finished with value: 0.22556989155449234 and parameters: {'observation_period_num': 80, 'train_rates': 0.9505833759081324, 'learning_rate': 2.8467537891118006e-05, 'batch_size': 122, 'step_size': 9, 'gamma': 0.7615790860807128}. Best is trial 0 with value: 0.22556989155449234.[0m
[32m[I 2025-02-06 09:40:05,758][0m Trial 1 finished with value: 1.0760207267293342 and parameters: {'observation_period_num': 94, 'train_rates': 0.7393636753834887, 'learning_rate': 2.3040502240543185e-06, 'batch_size': 85, 'step_size': 7, 'gamma': 0.8001364709802473}. Best is trial 0 with value: 0.22556989155449234.[0m
Early stopping at epoch 82
[32m[I 2025-02-06 09:43:22,215][0m Trial 2 finished with value: 1.0318215091213918 and parameters: {'observation_period_num': 209, 'train_rates': 0.8215221930392765, 'learning_rate': 1.080586208100909e-06, 'batch_size': 21, 'step_size': 1, 'gamma': 0.8559221414756649}. Best is trial 0 with value: 0.22556989155449234.[0m
[32m[I 2025-02-06 09:46:10,053][0m Trial 3 finished with value: 0.06096110365005455 and parameters: {'observation_period_num': 34, 'train_rates': 0.9663316198560814, 'learning_rate': 1.8811477328877745e-05, 'batch_size': 35, 'step_size': 11, 'gamma': 0.958102014423443}. Best is trial 3 with value: 0.06096110365005455.[0m
[32m[I 2025-02-06 09:46:37,123][0m Trial 4 finished with value: 0.30610764724348616 and parameters: {'observation_period_num': 189, 'train_rates': 0.7215707368843839, 'learning_rate': 0.0002139686664508626, 'batch_size': 185, 'step_size': 8, 'gamma': 0.8768848587281171}. Best is trial 3 with value: 0.06096110365005455.[0m
[32m[I 2025-02-06 09:47:31,720][0m Trial 5 finished with value: 0.05262631588520254 and parameters: {'observation_period_num': 52, 'train_rates': 0.8619469651716238, 'learning_rate': 0.00019099014817411272, 'batch_size': 107, 'step_size': 5, 'gamma': 0.899580923528833}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:50:15,063][0m Trial 6 finished with value: 0.26574169981577794 and parameters: {'observation_period_num': 114, 'train_rates': 0.7651493108884937, 'learning_rate': 0.00013685845044787291, 'batch_size': 30, 'step_size': 15, 'gamma': 0.9119049015783622}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:51:43,804][0m Trial 7 finished with value: 0.22804052679095577 and parameters: {'observation_period_num': 125, 'train_rates': 0.7425767149831941, 'learning_rate': 0.00016695231260310665, 'batch_size': 55, 'step_size': 10, 'gamma': 0.8493784839618437}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:52:26,637][0m Trial 8 finished with value: 0.19487588693833405 and parameters: {'observation_period_num': 217, 'train_rates': 0.8391812291519554, 'learning_rate': 0.000561299594747593, 'batch_size': 125, 'step_size': 13, 'gamma': 0.7558791940726656}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:52:49,284][0m Trial 9 finished with value: 0.24759926337066354 and parameters: {'observation_period_num': 107, 'train_rates': 0.7664538686692636, 'learning_rate': 0.0002201360284608809, 'batch_size': 245, 'step_size': 10, 'gamma': 0.9237612250424586}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:53:15,172][0m Trial 10 finished with value: 0.22238925563052614 and parameters: {'observation_period_num': 14, 'train_rates': 0.6167717474769694, 'learning_rate': 7.701746025145976e-06, 'batch_size': 190, 'step_size': 4, 'gamma': 0.9859851834590376}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:54:37,360][0m Trial 11 finished with value: 0.06178309768438339 and parameters: {'observation_period_num': 22, 'train_rates': 0.9812250685804902, 'learning_rate': 3.70062853686393e-05, 'batch_size': 75, 'step_size': 5, 'gamma': 0.9788781730582405}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:55:37,803][0m Trial 12 finished with value: 0.08944841532960651 and parameters: {'observation_period_num': 51, 'train_rates': 0.9128759470789287, 'learning_rate': 2.2676687153644686e-05, 'batch_size': 97, 'step_size': 12, 'gamma': 0.9369644014450703}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:56:12,196][0m Trial 13 finished with value: 0.2632685675070836 and parameters: {'observation_period_num': 57, 'train_rates': 0.8879662396854988, 'learning_rate': 8.270232467370712e-06, 'batch_size': 172, 'step_size': 5, 'gamma': 0.9502178919539158}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:57:52,130][0m Trial 14 finished with value: 0.1784174283192708 and parameters: {'observation_period_num': 154, 'train_rates': 0.8843031374635171, 'learning_rate': 0.0008606536052754372, 'batch_size': 54, 'step_size': 2, 'gamma': 0.9031356809885825}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:58:33,403][0m Trial 15 finished with value: 0.07666486686801319 and parameters: {'observation_period_num': 45, 'train_rates': 0.944890761678083, 'learning_rate': 6.863097713020253e-05, 'batch_size': 145, 'step_size': 7, 'gamma': 0.9643142504501978}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:59:02,380][0m Trial 16 finished with value: 0.1907065510749817 and parameters: {'observation_period_num': 6, 'train_rates': 0.9890862140326837, 'learning_rate': 9.26649238442766e-06, 'batch_size': 226, 'step_size': 12, 'gamma': 0.8836549774097351}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 09:59:51,475][0m Trial 17 finished with value: 0.30465783222633247 and parameters: {'observation_period_num': 249, 'train_rates': 0.8494095545794794, 'learning_rate': 6.38016486570686e-05, 'batch_size': 108, 'step_size': 3, 'gamma': 0.8263742016415377}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 10:00:23,437][0m Trial 18 finished with value: 0.15976176220832056 and parameters: {'observation_period_num': 75, 'train_rates': 0.6614427959537248, 'learning_rate': 0.0004113891227477522, 'batch_size': 149, 'step_size': 6, 'gamma': 0.9015560339774965}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 10:01:54,654][0m Trial 19 finished with value: 0.13229777754730537 and parameters: {'observation_period_num': 148, 'train_rates': 0.915546587089677, 'learning_rate': 1.7095853594334533e-05, 'batch_size': 61, 'step_size': 15, 'gamma': 0.9478809633171261}. Best is trial 5 with value: 0.05262631588520254.[0m
[32m[I 2025-02-06 10:07:30,952][0m Trial 20 finished with value: 0.04832129806135465 and parameters: {'observation_period_num': 37, 'train_rates': 0.8589046408659395, 'learning_rate': 7.791228746356926e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.929130541814049}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:10:07,505][0m Trial 21 finished with value: 0.11037518606133705 and parameters: {'observation_period_num': 37, 'train_rates': 0.8594813464667329, 'learning_rate': 7.08288736806171e-05, 'batch_size': 35, 'step_size': 11, 'gamma': 0.933356363016399}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:15:03,896][0m Trial 22 finished with value: 0.10779364918313035 and parameters: {'observation_period_num': 69, 'train_rates': 0.8019485329779025, 'learning_rate': 0.0001084644508852468, 'batch_size': 17, 'step_size': 13, 'gamma': 0.9681608755933921}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:17:04,491][0m Trial 23 finished with value: 0.08836182156683868 and parameters: {'observation_period_num': 30, 'train_rates': 0.9341819055647175, 'learning_rate': 0.0003331281731553391, 'batch_size': 48, 'step_size': 9, 'gamma': 0.886458497114513}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:18:17,975][0m Trial 24 finished with value: 0.06687467964217304 and parameters: {'observation_period_num': 62, 'train_rates': 0.8918298016516648, 'learning_rate': 4.855773831700585e-05, 'batch_size': 77, 'step_size': 11, 'gamma': 0.9184518746876221}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:20:27,478][0m Trial 25 finished with value: 0.2454922959808535 and parameters: {'observation_period_num': 90, 'train_rates': 0.6882072057691178, 'learning_rate': 1.6715157762324094e-05, 'batch_size': 36, 'step_size': 8, 'gamma': 0.9577513177024759}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:21:19,958][0m Trial 26 finished with value: 0.21172082656590943 and parameters: {'observation_period_num': 31, 'train_rates': 0.8028444557742884, 'learning_rate': 4.940973417752657e-06, 'batch_size': 104, 'step_size': 14, 'gamma': 0.898834094573518}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:26:58,114][0m Trial 27 finished with value: 0.05427095652452344 and parameters: {'observation_period_num': 5, 'train_rates': 0.865563566814155, 'learning_rate': 0.0001103178190856826, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8634268217871987}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:27:24,917][0m Trial 28 finished with value: 0.06584933754241112 and parameters: {'observation_period_num': 5, 'train_rates': 0.863321989051599, 'learning_rate': 9.236039109808452e-05, 'batch_size': 213, 'step_size': 6, 'gamma': 0.857412143346175}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:28:10,027][0m Trial 29 finished with value: 0.06501490532864335 and parameters: {'observation_period_num': 83, 'train_rates': 0.8298627825834519, 'learning_rate': 0.00033670098717980936, 'batch_size': 124, 'step_size': 9, 'gamma': 0.8234961586468322}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:28:46,786][0m Trial 30 finished with value: 0.24166586808860302 and parameters: {'observation_period_num': 49, 'train_rates': 0.7826027084218488, 'learning_rate': 3.573343785236114e-05, 'batch_size': 158, 'step_size': 8, 'gamma': 0.8302309774902642}. Best is trial 20 with value: 0.04832129806135465.[0m
[32m[I 2025-02-06 10:34:17,282][0m Trial 31 finished with value: 0.03743234397732167 and parameters: {'observation_period_num': 26, 'train_rates': 0.9144353630148705, 'learning_rate': 2.4830515197158855e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.9363393891189291}. Best is trial 31 with value: 0.03743234397732167.[0m
[32m[I 2025-02-06 10:39:00,873][0m Trial 32 finished with value: 0.03719659638138133 and parameters: {'observation_period_num': 19, 'train_rates': 0.912584268294868, 'learning_rate': 0.000105481597313309, 'batch_size': 20, 'step_size': 10, 'gamma': 0.7910663197277711}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 10:40:28,424][0m Trial 33 finished with value: 0.053726678592015485 and parameters: {'observation_period_num': 22, 'train_rates': 0.916910491659172, 'learning_rate': 4.552676178451885e-05, 'batch_size': 66, 'step_size': 12, 'gamma': 0.7798590357202121}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 10:42:55,936][0m Trial 34 finished with value: 0.04933569140054963 and parameters: {'observation_period_num': 41, 'train_rates': 0.9624639865341048, 'learning_rate': 0.00018549441914986642, 'batch_size': 40, 'step_size': 7, 'gamma': 0.7773604966281499}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 10:45:14,546][0m Trial 35 finished with value: 0.08026324840616589 and parameters: {'observation_period_num': 22, 'train_rates': 0.9590567954931023, 'learning_rate': 2.5490447938262825e-05, 'batch_size': 43, 'step_size': 7, 'gamma': 0.7771823309671381}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 10:48:52,072][0m Trial 36 finished with value: 0.060113507578539296 and parameters: {'observation_period_num': 39, 'train_rates': 0.9263542887828449, 'learning_rate': 0.0002547002156554276, 'batch_size': 26, 'step_size': 9, 'gamma': 0.7974875635991484}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 10:51:06,544][0m Trial 37 finished with value: 0.17791333120120198 and parameters: {'observation_period_num': 65, 'train_rates': 0.9671998275006576, 'learning_rate': 1.2168837310913442e-05, 'batch_size': 44, 'step_size': 10, 'gamma': 0.771808955588922}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 10:52:09,567][0m Trial 38 finished with value: 0.9105338067621798 and parameters: {'observation_period_num': 83, 'train_rates': 0.8971482341395562, 'learning_rate': 1.513293235669803e-06, 'batch_size': 91, 'step_size': 8, 'gamma': 0.7986692161063064}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 10:55:20,545][0m Trial 39 finished with value: 0.1556582541458986 and parameters: {'observation_period_num': 145, 'train_rates': 0.9373837466650896, 'learning_rate': 0.00014753086809282117, 'batch_size': 29, 'step_size': 11, 'gamma': 0.7515600200052}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 10:56:45,115][0m Trial 40 finished with value: 0.5097439006223516 and parameters: {'observation_period_num': 100, 'train_rates': 0.9588401103978896, 'learning_rate': 4.392742740970633e-06, 'batch_size': 70, 'step_size': 13, 'gamma': 0.8131967774792747}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 11:01:46,097][0m Trial 41 finished with value: 0.11238275213059544 and parameters: {'observation_period_num': 46, 'train_rates': 0.8156041452511246, 'learning_rate': 0.00016930922931217485, 'batch_size': 17, 'step_size': 6, 'gamma': 0.9382538838140346}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 11:04:16,438][0m Trial 42 finished with value: 0.04496032915258107 and parameters: {'observation_period_num': 20, 'train_rates': 0.8786488287078874, 'learning_rate': 8.304753330250615e-05, 'batch_size': 37, 'step_size': 4, 'gamma': 0.8457879563007156}. Best is trial 32 with value: 0.03719659638138133.[0m
Early stopping at epoch 73
[32m[I 2025-02-06 11:06:04,207][0m Trial 43 finished with value: 0.0708760118353137 and parameters: {'observation_period_num': 16, 'train_rates': 0.8766708709447876, 'learning_rate': 8.330778758984029e-05, 'batch_size': 38, 'step_size': 1, 'gamma': 0.8438393621951258}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 11:07:50,594][0m Trial 44 finished with value: 0.08004047566062801 and parameters: {'observation_period_num': 31, 'train_rates': 0.9068199878857318, 'learning_rate': 5.335729590879039e-05, 'batch_size': 54, 'step_size': 4, 'gamma': 0.7696676067908221}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 11:11:09,173][0m Trial 45 finished with value: 0.052791799503662015 and parameters: {'observation_period_num': 23, 'train_rates': 0.8431583358605027, 'learning_rate': 0.00011920546339704385, 'batch_size': 27, 'step_size': 9, 'gamma': 0.7931129463316438}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 11:14:46,771][0m Trial 46 finished with value: 0.06907299381088126 and parameters: {'observation_period_num': 56, 'train_rates': 0.9698963542293652, 'learning_rate': 3.520934723846461e-05, 'batch_size': 27, 'step_size': 10, 'gamma': 0.8119104164535278}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 11:16:03,202][0m Trial 47 finished with value: 0.05580439451118033 and parameters: {'observation_period_num': 41, 'train_rates': 0.9020609686263704, 'learning_rate': 0.0005385034219306334, 'batch_size': 79, 'step_size': 3, 'gamma': 0.7900785123507581}. Best is trial 32 with value: 0.03719659638138133.[0m
[32m[I 2025-02-06 11:17:51,157][0m Trial 48 finished with value: 0.03192205306163615 and parameters: {'observation_period_num': 15, 'train_rates': 0.939860573530533, 'learning_rate': 0.00025142320948791635, 'batch_size': 55, 'step_size': 7, 'gamma': 0.762150358644485}. Best is trial 48 with value: 0.03192205306163615.[0m
[32m[I 2025-02-06 11:19:23,946][0m Trial 49 finished with value: 0.15016927144672038 and parameters: {'observation_period_num': 169, 'train_rates': 0.8789589251034051, 'learning_rate': 0.0002615079125746563, 'batch_size': 57, 'step_size': 11, 'gamma': 0.7592802216005623}. Best is trial 48 with value: 0.03192205306163615.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-06 11:19:23,957][0m A new study created in memory with name: no-name-631a7767-6fa1-493a-9521-59040c291a1c[0m
[32m[I 2025-02-06 11:20:05,488][0m Trial 0 finished with value: 0.10577556490898132 and parameters: {'observation_period_num': 66, 'train_rates': 0.9846458981717523, 'learning_rate': 5.5712773798515e-05, 'batch_size': 154, 'step_size': 15, 'gamma': 0.8553666255812737}. Best is trial 0 with value: 0.10577556490898132.[0m
[32m[I 2025-02-06 11:21:27,107][0m Trial 1 finished with value: 0.6052340287990853 and parameters: {'observation_period_num': 92, 'train_rates': 0.8350136567780169, 'learning_rate': 3.10470153858878e-06, 'batch_size': 65, 'step_size': 3, 'gamma': 0.9086082537767568}. Best is trial 0 with value: 0.10577556490898132.[0m
[32m[I 2025-02-06 11:24:30,094][0m Trial 2 finished with value: 0.2006329481158397 and parameters: {'observation_period_num': 88, 'train_rates': 0.6925202896279699, 'learning_rate': 0.0006505676259328526, 'batch_size': 25, 'step_size': 2, 'gamma': 0.8760319271954141}. Best is trial 0 with value: 0.10577556490898132.[0m
[32m[I 2025-02-06 11:25:02,695][0m Trial 3 finished with value: 0.7016070296184174 and parameters: {'observation_period_num': 222, 'train_rates': 0.8214259913399664, 'learning_rate': 1.4127952285759073e-06, 'batch_size': 170, 'step_size': 9, 'gamma': 0.9181449517186104}. Best is trial 0 with value: 0.10577556490898132.[0m
[32m[I 2025-02-06 11:25:50,488][0m Trial 4 finished with value: 0.7853043687086982 and parameters: {'observation_period_num': 73, 'train_rates': 0.6455296342884685, 'learning_rate': 2.948296846251098e-06, 'batch_size': 97, 'step_size': 7, 'gamma': 0.7677014966430953}. Best is trial 0 with value: 0.10577556490898132.[0m
[32m[I 2025-02-06 11:30:29,952][0m Trial 5 finished with value: 0.2693372591515411 and parameters: {'observation_period_num': 140, 'train_rates': 0.756453105506625, 'learning_rate': 0.0006109363652598177, 'batch_size': 17, 'step_size': 12, 'gamma': 0.9083589898192248}. Best is trial 0 with value: 0.10577556490898132.[0m
[32m[I 2025-02-06 11:31:09,535][0m Trial 6 finished with value: 0.15872182705892404 and parameters: {'observation_period_num': 172, 'train_rates': 0.9090974217516783, 'learning_rate': 0.0006840397073129973, 'batch_size': 145, 'step_size': 14, 'gamma': 0.788814999047604}. Best is trial 0 with value: 0.10577556490898132.[0m
[32m[I 2025-02-06 11:31:51,963][0m Trial 7 finished with value: 0.11545623142372145 and parameters: {'observation_period_num': 196, 'train_rates': 0.9054830215924632, 'learning_rate': 0.000310138594015565, 'batch_size': 136, 'step_size': 10, 'gamma': 0.8992191444113818}. Best is trial 0 with value: 0.10577556490898132.[0m
[32m[I 2025-02-06 11:33:09,199][0m Trial 8 finished with value: 0.23521720059216022 and parameters: {'observation_period_num': 73, 'train_rates': 0.667201787034233, 'learning_rate': 0.0006702036695529827, 'batch_size': 60, 'step_size': 7, 'gamma': 0.9819677197661145}. Best is trial 0 with value: 0.10577556490898132.[0m
[32m[I 2025-02-06 11:33:38,420][0m Trial 9 finished with value: 1.2121568766736768 and parameters: {'observation_period_num': 162, 'train_rates': 0.7636443981271753, 'learning_rate': 1.2345221761735253e-06, 'batch_size': 186, 'step_size': 5, 'gamma': 0.8622158977131515}. Best is trial 0 with value: 0.10577556490898132.[0m
[32m[I 2025-02-06 11:34:07,319][0m Trial 10 finished with value: 0.08542724698781967 and parameters: {'observation_period_num': 9, 'train_rates': 0.9481660777232128, 'learning_rate': 4.894720103351929e-05, 'batch_size': 222, 'step_size': 15, 'gamma': 0.8183102267716665}. Best is trial 10 with value: 0.08542724698781967.[0m
[32m[I 2025-02-06 11:34:34,840][0m Trial 11 finished with value: 0.08031260967254639 and parameters: {'observation_period_num': 10, 'train_rates': 0.9886108440001979, 'learning_rate': 4.826700898661305e-05, 'batch_size': 251, 'step_size': 15, 'gamma': 0.8285369049438125}. Best is trial 11 with value: 0.08031260967254639.[0m
[32m[I 2025-02-06 11:35:02,101][0m Trial 12 finished with value: 0.09097876399755478 and parameters: {'observation_period_num': 7, 'train_rates': 0.9895435785720889, 'learning_rate': 3.712652146052519e-05, 'batch_size': 254, 'step_size': 13, 'gamma': 0.8190681555568736}. Best is trial 11 with value: 0.08031260967254639.[0m
[32m[I 2025-02-06 11:35:27,070][0m Trial 13 finished with value: 0.06921663135290146 and parameters: {'observation_period_num': 19, 'train_rates': 0.9167052398556808, 'learning_rate': 0.00011845367548393376, 'batch_size': 249, 'step_size': 12, 'gamma': 0.8218705565194278}. Best is trial 13 with value: 0.06921663135290146.[0m
[32m[I 2025-02-06 11:35:52,495][0m Trial 14 finished with value: 0.06262422986435959 and parameters: {'observation_period_num': 34, 'train_rates': 0.8812485782929604, 'learning_rate': 0.00014840032284592998, 'batch_size': 238, 'step_size': 11, 'gamma': 0.8238543273089761}. Best is trial 14 with value: 0.06262422986435959.[0m
[32m[I 2025-02-06 11:36:21,861][0m Trial 15 finished with value: 0.06771762611857358 and parameters: {'observation_period_num': 44, 'train_rates': 0.8815283122332046, 'learning_rate': 0.00015754021893056328, 'batch_size': 211, 'step_size': 11, 'gamma': 0.7876353132424518}. Best is trial 14 with value: 0.06262422986435959.[0m
[32m[I 2025-02-06 11:36:51,970][0m Trial 16 finished with value: 0.29044842897254286 and parameters: {'observation_period_num': 46, 'train_rates': 0.858583346665444, 'learning_rate': 1.1493807086594312e-05, 'batch_size': 205, 'step_size': 11, 'gamma': 0.773309218744337}. Best is trial 14 with value: 0.06262422986435959.[0m
[32m[I 2025-02-06 11:37:19,295][0m Trial 17 finished with value: 0.11313650340545835 and parameters: {'observation_period_num': 117, 'train_rates': 0.866002664376223, 'learning_rate': 0.00019901625824910913, 'batch_size': 219, 'step_size': 9, 'gamma': 0.7511426292560213}. Best is trial 14 with value: 0.06262422986435959.[0m
[32m[I 2025-02-06 11:37:47,263][0m Trial 18 finished with value: 0.4145167110071726 and parameters: {'observation_period_num': 42, 'train_rates': 0.7713563704381188, 'learning_rate': 1.644125437173937e-05, 'batch_size': 195, 'step_size': 5, 'gamma': 0.7963140070991945}. Best is trial 14 with value: 0.06262422986435959.[0m
[32m[I 2025-02-06 11:38:14,732][0m Trial 19 finished with value: 0.12035113619906562 and parameters: {'observation_period_num': 112, 'train_rates': 0.8770144079589641, 'learning_rate': 0.00011719571125630273, 'batch_size': 222, 'step_size': 11, 'gamma': 0.8451987982380154}. Best is trial 14 with value: 0.06262422986435959.[0m
[32m[I 2025-02-06 11:39:02,929][0m Trial 20 finished with value: 0.0744526719756886 and parameters: {'observation_period_num': 41, 'train_rates': 0.8063309914591661, 'learning_rate': 0.00012521988314607193, 'batch_size': 114, 'step_size': 8, 'gamma': 0.9460637848935597}. Best is trial 14 with value: 0.06262422986435959.[0m
[32m[I 2025-02-06 11:39:29,971][0m Trial 21 finished with value: 0.08089899271726608 and parameters: {'observation_period_num': 33, 'train_rates': 0.9268789085100213, 'learning_rate': 0.00010335348443358938, 'batch_size': 240, 'step_size': 12, 'gamma': 0.8081633528836639}. Best is trial 14 with value: 0.06262422986435959.[0m
[32m[I 2025-02-06 11:39:57,098][0m Trial 22 finished with value: 0.06314904242753983 and parameters: {'observation_period_num': 30, 'train_rates': 0.9461283903659572, 'learning_rate': 0.00025081413564364814, 'batch_size': 235, 'step_size': 13, 'gamma': 0.8335450814652539}. Best is trial 14 with value: 0.06262422986435959.[0m
[32m[I 2025-02-06 11:40:32,368][0m Trial 23 finished with value: 0.06216049566864967 and parameters: {'observation_period_num': 49, 'train_rates': 0.9457669147826191, 'learning_rate': 0.0003082960203839694, 'batch_size': 176, 'step_size': 13, 'gamma': 0.8409977615836554}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:41:05,717][0m Trial 24 finished with value: 0.16789448261260986 and parameters: {'observation_period_num': 251, 'train_rates': 0.9481685343358383, 'learning_rate': 0.00029490387900928453, 'batch_size': 176, 'step_size': 13, 'gamma': 0.8759666283812823}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:41:32,778][0m Trial 25 finished with value: 0.10154435783624649 and parameters: {'observation_period_num': 56, 'train_rates': 0.956187375310618, 'learning_rate': 0.00033706330145390266, 'batch_size': 231, 'step_size': 13, 'gamma': 0.8389477648819158}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:42:10,743][0m Trial 26 finished with value: 0.16891469693545139 and parameters: {'observation_period_num': 99, 'train_rates': 0.9422957249119911, 'learning_rate': 0.00037734036067033755, 'batch_size': 160, 'step_size': 14, 'gamma': 0.8839128479016143}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:42:42,583][0m Trial 27 finished with value: 0.1315388267190711 and parameters: {'observation_period_num': 29, 'train_rates': 0.8946240640721063, 'learning_rate': 1.6623375723519845e-05, 'batch_size': 195, 'step_size': 10, 'gamma': 0.8379894429776404}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:43:27,699][0m Trial 28 finished with value: 0.14956805292028108 and parameters: {'observation_period_num': 137, 'train_rates': 0.8434689683510311, 'learning_rate': 7.584868279181169e-05, 'batch_size': 122, 'step_size': 14, 'gamma': 0.8553062406764191}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:43:54,668][0m Trial 29 finished with value: 0.1241302341222763 and parameters: {'observation_period_num': 63, 'train_rates': 0.9631596676859052, 'learning_rate': 0.00022084370610385602, 'batch_size': 234, 'step_size': 10, 'gamma': 0.8536080450939161}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:44:34,198][0m Trial 30 finished with value: 0.07794761657714844 and parameters: {'observation_period_num': 24, 'train_rates': 0.9657797518118412, 'learning_rate': 7.28468382288289e-05, 'batch_size': 163, 'step_size': 15, 'gamma': 0.7990091189957146}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:45:04,122][0m Trial 31 finished with value: 0.07455864396080913 and parameters: {'observation_period_num': 57, 'train_rates': 0.8856330892504233, 'learning_rate': 0.000185911428564202, 'batch_size': 204, 'step_size': 11, 'gamma': 0.7851163763559895}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:45:35,233][0m Trial 32 finished with value: 0.07517166178801964 and parameters: {'observation_period_num': 80, 'train_rates': 0.9196404195580927, 'learning_rate': 0.0004943728549099637, 'batch_size': 210, 'step_size': 12, 'gamma': 0.8033469950896509}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:46:04,058][0m Trial 33 finished with value: 0.18226213502967648 and parameters: {'observation_period_num': 51, 'train_rates': 0.7305982146727821, 'learning_rate': 0.0009133777617202341, 'batch_size': 184, 'step_size': 13, 'gamma': 0.7730191879649548}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:46:29,342][0m Trial 34 finished with value: 0.11008591697210783 and parameters: {'observation_period_num': 94, 'train_rates': 0.8413991229455895, 'learning_rate': 0.0001728325638215767, 'batch_size': 235, 'step_size': 9, 'gamma': 0.8326406784015027}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:46:56,887][0m Trial 35 finished with value: 0.08723632424362819 and parameters: {'observation_period_num': 35, 'train_rates': 0.8126185579162089, 'learning_rate': 7.513088034776572e-05, 'batch_size': 215, 'step_size': 11, 'gamma': 0.750866660608459}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:47:28,235][0m Trial 36 finished with value: 0.11070556193590164 and parameters: {'observation_period_num': 70, 'train_rates': 0.9328924377653504, 'learning_rate': 0.00043043044897390155, 'batch_size': 196, 'step_size': 14, 'gamma': 0.8104437474666525}. Best is trial 23 with value: 0.06216049566864967.[0m
Early stopping at epoch 95
[32m[I 2025-02-06 11:48:00,782][0m Trial 37 finished with value: 0.40149731694021484 and parameters: {'observation_period_num': 87, 'train_rates': 0.88528651631772, 'learning_rate': 3.06435340501163e-05, 'batch_size': 176, 'step_size': 1, 'gamma': 0.8896845553616916}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:48:38,440][0m Trial 38 finished with value: 0.18312764086332353 and parameters: {'observation_period_num': 22, 'train_rates': 0.7879439630545866, 'learning_rate': 0.0002636343328479903, 'batch_size': 150, 'step_size': 8, 'gamma': 0.8665446299937497}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:49:05,344][0m Trial 39 finished with value: 0.09189356118440628 and parameters: {'observation_period_num': 65, 'train_rates': 0.9694792134541492, 'learning_rate': 0.0009597273785427979, 'batch_size': 242, 'step_size': 12, 'gamma': 0.7854854809906489}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:50:21,334][0m Trial 40 finished with value: 0.08436794282608134 and parameters: {'observation_period_num': 103, 'train_rates': 0.9024339867594894, 'learning_rate': 0.00013178221165757476, 'batch_size': 74, 'step_size': 10, 'gamma': 0.8472101610161}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:50:46,066][0m Trial 41 finished with value: 0.06473248451948166 and parameters: {'observation_period_num': 20, 'train_rates': 0.9260706831582269, 'learning_rate': 0.00016138705400162474, 'batch_size': 250, 'step_size': 12, 'gamma': 0.8239157691603927}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:51:07,463][0m Trial 42 finished with value: 0.1739873174258268 and parameters: {'observation_period_num': 17, 'train_rates': 0.6114870753904726, 'learning_rate': 0.00016296456672402666, 'batch_size': 231, 'step_size': 13, 'gamma': 0.825415503493943}. Best is trial 23 with value: 0.06216049566864967.[0m
[32m[I 2025-02-06 11:51:32,512][0m Trial 43 finished with value: 0.05384292080998421 and parameters: {'observation_period_num': 33, 'train_rates': 0.9241565860016796, 'learning_rate': 0.0005829422794997776, 'batch_size': 255, 'step_size': 11, 'gamma': 0.8134075058886591}. Best is trial 43 with value: 0.05384292080998421.[0m
[32m[I 2025-02-06 11:51:57,314][0m Trial 44 finished with value: 0.04448765516281128 and parameters: {'observation_period_num': 5, 'train_rates': 0.9347384973681562, 'learning_rate': 0.00046268808265232037, 'batch_size': 254, 'step_size': 14, 'gamma': 0.8112639999753136}. Best is trial 44 with value: 0.04448765516281128.[0m
[32m[I 2025-02-06 11:52:24,126][0m Trial 45 finished with value: 0.05068008601665497 and parameters: {'observation_period_num': 6, 'train_rates': 0.9681363405513904, 'learning_rate': 0.0005323439069352668, 'batch_size': 254, 'step_size': 14, 'gamma': 0.8099552489176122}. Best is trial 44 with value: 0.04448765516281128.[0m
[32m[I 2025-02-06 11:52:50,665][0m Trial 46 finished with value: 0.047944579273462296 and parameters: {'observation_period_num': 5, 'train_rates': 0.9811544110062287, 'learning_rate': 0.0006345323983838698, 'batch_size': 255, 'step_size': 15, 'gamma': 0.810880549375984}. Best is trial 44 with value: 0.04448765516281128.[0m
[32m[I 2025-02-06 11:53:17,564][0m Trial 47 finished with value: 0.048418425023555756 and parameters: {'observation_period_num': 14, 'train_rates': 0.9773146309560711, 'learning_rate': 0.0005626070878578148, 'batch_size': 249, 'step_size': 15, 'gamma': 0.8135405309048042}. Best is trial 44 with value: 0.04448765516281128.[0m
[32m[I 2025-02-06 11:53:45,421][0m Trial 48 finished with value: 0.04525744915008545 and parameters: {'observation_period_num': 5, 'train_rates': 0.9812211806591943, 'learning_rate': 0.0005780912270713858, 'batch_size': 254, 'step_size': 15, 'gamma': 0.7628240531408697}. Best is trial 44 with value: 0.04448765516281128.[0m
[32m[I 2025-02-06 11:54:13,364][0m Trial 49 finished with value: 0.04231974482536316 and parameters: {'observation_period_num': 5, 'train_rates': 0.9779112984252135, 'learning_rate': 0.0007598525048823074, 'batch_size': 256, 'step_size': 15, 'gamma': 0.7614232702311378}. Best is trial 49 with value: 0.04231974482536316.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-06 11:54:13,374][0m A new study created in memory with name: no-name-da946aad-9071-4da5-89b5-d8a23ef6a5d1[0m
[32m[I 2025-02-06 11:54:34,136][0m Trial 0 finished with value: 0.7344475402381302 and parameters: {'observation_period_num': 113, 'train_rates': 0.6596451771256702, 'learning_rate': 1.8929028407274394e-06, 'batch_size': 237, 'step_size': 13, 'gamma': 0.9548372220809194}. Best is trial 0 with value: 0.7344475402381302.[0m
[32m[I 2025-02-06 11:55:00,037][0m Trial 1 finished with value: 0.30675414437619747 and parameters: {'observation_period_num': 220, 'train_rates': 0.7335399420539827, 'learning_rate': 0.0005442413158794094, 'batch_size': 195, 'step_size': 6, 'gamma': 0.9639395032617198}. Best is trial 1 with value: 0.30675414437619747.[0m
[32m[I 2025-02-06 11:56:06,997][0m Trial 2 finished with value: 0.1881895797835155 and parameters: {'observation_period_num': 51, 'train_rates': 0.6972879271889407, 'learning_rate': 0.00021874082620093735, 'batch_size': 72, 'step_size': 14, 'gamma': 0.935065702243943}. Best is trial 2 with value: 0.1881895797835155.[0m
[32m[I 2025-02-06 11:56:27,572][0m Trial 3 finished with value: 0.41798863926449337 and parameters: {'observation_period_num': 203, 'train_rates': 0.6640407855368365, 'learning_rate': 4.628764573881006e-05, 'batch_size': 242, 'step_size': 3, 'gamma': 0.8719165080984281}. Best is trial 2 with value: 0.1881895797835155.[0m
[32m[I 2025-02-06 11:56:58,408][0m Trial 4 finished with value: 0.24130582518844684 and parameters: {'observation_period_num': 138, 'train_rates': 0.7436459722406706, 'learning_rate': 0.00015963667787220867, 'batch_size': 168, 'step_size': 13, 'gamma': 0.8460833987757663}. Best is trial 2 with value: 0.1881895797835155.[0m
[32m[I 2025-02-06 11:58:44,940][0m Trial 5 finished with value: 0.21552965851654685 and parameters: {'observation_period_num': 107, 'train_rates': 0.6266384461793549, 'learning_rate': 9.982964090180871e-05, 'batch_size': 40, 'step_size': 7, 'gamma': 0.8821170071393702}. Best is trial 2 with value: 0.1881895797835155.[0m
[32m[I 2025-02-06 11:59:26,364][0m Trial 6 finished with value: 0.6919847848641612 and parameters: {'observation_period_num': 209, 'train_rates': 0.8007960949722526, 'learning_rate': 1.7756307619076325e-06, 'batch_size': 128, 'step_size': 13, 'gamma': 0.8107798573033745}. Best is trial 2 with value: 0.1881895797835155.[0m
[32m[I 2025-02-06 12:00:00,640][0m Trial 7 finished with value: 0.28278081572676306 and parameters: {'observation_period_num': 52, 'train_rates': 0.7818350357211381, 'learning_rate': 3.2772382764901254e-05, 'batch_size': 162, 'step_size': 2, 'gamma': 0.9395593280570605}. Best is trial 2 with value: 0.1881895797835155.[0m
[32m[I 2025-02-06 12:00:51,602][0m Trial 8 finished with value: 0.053527933088215914 and parameters: {'observation_period_num': 35, 'train_rates': 0.8307086104369726, 'learning_rate': 0.00010486261565311582, 'batch_size': 108, 'step_size': 9, 'gamma': 0.8043942482679036}. Best is trial 8 with value: 0.053527933088215914.[0m
[32m[I 2025-02-06 12:01:31,964][0m Trial 9 finished with value: 0.2563298041235187 and parameters: {'observation_period_num': 181, 'train_rates': 0.6993010746909094, 'learning_rate': 0.00022679261405446536, 'batch_size': 120, 'step_size': 3, 'gamma': 0.8612350690168975}. Best is trial 8 with value: 0.053527933088215914.[0m
[32m[I 2025-02-06 12:02:42,312][0m Trial 10 finished with value: 0.14642972062479276 and parameters: {'observation_period_num': 5, 'train_rates': 0.9176216683422347, 'learning_rate': 8.309794605975314e-06, 'batch_size': 83, 'step_size': 9, 'gamma': 0.7514356610429627}. Best is trial 8 with value: 0.053527933088215914.[0m
[32m[I 2025-02-06 12:03:58,233][0m Trial 11 finished with value: 0.13211615808901533 and parameters: {'observation_period_num': 7, 'train_rates': 0.9235669780236055, 'learning_rate': 6.775457087368555e-06, 'batch_size': 79, 'step_size': 10, 'gamma': 0.7548607256591284}. Best is trial 8 with value: 0.053527933088215914.[0m
[32m[I 2025-02-06 12:09:56,459][0m Trial 12 finished with value: 0.05055591448274239 and parameters: {'observation_period_num': 6, 'train_rates': 0.9359167545334859, 'learning_rate': 9.95184648154834e-06, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7562224531761766}. Best is trial 12 with value: 0.05055591448274239.[0m
[32m[I 2025-02-06 12:15:12,148][0m Trial 13 finished with value: 0.06840979496948421 and parameters: {'observation_period_num': 50, 'train_rates': 0.8626210317585317, 'learning_rate': 9.925435596512252e-06, 'batch_size': 17, 'step_size': 10, 'gamma': 0.7967397135534947}. Best is trial 12 with value: 0.05055591448274239.[0m
[32m[I 2025-02-06 12:17:02,115][0m Trial 14 finished with value: 0.11354312310113182 and parameters: {'observation_period_num': 82, 'train_rates': 0.862635984356896, 'learning_rate': 0.0008985935142816903, 'batch_size': 49, 'step_size': 5, 'gamma': 0.8002497124055687}. Best is trial 12 with value: 0.05055591448274239.[0m
[32m[I 2025-02-06 12:18:08,276][0m Trial 15 finished with value: 0.04730580747127533 and parameters: {'observation_period_num': 30, 'train_rates': 0.9843128184926682, 'learning_rate': 7.780565888696069e-05, 'batch_size': 95, 'step_size': 11, 'gamma': 0.7783433695551152}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:23:27,798][0m Trial 16 finished with value: 0.21770597082011553 and parameters: {'observation_period_num': 249, 'train_rates': 0.9819077199611411, 'learning_rate': 1.616965124778134e-05, 'batch_size': 17, 'step_size': 11, 'gamma': 0.7819375282361207}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:25:20,287][0m Trial 17 finished with value: 0.3603678300695599 and parameters: {'observation_period_num': 147, 'train_rates': 0.9813505456407728, 'learning_rate': 4.639336007964531e-06, 'batch_size': 52, 'step_size': 15, 'gamma': 0.905388149180621}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:26:19,125][0m Trial 18 finished with value: 0.13416830607510488 and parameters: {'observation_period_num': 75, 'train_rates': 0.9244668778502345, 'learning_rate': 2.0012463681628806e-05, 'batch_size': 100, 'step_size': 12, 'gamma': 0.8339434127103416}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:27:00,284][0m Trial 19 finished with value: 0.5847622156143188 and parameters: {'observation_period_num': 22, 'train_rates': 0.9531513037948431, 'learning_rate': 2.930584865893972e-06, 'batch_size': 153, 'step_size': 7, 'gamma': 0.776721532876823}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:27:30,194][0m Trial 20 finished with value: 0.09675930912739464 and parameters: {'observation_period_num': 77, 'train_rates': 0.8835657320817397, 'learning_rate': 5.6695537519079775e-05, 'batch_size': 199, 'step_size': 11, 'gamma': 0.9869920179044231}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:28:19,795][0m Trial 21 finished with value: 0.05305723547531295 and parameters: {'observation_period_num': 30, 'train_rates': 0.8110005441065087, 'learning_rate': 8.97451558413769e-05, 'batch_size': 111, 'step_size': 8, 'gamma': 0.8262927820381177}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:29:05,220][0m Trial 22 finished with value: 0.07288151234388351 and parameters: {'observation_period_num': 30, 'train_rates': 0.9567201581858963, 'learning_rate': 7.391836940618631e-05, 'batch_size': 140, 'step_size': 8, 'gamma': 0.8213765240569738}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:30:11,210][0m Trial 23 finished with value: 0.17136785277026764 and parameters: {'observation_period_num': 24, 'train_rates': 0.9041074588871547, 'learning_rate': 2.2500835353550213e-05, 'batch_size': 90, 'step_size': 5, 'gamma': 0.7741015463718234}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:31:41,679][0m Trial 24 finished with value: 0.06260006193802735 and parameters: {'observation_period_num': 60, 'train_rates': 0.8154153392894864, 'learning_rate': 0.00034899678803652716, 'batch_size': 58, 'step_size': 9, 'gamma': 0.7665357442554454}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:34:38,832][0m Trial 25 finished with value: 0.08410929792966598 and parameters: {'observation_period_num': 97, 'train_rates': 0.9455244102932086, 'learning_rate': 3.65783919467512e-05, 'batch_size': 32, 'step_size': 11, 'gamma': 0.8260841057627383}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:35:31,930][0m Trial 26 finished with value: 0.38205939531326294 and parameters: {'observation_period_num': 163, 'train_rates': 0.9885891194370268, 'learning_rate': 1.6629776124618285e-05, 'batch_size': 111, 'step_size': 8, 'gamma': 0.847535988804209}. Best is trial 15 with value: 0.04730580747127533.[0m
[32m[I 2025-02-06 12:36:55,356][0m Trial 27 finished with value: 0.04557605906678685 and parameters: {'observation_period_num': 35, 'train_rates': 0.8413493379518143, 'learning_rate': 0.00014036062852792674, 'batch_size': 66, 'step_size': 10, 'gamma': 0.7909849949771698}. Best is trial 27 with value: 0.04557605906678685.[0m
[32m[I 2025-02-06 12:38:23,490][0m Trial 28 finished with value: 0.03166180901667651 and parameters: {'observation_period_num': 10, 'train_rates': 0.8847429453582101, 'learning_rate': 0.00039336064384533397, 'batch_size': 64, 'step_size': 12, 'gamma': 0.7863620498775314}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:39:41,791][0m Trial 29 finished with value: 0.11192525091440711 and parameters: {'observation_period_num': 121, 'train_rates': 0.8455715794917813, 'learning_rate': 0.00040137362769360825, 'batch_size': 67, 'step_size': 12, 'gamma': 0.7885182258908209}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:40:33,843][0m Trial 30 finished with value: 0.2077374289417925 and parameters: {'observation_period_num': 93, 'train_rates': 0.772230002782505, 'learning_rate': 0.00016240579947742444, 'batch_size': 97, 'step_size': 15, 'gamma': 0.8127846253626166}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:43:15,887][0m Trial 31 finished with value: 0.06606815829211792 and parameters: {'observation_period_num': 6, 'train_rates': 0.885722118611348, 'learning_rate': 0.0008797636417789078, 'batch_size': 34, 'step_size': 12, 'gamma': 0.7656368139627856}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:44:50,344][0m Trial 32 finished with value: 0.6151667055051038 and parameters: {'observation_period_num': 43, 'train_rates': 0.8923456182693003, 'learning_rate': 1.120062048254815e-06, 'batch_size': 60, 'step_size': 10, 'gamma': 0.7504091849197004}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:46:07,115][0m Trial 33 finished with value: 0.08160092172826208 and parameters: {'observation_period_num': 64, 'train_rates': 0.9433056066856967, 'learning_rate': 0.00040741102885410085, 'batch_size': 77, 'step_size': 14, 'gamma': 0.789939231317235}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:49:06,961][0m Trial 34 finished with value: 0.04137355453345213 and parameters: {'observation_period_num': 14, 'train_rates': 0.8489490916725339, 'learning_rate': 0.00025091047962315713, 'batch_size': 30, 'step_size': 11, 'gamma': 0.7644387385835564}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:50:32,821][0m Trial 35 finished with value: 0.0387954432251198 and parameters: {'observation_period_num': 19, 'train_rates': 0.8569529202977494, 'learning_rate': 0.0002424388746798499, 'batch_size': 66, 'step_size': 14, 'gamma': 0.7708584942949556}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:52:53,719][0m Trial 36 finished with value: 0.03844452523895778 and parameters: {'observation_period_num': 19, 'train_rates': 0.835217076379846, 'learning_rate': 0.0002496511153170301, 'batch_size': 38, 'step_size': 14, 'gamma': 0.7635407046613316}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:55:37,720][0m Trial 37 finished with value: 0.04630123416252022 and parameters: {'observation_period_num': 18, 'train_rates': 0.8647602255390044, 'learning_rate': 0.00025825473823705956, 'batch_size': 33, 'step_size': 14, 'gamma': 0.7667439748304784}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:56:05,752][0m Trial 38 finished with value: 0.19752771184384393 and parameters: {'observation_period_num': 64, 'train_rates': 0.7597563086282892, 'learning_rate': 0.00027866765602499357, 'batch_size': 198, 'step_size': 13, 'gamma': 0.9055719762432018}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:58:02,824][0m Trial 39 finished with value: 0.07265332935243796 and parameters: {'observation_period_num': 48, 'train_rates': 0.7946970051372009, 'learning_rate': 0.0006924873600876581, 'batch_size': 44, 'step_size': 14, 'gamma': 0.7666260585939224}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:58:26,460][0m Trial 40 finished with value: 0.03679986414117891 and parameters: {'observation_period_num': 17, 'train_rates': 0.8339440165887682, 'learning_rate': 0.0005671050976415383, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8037851902485518}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:58:52,665][0m Trial 41 finished with value: 0.03854644104955703 and parameters: {'observation_period_num': 17, 'train_rates': 0.827135352642516, 'learning_rate': 0.0005501536646060666, 'batch_size': 222, 'step_size': 15, 'gamma': 0.8080707177551297}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:59:16,477][0m Trial 42 finished with value: 0.05937455262637623 and parameters: {'observation_period_num': 41, 'train_rates': 0.814589328766978, 'learning_rate': 0.0005208351570579709, 'batch_size': 255, 'step_size': 15, 'gamma': 0.8139092473375099}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 12:59:40,522][0m Trial 43 finished with value: 0.15064996436460695 and parameters: {'observation_period_num': 19, 'train_rates': 0.7262416207191527, 'learning_rate': 0.00048042512356263927, 'batch_size': 218, 'step_size': 13, 'gamma': 0.8376419844991952}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 13:00:06,576][0m Trial 44 finished with value: 0.06033261439735346 and parameters: {'observation_period_num': 56, 'train_rates': 0.8258826006681563, 'learning_rate': 0.000618151438083039, 'batch_size': 229, 'step_size': 15, 'gamma': 0.800037744271498}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 13:00:30,656][0m Trial 45 finished with value: 0.1978238663863187 and parameters: {'observation_period_num': 22, 'train_rates': 0.7947062556297344, 'learning_rate': 0.00017106162960144145, 'batch_size': 254, 'step_size': 14, 'gamma': 0.8641112717760863}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 13:01:02,835][0m Trial 46 finished with value: 0.04754338008800739 and parameters: {'observation_period_num': 41, 'train_rates': 0.8649096755872557, 'learning_rate': 0.0009828425868661287, 'batch_size': 182, 'step_size': 13, 'gamma': 0.850427584903837}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 13:01:29,940][0m Trial 47 finished with value: 0.059119009599089625 and parameters: {'observation_period_num': 11, 'train_rates': 0.9050539706965268, 'learning_rate': 0.00012302743003819077, 'batch_size': 238, 'step_size': 15, 'gamma': 0.8054878809916215}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 13:01:56,713][0m Trial 48 finished with value: 0.11612097794009793 and parameters: {'observation_period_num': 190, 'train_rates': 0.8318526728029682, 'learning_rate': 0.0003369989154899929, 'batch_size': 213, 'step_size': 14, 'gamma': 0.7860508831005406}. Best is trial 28 with value: 0.03166180901667651.[0m
[32m[I 2025-02-06 13:02:17,980][0m Trial 49 finished with value: 0.19002855384457173 and parameters: {'observation_period_num': 35, 'train_rates': 0.6008730190012761, 'learning_rate': 0.0006814588396215078, 'batch_size': 227, 'step_size': 14, 'gamma': 0.8826057088251075}. Best is trial 28 with value: 0.03166180901667651.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-06 13:02:17,991][0m A new study created in memory with name: no-name-a8a80bc9-ab10-47f3-ab5b-c4a16424b735[0m
[32m[I 2025-02-06 13:02:48,201][0m Trial 0 finished with value: 1.21889328956604 and parameters: {'observation_period_num': 249, 'train_rates': 0.9545688047929441, 'learning_rate': 1.4728769306543337e-06, 'batch_size': 200, 'step_size': 4, 'gamma': 0.8026699779814144}. Best is trial 0 with value: 1.21889328956604.[0m
[32m[I 2025-02-06 13:05:32,156][0m Trial 1 finished with value: 0.42592381334903495 and parameters: {'observation_period_num': 85, 'train_rates': 0.7642434406887199, 'learning_rate': 3.134685630668877e-06, 'batch_size': 30, 'step_size': 9, 'gamma': 0.8290675467369931}. Best is trial 1 with value: 0.42592381334903495.[0m
[32m[I 2025-02-06 13:06:01,421][0m Trial 2 finished with value: 0.24630058753571177 and parameters: {'observation_period_num': 104, 'train_rates': 0.6893097662292994, 'learning_rate': 0.0002885557781109416, 'batch_size': 170, 'step_size': 2, 'gamma': 0.8875011131462098}. Best is trial 2 with value: 0.24630058753571177.[0m
[32m[I 2025-02-06 13:06:25,951][0m Trial 3 finished with value: 0.20308571536630332 and parameters: {'observation_period_num': 245, 'train_rates': 0.8132571898991657, 'learning_rate': 5.0017442708745394e-05, 'batch_size': 238, 'step_size': 12, 'gamma': 0.9745014471752265}. Best is trial 3 with value: 0.20308571536630332.[0m
[32m[I 2025-02-06 13:06:58,243][0m Trial 4 finished with value: 0.9288602984000626 and parameters: {'observation_period_num': 187, 'train_rates': 0.8245050267630998, 'learning_rate': 1.2709602916825237e-06, 'batch_size': 164, 'step_size': 6, 'gamma': 0.8486078365372844}. Best is trial 3 with value: 0.20308571536630332.[0m
[32m[I 2025-02-06 13:08:25,169][0m Trial 5 finished with value: 0.28916735690636713 and parameters: {'observation_period_num': 216, 'train_rates': 0.7328712189751584, 'learning_rate': 0.00010019903546474954, 'batch_size': 54, 'step_size': 15, 'gamma': 0.82485681730801}. Best is trial 3 with value: 0.20308571536630332.[0m
[32m[I 2025-02-06 13:09:00,928][0m Trial 6 finished with value: 0.050291247835213486 and parameters: {'observation_period_num': 32, 'train_rates': 0.9097381798412624, 'learning_rate': 0.00016146940247428702, 'batch_size': 171, 'step_size': 13, 'gamma': 0.8112396478025837}. Best is trial 6 with value: 0.050291247835213486.[0m
[32m[I 2025-02-06 13:11:29,589][0m Trial 7 finished with value: 0.12589809198250396 and parameters: {'observation_period_num': 147, 'train_rates': 0.8374500971875023, 'learning_rate': 7.80634362373088e-05, 'batch_size': 35, 'step_size': 15, 'gamma': 0.782906186231953}. Best is trial 6 with value: 0.050291247835213486.[0m
[32m[I 2025-02-06 13:12:25,759][0m Trial 8 finished with value: 1.961938597125891 and parameters: {'observation_period_num': 90, 'train_rates': 0.6559290605121239, 'learning_rate': 1.149032260986232e-06, 'batch_size': 84, 'step_size': 3, 'gamma': 0.8126716229244404}. Best is trial 6 with value: 0.050291247835213486.[0m
[32m[I 2025-02-06 13:15:49,316][0m Trial 9 finished with value: 0.23366802126978842 and parameters: {'observation_period_num': 127, 'train_rates': 0.7269912891692125, 'learning_rate': 5.976782606533069e-05, 'batch_size': 23, 'step_size': 7, 'gamma': 0.9351240676206928}. Best is trial 6 with value: 0.050291247835213486.[0m
[32m[I 2025-02-06 13:16:47,259][0m Trial 10 finished with value: 0.03946893662214279 and parameters: {'observation_period_num': 9, 'train_rates': 0.9887352430182683, 'learning_rate': 0.0006894148134266101, 'batch_size': 111, 'step_size': 11, 'gamma': 0.7543427949461494}. Best is trial 10 with value: 0.03946893662214279.[0m
[32m[I 2025-02-06 13:17:46,669][0m Trial 11 finished with value: 0.024381058290600777 and parameters: {'observation_period_num': 10, 'train_rates': 0.9829782009317956, 'learning_rate': 0.0008824583713776, 'batch_size': 104, 'step_size': 11, 'gamma': 0.7542167200770462}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:18:46,107][0m Trial 12 finished with value: 0.03100944124162197 and parameters: {'observation_period_num': 16, 'train_rates': 0.9881893379570343, 'learning_rate': 0.0009987772852598767, 'batch_size': 106, 'step_size': 10, 'gamma': 0.7611743367759013}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:19:40,929][0m Trial 13 finished with value: 0.0810070220317881 and parameters: {'observation_period_num': 53, 'train_rates': 0.8985774789493187, 'learning_rate': 0.0008225556999904276, 'batch_size': 105, 'step_size': 9, 'gamma': 0.7508122539416034}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:20:26,078][0m Trial 14 finished with value: 0.0784764907262431 and parameters: {'observation_period_num': 5, 'train_rates': 0.9008950232905605, 'learning_rate': 1.4951510294475512e-05, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8731197117224377}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:21:47,964][0m Trial 15 finished with value: 0.05767800658941269 and parameters: {'observation_period_num': 54, 'train_rates': 0.9869721960154829, 'learning_rate': 0.0002968721467820219, 'batch_size': 76, 'step_size': 6, 'gamma': 0.7753537224840631}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:22:21,921][0m Trial 16 finished with value: 0.3799899098901102 and parameters: {'observation_period_num': 52, 'train_rates': 0.6035762282980979, 'learning_rate': 1.192467925677989e-05, 'batch_size': 135, 'step_size': 13, 'gamma': 0.9221591658915992}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:23:36,038][0m Trial 17 finished with value: 0.04815695351584161 and parameters: {'observation_period_num': 33, 'train_rates': 0.8713715319766179, 'learning_rate': 0.000465026226708369, 'batch_size': 76, 'step_size': 8, 'gamma': 0.7774808586422782}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:24:28,672][0m Trial 18 finished with value: 0.14499724489269836 and parameters: {'observation_period_num': 169, 'train_rates': 0.9409264120548168, 'learning_rate': 0.000966025351624933, 'batch_size': 112, 'step_size': 11, 'gamma': 0.8478298486468859}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:24:58,352][0m Trial 19 finished with value: 0.08276322484016418 and parameters: {'observation_period_num': 77, 'train_rates': 0.9365821819984687, 'learning_rate': 0.000198820225857037, 'batch_size': 217, 'step_size': 13, 'gamma': 0.788898005071191}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:26:37,730][0m Trial 20 finished with value: 0.0639127845749443 and parameters: {'observation_period_num': 26, 'train_rates': 0.8617322617375656, 'learning_rate': 1.747216469490852e-05, 'batch_size': 56, 'step_size': 5, 'gamma': 0.9032021299627864}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:27:44,376][0m Trial 21 finished with value: 0.027067208662629128 and parameters: {'observation_period_num': 6, 'train_rates': 0.9877528135040823, 'learning_rate': 0.0005178745006825533, 'batch_size': 96, 'step_size': 11, 'gamma': 0.7534721126548156}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:28:48,961][0m Trial 22 finished with value: 0.03083335642368739 and parameters: {'observation_period_num': 6, 'train_rates': 0.9557184722328933, 'learning_rate': 0.00044751121979695314, 'batch_size': 94, 'step_size': 11, 'gamma': 0.7687527207267477}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:29:30,883][0m Trial 23 finished with value: 0.04651133343577385 and parameters: {'observation_period_num': 46, 'train_rates': 0.9507780508352718, 'learning_rate': 0.0004116670403457213, 'batch_size': 150, 'step_size': 12, 'gamma': 0.7674327312201868}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:30:35,277][0m Trial 24 finished with value: 0.03307495805723914 and parameters: {'observation_period_num': 6, 'train_rates': 0.9214247209970503, 'learning_rate': 0.0001491970588732821, 'batch_size': 91, 'step_size': 14, 'gamma': 0.7960497271728083}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:32:13,012][0m Trial 25 finished with value: 0.08730466785479565 and parameters: {'observation_period_num': 65, 'train_rates': 0.9664225746975642, 'learning_rate': 0.00046487144000585505, 'batch_size': 62, 'step_size': 11, 'gamma': 0.7507643326013782}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:32:59,212][0m Trial 26 finished with value: 0.11635496999536242 and parameters: {'observation_period_num': 36, 'train_rates': 0.8778654171909266, 'learning_rate': 3.445432510387076e-05, 'batch_size': 124, 'step_size': 9, 'gamma': 0.7733728610967348}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:34:04,912][0m Trial 27 finished with value: 0.1010515992758704 and parameters: {'observation_period_num': 109, 'train_rates': 0.9573193298646965, 'learning_rate': 0.00025527768778707614, 'batch_size': 91, 'step_size': 8, 'gamma': 0.8428687171034792}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:34:44,779][0m Trial 28 finished with value: 0.03359114848932282 and parameters: {'observation_period_num': 22, 'train_rates': 0.9169831181737347, 'learning_rate': 0.000533557185566922, 'batch_size': 152, 'step_size': 12, 'gamma': 0.7939398695216654}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:35:17,703][0m Trial 29 finished with value: 0.6145498752593994 and parameters: {'observation_period_num': 69, 'train_rates': 0.9635321368880748, 'learning_rate': 4.856002615699369e-06, 'batch_size': 193, 'step_size': 10, 'gamma': 0.8035560053516257}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:37:30,777][0m Trial 30 finished with value: 0.04721971400655233 and parameters: {'observation_period_num': 41, 'train_rates': 0.937821796641504, 'learning_rate': 0.00011801790107597614, 'batch_size': 44, 'step_size': 14, 'gamma': 0.9852612104992073}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:38:32,212][0m Trial 31 finished with value: 0.029729275032877922 and parameters: {'observation_period_num': 18, 'train_rates': 0.9875988925313196, 'learning_rate': 0.0008548069747744926, 'batch_size': 100, 'step_size': 10, 'gamma': 0.7682796456804728}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:39:23,592][0m Trial 32 finished with value: 0.0346689447760582 and parameters: {'observation_period_num': 24, 'train_rates': 0.9722672657939967, 'learning_rate': 0.0006913698007902873, 'batch_size': 120, 'step_size': 10, 'gamma': 0.7669568000817045}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:40:20,707][0m Trial 33 finished with value: 0.16340237383275305 and parameters: {'observation_period_num': 18, 'train_rates': 0.7682576180569876, 'learning_rate': 0.0003304558733461014, 'batch_size': 93, 'step_size': 11, 'gamma': 0.7844898990453556}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:41:51,175][0m Trial 34 finished with value: 0.027588401375145746 and parameters: {'observation_period_num': 7, 'train_rates': 0.9705585302235497, 'learning_rate': 0.0005934642275080198, 'batch_size': 69, 'step_size': 9, 'gamma': 0.82233805693441}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:43:17,076][0m Trial 35 finished with value: 0.10000143601344182 and parameters: {'observation_period_num': 93, 'train_rates': 0.9728977999361378, 'learning_rate': 0.0006372031329779785, 'batch_size': 70, 'step_size': 9, 'gamma': 0.8375905794433373}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:45:28,210][0m Trial 36 finished with value: 0.06139914634717902 and parameters: {'observation_period_num': 61, 'train_rates': 0.9321211686416487, 'learning_rate': 0.00023837215311270257, 'batch_size': 44, 'step_size': 7, 'gamma': 0.8227893279266382}. Best is trial 11 with value: 0.024381058290600777.[0m
Early stopping at epoch 73
[32m[I 2025-02-06 13:46:30,512][0m Trial 37 finished with value: 1.702968700482295 and parameters: {'observation_period_num': 39, 'train_rates': 0.8887057026842298, 'learning_rate': 2.2552098803719604e-06, 'batch_size': 67, 'step_size': 1, 'gamma': 0.8632293082267462}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:51:54,848][0m Trial 38 finished with value: 0.15535858273506165 and parameters: {'observation_period_num': 251, 'train_rates': 0.9864704581757038, 'learning_rate': 0.00031874208458564085, 'batch_size': 17, 'step_size': 7, 'gamma': 0.8104533749780437}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:52:31,239][0m Trial 39 finished with value: 0.12737735873646588 and parameters: {'observation_period_num': 189, 'train_rates': 0.849290290581848, 'learning_rate': 0.00017431054152489212, 'batch_size': 148, 'step_size': 9, 'gamma': 0.8216195712808885}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:53:16,911][0m Trial 40 finished with value: 0.10968292500291552 and parameters: {'observation_period_num': 117, 'train_rates': 0.8150277852964627, 'learning_rate': 0.0006759182193908282, 'batch_size': 121, 'step_size': 12, 'gamma': 0.7596419942015917}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:54:19,072][0m Trial 41 finished with value: 0.030845053674232577 and parameters: {'observation_period_num': 16, 'train_rates': 0.9581177434703668, 'learning_rate': 0.0003778375532308135, 'batch_size': 100, 'step_size': 11, 'gamma': 0.7682375195866572}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:55:33,339][0m Trial 42 finished with value: 0.02947661048416787 and parameters: {'observation_period_num': 8, 'train_rates': 0.9233051058038106, 'learning_rate': 0.000526726610415226, 'batch_size': 82, 'step_size': 12, 'gamma': 0.7837549228221279}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:56:46,564][0m Trial 43 finished with value: 0.05673905000717283 and parameters: {'observation_period_num': 30, 'train_rates': 0.9269915972811096, 'learning_rate': 0.0009945818859141114, 'batch_size': 81, 'step_size': 14, 'gamma': 0.800623513469965}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:58:36,390][0m Trial 44 finished with value: 0.17900551855564117 and parameters: {'observation_period_num': 228, 'train_rates': 0.9753628185025925, 'learning_rate': 0.0006374001914235081, 'batch_size': 51, 'step_size': 12, 'gamma': 0.7840303694298539}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 13:59:44,592][0m Trial 45 finished with value: 0.04813237811493523 and parameters: {'observation_period_num': 16, 'train_rates': 0.9076879906373255, 'learning_rate': 8.298806984973415e-05, 'batch_size': 84, 'step_size': 10, 'gamma': 0.7546251587178581}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 14:02:38,458][0m Trial 46 finished with value: 0.0283941138395161 and parameters: {'observation_period_num': 5, 'train_rates': 0.9462315958971782, 'learning_rate': 0.0005362036204806941, 'batch_size': 34, 'step_size': 13, 'gamma': 0.8096575408555571}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 14:05:27,066][0m Trial 47 finished with value: 0.15631770322175875 and parameters: {'observation_period_num': 147, 'train_rates': 0.9480057083913719, 'learning_rate': 0.00012884031526327346, 'batch_size': 34, 'step_size': 15, 'gamma': 0.8083668647290361}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 14:05:51,345][0m Trial 48 finished with value: 0.1740324652801717 and parameters: {'observation_period_num': 5, 'train_rates': 0.7828037983361379, 'learning_rate': 0.00022640852374618243, 'batch_size': 249, 'step_size': 13, 'gamma': 0.861660677301803}. Best is trial 11 with value: 0.024381058290600777.[0m
[32m[I 2025-02-06 14:08:19,085][0m Trial 49 finished with value: 0.17276311812178832 and parameters: {'observation_period_num': 42, 'train_rates': 0.6665795245603441, 'learning_rate': 0.000543151686723676, 'batch_size': 31, 'step_size': 14, 'gamma': 0.8333431387367942}. Best is trial 11 with value: 0.024381058290600777.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-06 14:08:19,095][0m A new study created in memory with name: no-name-a6843431-dead-4d0c-a12d-a90d13f0f7ca[0m
[32m[I 2025-02-06 14:09:24,628][0m Trial 0 finished with value: 0.11982945049988394 and parameters: {'observation_period_num': 135, 'train_rates': 0.8012782644843397, 'learning_rate': 4.01144252296204e-05, 'batch_size': 79, 'step_size': 4, 'gamma': 0.9487440502800242}. Best is trial 0 with value: 0.11982945049988394.[0m
[32m[I 2025-02-06 14:09:46,620][0m Trial 1 finished with value: 0.400702520032947 and parameters: {'observation_period_num': 67, 'train_rates': 0.7535068014051811, 'learning_rate': 2.1370328850225273e-05, 'batch_size': 250, 'step_size': 1, 'gamma': 0.9783153698061499}. Best is trial 0 with value: 0.11982945049988394.[0m
[32m[I 2025-02-06 14:10:10,529][0m Trial 2 finished with value: 0.46869941486472705 and parameters: {'observation_period_num': 159, 'train_rates': 0.8926823271180306, 'learning_rate': 5.232239916178894e-06, 'batch_size': 250, 'step_size': 13, 'gamma': 0.8938549884740995}. Best is trial 0 with value: 0.11982945049988394.[0m
[32m[I 2025-02-06 14:10:35,604][0m Trial 3 finished with value: 1.4661389725117744 and parameters: {'observation_period_num': 172, 'train_rates': 0.6594143310403402, 'learning_rate': 2.9819117989897754e-06, 'batch_size': 192, 'step_size': 4, 'gamma': 0.80326153215458}. Best is trial 0 with value: 0.11982945049988394.[0m
[32m[I 2025-02-06 14:11:51,113][0m Trial 4 finished with value: 0.18445622286222377 and parameters: {'observation_period_num': 111, 'train_rates': 0.6166493320729675, 'learning_rate': 0.0001951950450577358, 'batch_size': 57, 'step_size': 7, 'gamma': 0.8812237308174281}. Best is trial 0 with value: 0.11982945049988394.[0m
[32m[I 2025-02-06 14:12:30,619][0m Trial 5 finished with value: 0.782098042448935 and parameters: {'observation_period_num': 237, 'train_rates': 0.6686561814318982, 'learning_rate': 2.410954592313372e-06, 'batch_size': 120, 'step_size': 4, 'gamma': 0.9123434361241274}. Best is trial 0 with value: 0.11982945049988394.[0m
[32m[I 2025-02-06 14:13:06,064][0m Trial 6 finished with value: 0.5151741513565405 and parameters: {'observation_period_num': 63, 'train_rates': 0.7686016552617839, 'learning_rate': 4.214161287167398e-06, 'batch_size': 158, 'step_size': 15, 'gamma': 0.7983093742054299}. Best is trial 0 with value: 0.11982945049988394.[0m
[32m[I 2025-02-06 14:14:21,266][0m Trial 7 finished with value: 0.6816319855310584 and parameters: {'observation_period_num': 93, 'train_rates': 0.967592516976522, 'learning_rate': 3.7452957967072373e-06, 'batch_size': 80, 'step_size': 7, 'gamma': 0.8307740962405985}. Best is trial 0 with value: 0.11982945049988394.[0m
[32m[I 2025-02-06 14:15:02,520][0m Trial 8 finished with value: 0.9925941228866577 and parameters: {'observation_period_num': 67, 'train_rates': 0.9769849929360406, 'learning_rate': 1.0060674984929897e-06, 'batch_size': 153, 'step_size': 8, 'gamma': 0.8463720562999762}. Best is trial 0 with value: 0.11982945049988394.[0m
[32m[I 2025-02-06 14:15:25,147][0m Trial 9 finished with value: 0.28091096876212107 and parameters: {'observation_period_num': 186, 'train_rates': 0.7075301293032766, 'learning_rate': 0.0003374334614974227, 'batch_size': 223, 'step_size': 5, 'gamma': 0.8013373094673905}. Best is trial 0 with value: 0.11982945049988394.[0m
[32m[I 2025-02-06 14:19:21,706][0m Trial 10 finished with value: 0.035693531507246586 and parameters: {'observation_period_num': 6, 'train_rates': 0.8571398974349551, 'learning_rate': 5.5402853072279025e-05, 'batch_size': 23, 'step_size': 11, 'gamma': 0.9857376168521476}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:23:22,945][0m Trial 11 finished with value: 0.038651055665896224 and parameters: {'observation_period_num': 19, 'train_rates': 0.8334957489402116, 'learning_rate': 6.91609290749203e-05, 'batch_size': 22, 'step_size': 11, 'gamma': 0.9851251772357518}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:28:22,513][0m Trial 12 finished with value: 0.04303277033233197 and parameters: {'observation_period_num': 14, 'train_rates': 0.8547944615899408, 'learning_rate': 7.389323329755244e-05, 'batch_size': 18, 'step_size': 11, 'gamma': 0.9619817640304684}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:34:03,267][0m Trial 13 finished with value: 0.03653747655027281 and parameters: {'observation_period_num': 11, 'train_rates': 0.8747909914992934, 'learning_rate': 0.0001227782332738445, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9324912607281634}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:35:58,834][0m Trial 14 finished with value: 0.041668049927990315 and parameters: {'observation_period_num': 5, 'train_rates': 0.9001217997742904, 'learning_rate': 0.000968634901340157, 'batch_size': 50, 'step_size': 11, 'gamma': 0.9301183008165184}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:36:57,645][0m Trial 15 finished with value: 0.14774575197892914 and parameters: {'observation_period_num': 42, 'train_rates': 0.9167872014480418, 'learning_rate': 1.527280056014545e-05, 'batch_size': 100, 'step_size': 13, 'gamma': 0.7524084693324966}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:39:04,583][0m Trial 16 finished with value: 0.0724896838701448 and parameters: {'observation_period_num': 44, 'train_rates': 0.8426998609036919, 'learning_rate': 0.00023678618959029415, 'batch_size': 43, 'step_size': 10, 'gamma': 0.9218038907564207}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:44:54,799][0m Trial 17 finished with value: 0.11599565266321103 and parameters: {'observation_period_num': 94, 'train_rates': 0.933006453261284, 'learning_rate': 0.0001203522029004818, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9522674009515705}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:46:07,450][0m Trial 18 finished with value: 0.09379091226231412 and parameters: {'observation_period_num': 36, 'train_rates': 0.8660569804246045, 'learning_rate': 1.0843936992759966e-05, 'batch_size': 77, 'step_size': 9, 'gamma': 0.9369954568640373}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:46:56,437][0m Trial 19 finished with value: 0.19951590900853874 and parameters: {'observation_period_num': 204, 'train_rates': 0.7918635644155233, 'learning_rate': 0.0007693426251863586, 'batch_size': 104, 'step_size': 13, 'gamma': 0.9044061251965292}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:48:53,407][0m Trial 20 finished with value: 0.09470961209453509 and parameters: {'observation_period_num': 143, 'train_rates': 0.9407123955197434, 'learning_rate': 4.166357110311201e-05, 'batch_size': 49, 'step_size': 12, 'gamma': 0.9879166377606725}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:51:57,838][0m Trial 21 finished with value: 0.04354293771978954 and parameters: {'observation_period_num': 23, 'train_rates': 0.8276057896064155, 'learning_rate': 6.325756972810921e-05, 'batch_size': 29, 'step_size': 10, 'gamma': 0.9716362468622127}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:54:50,401][0m Trial 22 finished with value: 0.03878016930641438 and parameters: {'observation_period_num': 7, 'train_rates': 0.8687834420300783, 'learning_rate': 8.77675994656684e-05, 'batch_size': 32, 'step_size': 9, 'gamma': 0.988283941330668}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 14:56:20,843][0m Trial 23 finished with value: 0.04778927430510521 and parameters: {'observation_period_num': 32, 'train_rates': 0.8291003461249852, 'learning_rate': 0.00015111921193978293, 'batch_size': 60, 'step_size': 11, 'gamma': 0.9574020663098}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 15:01:06,268][0m Trial 24 finished with value: 0.19473198829973873 and parameters: {'observation_period_num': 54, 'train_rates': 0.7450798469424723, 'learning_rate': 2.7964063630412505e-05, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9360752258893028}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 15:02:23,304][0m Trial 25 finished with value: 0.08819675997006424 and parameters: {'observation_period_num': 83, 'train_rates': 0.8058401692018982, 'learning_rate': 0.0002763181065459873, 'batch_size': 67, 'step_size': 12, 'gamma': 0.9683423705718529}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 15:05:04,194][0m Trial 26 finished with value: 0.039320288927954805 and parameters: {'observation_period_num': 24, 'train_rates': 0.8852352170025928, 'learning_rate': 0.00042996355605978, 'batch_size': 35, 'step_size': 9, 'gamma': 0.8605580377566601}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 15:06:00,449][0m Trial 27 finished with value: 0.21702543706918248 and parameters: {'observation_period_num': 119, 'train_rates': 0.7237922843239968, 'learning_rate': 5.1225654126610935e-05, 'batch_size': 93, 'step_size': 12, 'gamma': 0.9890889627355124}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 15:06:41,716][0m Trial 28 finished with value: 0.2790112534438022 and parameters: {'observation_period_num': 23, 'train_rates': 0.7838823011009617, 'learning_rate': 9.118290988585749e-06, 'batch_size': 132, 'step_size': 7, 'gamma': 0.9436242814282128}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 15:07:57,282][0m Trial 29 finished with value: 0.06181322537563943 and parameters: {'observation_period_num': 52, 'train_rates': 0.8088778420909309, 'learning_rate': 0.00010526958081813371, 'batch_size': 71, 'step_size': 10, 'gamma': 0.9515755831443556}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 15:10:42,114][0m Trial 30 finished with value: 0.036572494927574604 and parameters: {'observation_period_num': 5, 'train_rates': 0.9483263553304406, 'learning_rate': 3.069398932767749e-05, 'batch_size': 36, 'step_size': 14, 'gamma': 0.9132867149622139}. Best is trial 10 with value: 0.035693531507246586.[0m
[32m[I 2025-02-06 15:13:27,336][0m Trial 31 finished with value: 0.03491338194822365 and parameters: {'observation_period_num': 11, 'train_rates': 0.9462871126874793, 'learning_rate': 3.3741818066368953e-05, 'batch_size': 36, 'step_size': 14, 'gamma': 0.89247558460619}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:16:04,283][0m Trial 32 finished with value: 0.03970946181984343 and parameters: {'observation_period_num': 10, 'train_rates': 0.9429333197135129, 'learning_rate': 2.6812413343886e-05, 'batch_size': 38, 'step_size': 14, 'gamma': 0.8880830297644629}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:18:23,859][0m Trial 33 finished with value: 0.07801776379346848 and parameters: {'observation_period_num': 34, 'train_rates': 0.9890504993551238, 'learning_rate': 1.8313228420166464e-05, 'batch_size': 44, 'step_size': 14, 'gamma': 0.9103957690380852}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:20:04,482][0m Trial 34 finished with value: 0.07699239984331395 and parameters: {'observation_period_num': 74, 'train_rates': 0.9124285194011761, 'learning_rate': 3.6407010160509247e-05, 'batch_size': 58, 'step_size': 15, 'gamma': 0.8655769212754612}. Best is trial 31 with value: 0.03491338194822365.[0m
Early stopping at epoch 90
[32m[I 2025-02-06 15:21:08,299][0m Trial 35 finished with value: 0.6736654848542832 and parameters: {'observation_period_num': 52, 'train_rates': 0.9549107517597235, 'learning_rate': 1.2750510530073613e-05, 'batch_size': 86, 'step_size': 1, 'gamma': 0.878802324969028}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:21:39,157][0m Trial 36 finished with value: 0.14771942264550805 and parameters: {'observation_period_num': 5, 'train_rates': 0.8805531859001673, 'learning_rate': 6.699882477396024e-06, 'batch_size': 195, 'step_size': 13, 'gamma': 0.898472857435787}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:22:31,163][0m Trial 37 finished with value: 0.05739221229140027 and parameters: {'observation_period_num': 23, 'train_rates': 0.914580918261377, 'learning_rate': 0.00016155221769291515, 'batch_size': 112, 'step_size': 14, 'gamma': 0.9211086685659368}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:25:46,448][0m Trial 38 finished with value: 0.1756447821350421 and parameters: {'observation_period_num': 247, 'train_rates': 0.956435411834606, 'learning_rate': 2.582516292989461e-05, 'batch_size': 28, 'step_size': 12, 'gamma': 0.8457013525776897}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:27:16,235][0m Trial 39 finished with value: 0.1573102939289247 and parameters: {'observation_period_num': 144, 'train_rates': 0.9208717542817509, 'learning_rate': 4.1671028486106114e-05, 'batch_size': 64, 'step_size': 15, 'gamma': 0.9201589933073743}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:27:52,384][0m Trial 40 finished with value: 0.1396294814070493 and parameters: {'observation_period_num': 100, 'train_rates': 0.9025247216796718, 'learning_rate': 5.6985656489979454e-05, 'batch_size': 170, 'step_size': 6, 'gamma': 0.8916868472859792}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:31:26,709][0m Trial 41 finished with value: 0.04574623362025441 and parameters: {'observation_period_num': 17, 'train_rates': 0.8344853334591631, 'learning_rate': 0.00010919295875318106, 'batch_size': 25, 'step_size': 13, 'gamma': 0.9773819237780866}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:33:27,407][0m Trial 42 finished with value: 0.04010955492655436 and parameters: {'observation_period_num': 32, 'train_rates': 0.8615694981838774, 'learning_rate': 7.570715648945545e-05, 'batch_size': 45, 'step_size': 11, 'gamma': 0.8763360143833739}. Best is trial 31 with value: 0.03491338194822365.[0m
[32m[I 2025-02-06 15:39:11,250][0m Trial 43 finished with value: 0.03284219986561573 and parameters: {'observation_period_num': 17, 'train_rates': 0.8877015417672267, 'learning_rate': 2.004436287423855e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9656620346567689}. Best is trial 43 with value: 0.03284219986561573.[0m
[32m[I 2025-02-06 15:42:03,691][0m Trial 44 finished with value: 0.06179303997828637 and parameters: {'observation_period_num': 60, 'train_rates': 0.8883672406044829, 'learning_rate': 2.0264759371185e-05, 'batch_size': 32, 'step_size': 8, 'gamma': 0.9435663411438437}. Best is trial 43 with value: 0.03284219986561573.[0m
[32m[I 2025-02-06 15:43:52,534][0m Trial 45 finished with value: 0.06122637864806362 and parameters: {'observation_period_num': 43, 'train_rates': 0.9621166677087787, 'learning_rate': 3.5991749998447754e-05, 'batch_size': 55, 'step_size': 10, 'gamma': 0.9628534616732154}. Best is trial 43 with value: 0.03284219986561573.[0m
[32m[I 2025-02-06 15:49:32,754][0m Trial 46 finished with value: 0.03373658336335566 and parameters: {'observation_period_num': 5, 'train_rates': 0.9306261710096635, 'learning_rate': 1.8202748801900835e-05, 'batch_size': 17, 'step_size': 8, 'gamma': 0.9300569194235072}. Best is trial 43 with value: 0.03284219986561573.[0m
[32m[I 2025-02-06 15:55:24,795][0m Trial 47 finished with value: 0.07758173443550287 and parameters: {'observation_period_num': 74, 'train_rates': 0.925474083614279, 'learning_rate': 8.748044596767463e-06, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9692108709052287}. Best is trial 43 with value: 0.03284219986561573.[0m
[32m[I 2025-02-06 15:59:33,108][0m Trial 48 finished with value: 0.13308931448880365 and parameters: {'observation_period_num': 19, 'train_rates': 0.9768581452517423, 'learning_rate': 1.7651023429911995e-06, 'batch_size': 24, 'step_size': 9, 'gamma': 0.9297632021307695}. Best is trial 43 with value: 0.03284219986561573.[0m
[32m[I 2025-02-06 16:01:31,068][0m Trial 49 finished with value: 0.07916827007328103 and parameters: {'observation_period_num': 38, 'train_rates': 0.8470689944448321, 'learning_rate': 6.6661393878502706e-06, 'batch_size': 47, 'step_size': 7, 'gamma': 0.9780741464426023}. Best is trial 43 with value: 0.03284219986561573.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-06 16:01:31,079][0m A new study created in memory with name: no-name-46fb2909-dc9e-410e-99f6-3873e538bb45[0m
[32m[I 2025-02-06 16:01:59,349][0m Trial 0 finished with value: 0.7200115000091402 and parameters: {'observation_period_num': 211, 'train_rates': 0.6129549929186033, 'learning_rate': 1.910269377820498e-06, 'batch_size': 162, 'step_size': 6, 'gamma': 0.986278189052739}. Best is trial 0 with value: 0.7200115000091402.[0m
[32m[I 2025-02-06 16:02:34,618][0m Trial 1 finished with value: 0.11898402869701385 and parameters: {'observation_period_num': 61, 'train_rates': 0.9851995318218589, 'learning_rate': 0.0004950074993305836, 'batch_size': 186, 'step_size': 10, 'gamma': 0.9593637004778313}. Best is trial 1 with value: 0.11898402869701385.[0m
[32m[I 2025-02-06 16:03:05,301][0m Trial 2 finished with value: 0.17122463569337246 and parameters: {'observation_period_num': 125, 'train_rates': 0.8198938655788792, 'learning_rate': 2.4412618645214537e-05, 'batch_size': 181, 'step_size': 15, 'gamma': 0.9760918083363067}. Best is trial 1 with value: 0.11898402869701385.[0m
[32m[I 2025-02-06 16:04:32,532][0m Trial 3 finished with value: 0.028453392072174795 and parameters: {'observation_period_num': 18, 'train_rates': 0.973565516972446, 'learning_rate': 0.00045773746224681293, 'batch_size': 71, 'step_size': 5, 'gamma': 0.9022426810606876}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:04:55,942][0m Trial 4 finished with value: 0.1287389063351863 and parameters: {'observation_period_num': 175, 'train_rates': 0.8538634537156333, 'learning_rate': 0.0005674325904594736, 'batch_size': 242, 'step_size': 2, 'gamma': 0.9102664558651874}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:05:31,416][0m Trial 5 finished with value: 0.11192305386066437 and parameters: {'observation_period_num': 73, 'train_rates': 0.9839203029182648, 'learning_rate': 0.0009320385944935118, 'batch_size': 184, 'step_size': 10, 'gamma': 0.9310693928860079}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:06:15,799][0m Trial 6 finished with value: 0.44510559703168073 and parameters: {'observation_period_num': 124, 'train_rates': 0.6494452531667739, 'learning_rate': 3.677101701931202e-05, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9097984290773076}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:06:39,972][0m Trial 7 finished with value: 0.09373703354330205 and parameters: {'observation_period_num': 44, 'train_rates': 0.8391910547027633, 'learning_rate': 4.2042906153125295e-05, 'batch_size': 254, 'step_size': 15, 'gamma': 0.8464996118123269}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:08:02,191][0m Trial 8 finished with value: 0.4355583873610146 and parameters: {'observation_period_num': 231, 'train_rates': 0.6553610891158889, 'learning_rate': 2.5027335280713782e-05, 'batch_size': 51, 'step_size': 4, 'gamma': 0.7818282116858655}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:08:27,805][0m Trial 9 finished with value: 1.4666521549224854 and parameters: {'observation_period_num': 146, 'train_rates': 0.9609861683049676, 'learning_rate': 3.178447613958248e-06, 'batch_size': 247, 'step_size': 2, 'gamma': 0.8351130053483263}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:13:13,976][0m Trial 10 finished with value: 0.14601866566357363 and parameters: {'observation_period_num': 11, 'train_rates': 0.7428504001893566, 'learning_rate': 0.00016049012041986907, 'batch_size': 17, 'step_size': 6, 'gamma': 0.7550380205032311}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:14:18,297][0m Trial 11 finished with value: 0.03759995897981658 and parameters: {'observation_period_num': 8, 'train_rates': 0.8866211632351144, 'learning_rate': 0.00011249584548558278, 'batch_size': 92, 'step_size': 15, 'gamma': 0.8494903921099425}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:15:22,699][0m Trial 12 finished with value: 0.0364283955159503 and parameters: {'observation_period_num': 12, 'train_rates': 0.9026240560237193, 'learning_rate': 0.00023084301732666815, 'batch_size': 94, 'step_size': 12, 'gamma': 0.8704234387863253}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:16:24,612][0m Trial 13 finished with value: 0.0938031522580135 and parameters: {'observation_period_num': 93, 'train_rates': 0.9152237656291413, 'learning_rate': 0.00019173252003154914, 'batch_size': 96, 'step_size': 11, 'gamma': 0.8818245582578965}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:18:04,032][0m Trial 14 finished with value: 0.18507833611435456 and parameters: {'observation_period_num': 36, 'train_rates': 0.7632334929050015, 'learning_rate': 0.00030793639967055796, 'batch_size': 51, 'step_size': 12, 'gamma': 0.879930296803407}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:18:51,112][0m Trial 15 finished with value: 0.10401065422678893 and parameters: {'observation_period_num': 91, 'train_rates': 0.9078097463898852, 'learning_rate': 8.144094391209036e-05, 'batch_size': 129, 'step_size': 8, 'gamma': 0.8202240158157507}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:20:21,357][0m Trial 16 finished with value: 0.19880181332246974 and parameters: {'observation_period_num': 37, 'train_rates': 0.9254724940558908, 'learning_rate': 6.100118598476332e-06, 'batch_size': 66, 'step_size': 5, 'gamma': 0.9332575115468126}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:21:09,604][0m Trial 17 finished with value: 0.15087341287253817 and parameters: {'observation_period_num': 12, 'train_rates': 0.9452278970882616, 'learning_rate': 9.534364472243275e-06, 'batch_size': 130, 'step_size': 13, 'gamma': 0.7995074898568002}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:25:52,190][0m Trial 18 finished with value: 0.12675166188304957 and parameters: {'observation_period_num': 91, 'train_rates': 0.8706285377349214, 'learning_rate': 0.0003446999049244369, 'batch_size': 19, 'step_size': 4, 'gamma': 0.8743096881101818}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:26:59,403][0m Trial 19 finished with value: 0.4103870667591443 and parameters: {'observation_period_num': 170, 'train_rates': 0.7840090100879542, 'learning_rate': 5.617295282427011e-05, 'batch_size': 74, 'step_size': 1, 'gamma': 0.9162894628860159}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:27:34,718][0m Trial 20 finished with value: 0.23479832020235358 and parameters: {'observation_period_num': 59, 'train_rates': 0.722043921951787, 'learning_rate': 0.0009213000562471032, 'batch_size': 153, 'step_size': 9, 'gamma': 0.8976010246028985}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:28:39,107][0m Trial 21 finished with value: 0.04004855106609899 and parameters: {'observation_period_num': 12, 'train_rates': 0.8869395653488892, 'learning_rate': 0.00011054074872045466, 'batch_size': 89, 'step_size': 13, 'gamma': 0.8527248357789641}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:29:35,087][0m Trial 22 finished with value: 0.037128929117763484 and parameters: {'observation_period_num': 5, 'train_rates': 0.9452754942366368, 'learning_rate': 0.00022032169584685488, 'batch_size': 111, 'step_size': 14, 'gamma': 0.8596493392612983}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:30:25,276][0m Trial 23 finished with value: 0.08345322451881461 and parameters: {'observation_period_num': 34, 'train_rates': 0.9495725692943179, 'learning_rate': 0.00021117380634385688, 'batch_size': 121, 'step_size': 13, 'gamma': 0.86363025268179}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:32:40,774][0m Trial 24 finished with value: 0.0418445686512488 and parameters: {'observation_period_num': 25, 'train_rates': 0.9361774640100443, 'learning_rate': 0.0003999486013971671, 'batch_size': 43, 'step_size': 7, 'gamma': 0.8218902374481433}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:33:41,648][0m Trial 25 finished with value: 0.07837910950183868 and parameters: {'observation_period_num': 58, 'train_rates': 0.989837572578243, 'learning_rate': 0.00023374684559384148, 'batch_size': 105, 'step_size': 14, 'gamma': 0.8914544338111346}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:34:51,132][0m Trial 26 finished with value: 0.15374962603572473 and parameters: {'observation_period_num': 77, 'train_rates': 0.9037637676724373, 'learning_rate': 0.0006318702587293627, 'batch_size': 82, 'step_size': 11, 'gamma': 0.9461306051209702}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:35:44,783][0m Trial 27 finished with value: 0.050797425593265284 and parameters: {'observation_period_num': 5, 'train_rates': 0.9600539634696169, 'learning_rate': 7.067923978309127e-05, 'batch_size': 116, 'step_size': 12, 'gamma': 0.8693588762642667}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:36:24,993][0m Trial 28 finished with value: 0.12543590168613905 and parameters: {'observation_period_num': 25, 'train_rates': 0.8142329693664557, 'learning_rate': 1.56008040900624e-05, 'batch_size': 145, 'step_size': 14, 'gamma': 0.8255582357450867}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:37:50,610][0m Trial 29 finished with value: 0.051663774529168774 and parameters: {'observation_period_num': 48, 'train_rates': 0.8522038499524028, 'learning_rate': 0.00013937817833957914, 'batch_size': 64, 'step_size': 5, 'gamma': 0.805237952652931}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:38:24,439][0m Trial 30 finished with value: 0.14682276895413032 and parameters: {'observation_period_num': 197, 'train_rates': 0.8823872233655798, 'learning_rate': 0.00027733849722105585, 'batch_size': 168, 'step_size': 7, 'gamma': 0.9017173695626264}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:39:32,891][0m Trial 31 finished with value: 0.8267751121029412 and parameters: {'observation_period_num': 23, 'train_rates': 0.9009056579222223, 'learning_rate': 1.1205907506035576e-06, 'batch_size': 86, 'step_size': 15, 'gamma': 0.8533876187306506}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:42:27,320][0m Trial 32 finished with value: 0.03242025277154012 and parameters: {'observation_period_num': 5, 'train_rates': 0.962740974557509, 'learning_rate': 0.00010781838524585219, 'batch_size': 34, 'step_size': 14, 'gamma': 0.8392309369707558}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:46:01,400][0m Trial 33 finished with value: 0.04464369420619572 and parameters: {'observation_period_num': 24, 'train_rates': 0.9652204245391124, 'learning_rate': 0.0006088334402608676, 'batch_size': 28, 'step_size': 14, 'gamma': 0.8357306250976851}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:46:30,822][0m Trial 34 finished with value: 0.09552446007728577 and parameters: {'observation_period_num': 55, 'train_rates': 0.92668321370201, 'learning_rate': 9.446278136777092e-05, 'batch_size': 218, 'step_size': 12, 'gamma': 0.8632636787565456}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:49:01,826][0m Trial 35 finished with value: 0.15352196156978606 and parameters: {'observation_period_num': 110, 'train_rates': 0.9739694017982145, 'learning_rate': 0.0005400501558711923, 'batch_size': 39, 'step_size': 10, 'gamma': 0.8894184307988773}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:50:33,173][0m Trial 36 finished with value: 0.10813998430967331 and parameters: {'observation_period_num': 65, 'train_rates': 0.9891984377973748, 'learning_rate': 5.5796082663842215e-05, 'batch_size': 68, 'step_size': 11, 'gamma': 0.8031274876108971}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:51:29,880][0m Trial 37 finished with value: 0.09424700338148072 and parameters: {'observation_period_num': 41, 'train_rates': 0.9428961025267268, 'learning_rate': 0.0004421616464289513, 'batch_size': 107, 'step_size': 14, 'gamma': 0.9829306568672928}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:53:09,699][0m Trial 38 finished with value: 0.06614514769352897 and parameters: {'observation_period_num': 76, 'train_rates': 0.8276626863773309, 'learning_rate': 0.00015346096779196656, 'batch_size': 53, 'step_size': 9, 'gamma': 0.9648854044864908}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:56:14,624][0m Trial 39 finished with value: 0.03239945411135297 and parameters: {'observation_period_num': 5, 'train_rates': 0.9631636383489631, 'learning_rate': 0.00029459848015490225, 'batch_size': 32, 'step_size': 3, 'gamma': 0.921548425205181}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 16:59:15,508][0m Trial 40 finished with value: 0.04020306095480919 and parameters: {'observation_period_num': 22, 'train_rates': 0.9692603731364738, 'learning_rate': 0.000740870711863291, 'batch_size': 33, 'step_size': 2, 'gamma': 0.9282172961262879}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 17:01:01,845][0m Trial 41 finished with value: 0.047014864591451794 and parameters: {'observation_period_num': 18, 'train_rates': 0.9293578781273181, 'learning_rate': 0.0002619865530755853, 'batch_size': 56, 'step_size': 3, 'gamma': 0.917651617330722}. Best is trial 3 with value: 0.028453392072174795.[0m
[32m[I 2025-02-06 17:02:20,057][0m Trial 42 finished with value: 0.025181299399422562 and parameters: {'observation_period_num': 6, 'train_rates': 0.9533132134579991, 'learning_rate': 0.0004139196016946088, 'batch_size': 76, 'step_size': 3, 'gamma': 0.9407542909309634}. Best is trial 42 with value: 0.025181299399422562.[0m
[32m[I 2025-02-06 17:03:38,564][0m Trial 43 finished with value: 0.047394112683832645 and parameters: {'observation_period_num': 32, 'train_rates': 0.9702003718854275, 'learning_rate': 0.00038024607541076367, 'batch_size': 77, 'step_size': 3, 'gamma': 0.9454403050859862}. Best is trial 42 with value: 0.025181299399422562.[0m
[32m[I 2025-02-06 17:06:06,677][0m Trial 44 finished with value: 0.1690358114966438 and parameters: {'observation_period_num': 46, 'train_rates': 0.6130203830721725, 'learning_rate': 0.0009039625697542163, 'batch_size': 29, 'step_size': 1, 'gamma': 0.9558086997105762}. Best is trial 42 with value: 0.025181299399422562.[0m
[32m[I 2025-02-06 17:08:14,545][0m Trial 45 finished with value: 0.18861610775015183 and parameters: {'observation_period_num': 230, 'train_rates': 0.9591872049211663, 'learning_rate': 0.0004637935308467836, 'batch_size': 43, 'step_size': 3, 'gamma': 0.9054510302962598}. Best is trial 42 with value: 0.025181299399422562.[0m
[32m[I 2025-02-06 17:09:52,389][0m Trial 46 finished with value: 0.04280328089523999 and parameters: {'observation_period_num': 15, 'train_rates': 0.9171495877536394, 'learning_rate': 0.00016089936323851407, 'batch_size': 59, 'step_size': 4, 'gamma': 0.9229188053335241}. Best is trial 42 with value: 0.025181299399422562.[0m
[32m[I 2025-02-06 17:15:09,416][0m Trial 47 finished with value: 0.04987967514161822 and parameters: {'observation_period_num': 33, 'train_rates': 0.8649419801497318, 'learning_rate': 3.1488997575453376e-05, 'batch_size': 17, 'step_size': 5, 'gamma': 0.9330797976423607}. Best is trial 42 with value: 0.025181299399422562.[0m
[32m[I 2025-02-06 17:16:14,335][0m Trial 48 finished with value: 0.06727831065654755 and parameters: {'observation_period_num': 48, 'train_rates': 0.97984178513768, 'learning_rate': 0.00012110744972953869, 'batch_size': 97, 'step_size': 6, 'gamma': 0.8389371613691191}. Best is trial 42 with value: 0.025181299399422562.[0m
[32m[I 2025-02-06 17:17:29,461][0m Trial 49 finished with value: 0.1233538983383059 and parameters: {'observation_period_num': 143, 'train_rates': 0.9010000518877634, 'learning_rate': 0.00029233133012562974, 'batch_size': 73, 'step_size': 2, 'gamma': 0.9699416090861968}. Best is trial 42 with value: 0.025181299399422562.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 15, 'train_rates': 0.939860573530533, 'learning_rate': 0.00025142320948791635, 'batch_size': 55, 'step_size': 7, 'gamma': 0.762150358644485}
Epoch 1/300, trend Loss: 0.2333 | 0.1773
Epoch 2/300, trend Loss: 0.1478 | 0.1467
Epoch 3/300, trend Loss: 0.1316 | 0.1498
Epoch 4/300, trend Loss: 0.1177 | 0.1711
Epoch 5/300, trend Loss: 0.1174 | 0.1036
Epoch 6/300, trend Loss: 0.1158 | 0.0968
Epoch 7/300, trend Loss: 0.1045 | 0.0835
Epoch 8/300, trend Loss: 0.0991 | 0.0793
Epoch 9/300, trend Loss: 0.0988 | 0.0779
Epoch 10/300, trend Loss: 0.0957 | 0.0756
Epoch 11/300, trend Loss: 0.0949 | 0.0725
Epoch 12/300, trend Loss: 0.0938 | 0.0687
Epoch 13/300, trend Loss: 0.0929 | 0.0679
Epoch 14/300, trend Loss: 0.0893 | 0.0670
Epoch 15/300, trend Loss: 0.0872 | 0.0599
Epoch 16/300, trend Loss: 0.0860 | 0.0585
Epoch 17/300, trend Loss: 0.0848 | 0.0573
Epoch 18/300, trend Loss: 0.0835 | 0.0559
Epoch 19/300, trend Loss: 0.0819 | 0.0512
Epoch 20/300, trend Loss: 0.0807 | 0.0502
Epoch 21/300, trend Loss: 0.0801 | 0.0494
Epoch 22/300, trend Loss: 0.0815 | 0.0509
Epoch 23/300, trend Loss: 0.0825 | 0.0492
Epoch 24/300, trend Loss: 0.0835 | 0.0489
Epoch 25/300, trend Loss: 0.0831 | 0.0498
Epoch 26/300, trend Loss: 0.0819 | 0.0503
Epoch 27/300, trend Loss: 0.0795 | 0.0490
Epoch 28/300, trend Loss: 0.0772 | 0.0482
Epoch 29/300, trend Loss: 0.0775 | 0.0452
Epoch 30/300, trend Loss: 0.0786 | 0.0456
Epoch 31/300, trend Loss: 0.0793 | 0.0458
Epoch 32/300, trend Loss: 0.0796 | 0.0470
Epoch 33/300, trend Loss: 0.0791 | 0.0568
Epoch 34/300, trend Loss: 0.0773 | 0.0517
Epoch 35/300, trend Loss: 0.0755 | 0.0476
Epoch 36/300, trend Loss: 0.0763 | 0.0468
Epoch 37/300, trend Loss: 0.0777 | 0.0473
Epoch 38/300, trend Loss: 0.0773 | 0.0463
Epoch 39/300, trend Loss: 0.0757 | 0.0448
Epoch 40/300, trend Loss: 0.0742 | 0.0439
Epoch 41/300, trend Loss: 0.0729 | 0.0434
Epoch 42/300, trend Loss: 0.0724 | 0.0431
Epoch 43/300, trend Loss: 0.0725 | 0.0432
Epoch 44/300, trend Loss: 0.0726 | 0.0431
Epoch 45/300, trend Loss: 0.0726 | 0.0429
Epoch 46/300, trend Loss: 0.0726 | 0.0427
Epoch 47/300, trend Loss: 0.0726 | 0.0427
Epoch 48/300, trend Loss: 0.0725 | 0.0425
Epoch 49/300, trend Loss: 0.0723 | 0.0422
Epoch 50/300, trend Loss: 0.0722 | 0.0422
Epoch 51/300, trend Loss: 0.0718 | 0.0420
Epoch 52/300, trend Loss: 0.0715 | 0.0419
Epoch 53/300, trend Loss: 0.0713 | 0.0419
Epoch 54/300, trend Loss: 0.0712 | 0.0419
Epoch 55/300, trend Loss: 0.0710 | 0.0418
Epoch 56/300, trend Loss: 0.0710 | 0.0418
Epoch 57/300, trend Loss: 0.0709 | 0.0419
Epoch 58/300, trend Loss: 0.0708 | 0.0418
Epoch 59/300, trend Loss: 0.0707 | 0.0418
Epoch 60/300, trend Loss: 0.0707 | 0.0418
Epoch 61/300, trend Loss: 0.0706 | 0.0418
Epoch 62/300, trend Loss: 0.0706 | 0.0417
Epoch 63/300, trend Loss: 0.0705 | 0.0417
Epoch 64/300, trend Loss: 0.0705 | 0.0417
Epoch 65/300, trend Loss: 0.0705 | 0.0417
Epoch 66/300, trend Loss: 0.0704 | 0.0416
Epoch 67/300, trend Loss: 0.0704 | 0.0416
Epoch 68/300, trend Loss: 0.0704 | 0.0416
Epoch 69/300, trend Loss: 0.0704 | 0.0416
Epoch 70/300, trend Loss: 0.0703 | 0.0416
Epoch 71/300, trend Loss: 0.0703 | 0.0416
Epoch 72/300, trend Loss: 0.0703 | 0.0415
Epoch 73/300, trend Loss: 0.0703 | 0.0415
Epoch 74/300, trend Loss: 0.0703 | 0.0415
Epoch 75/300, trend Loss: 0.0703 | 0.0415
Epoch 76/300, trend Loss: 0.0702 | 0.0415
Epoch 77/300, trend Loss: 0.0702 | 0.0415
Epoch 78/300, trend Loss: 0.0702 | 0.0415
Epoch 79/300, trend Loss: 0.0702 | 0.0414
Epoch 80/300, trend Loss: 0.0702 | 0.0414
Epoch 81/300, trend Loss: 0.0702 | 0.0414
Epoch 82/300, trend Loss: 0.0702 | 0.0414
Epoch 83/300, trend Loss: 0.0702 | 0.0414
Epoch 84/300, trend Loss: 0.0702 | 0.0414
Epoch 85/300, trend Loss: 0.0701 | 0.0414
Epoch 86/300, trend Loss: 0.0701 | 0.0414
Epoch 87/300, trend Loss: 0.0701 | 0.0414
Epoch 88/300, trend Loss: 0.0701 | 0.0414
Epoch 89/300, trend Loss: 0.0701 | 0.0414
Epoch 90/300, trend Loss: 0.0701 | 0.0414
Epoch 91/300, trend Loss: 0.0701 | 0.0414
Epoch 92/300, trend Loss: 0.0701 | 0.0414
Epoch 93/300, trend Loss: 0.0701 | 0.0414
Epoch 94/300, trend Loss: 0.0701 | 0.0414
Epoch 95/300, trend Loss: 0.0701 | 0.0413
Epoch 96/300, trend Loss: 0.0701 | 0.0413
Epoch 97/300, trend Loss: 0.0701 | 0.0413
Epoch 98/300, trend Loss: 0.0701 | 0.0413
Epoch 99/300, trend Loss: 0.0701 | 0.0413
Epoch 100/300, trend Loss: 0.0701 | 0.0413
Epoch 101/300, trend Loss: 0.0701 | 0.0413
Epoch 102/300, trend Loss: 0.0701 | 0.0413
Epoch 103/300, trend Loss: 0.0701 | 0.0413
Epoch 104/300, trend Loss: 0.0701 | 0.0413
Epoch 105/300, trend Loss: 0.0701 | 0.0413
Epoch 106/300, trend Loss: 0.0701 | 0.0413
Epoch 107/300, trend Loss: 0.0701 | 0.0413
Epoch 108/300, trend Loss: 0.0701 | 0.0413
Epoch 109/300, trend Loss: 0.0701 | 0.0413
Epoch 110/300, trend Loss: 0.0701 | 0.0413
Epoch 111/300, trend Loss: 0.0701 | 0.0413
Epoch 112/300, trend Loss: 0.0701 | 0.0413
Epoch 113/300, trend Loss: 0.0701 | 0.0413
Epoch 114/300, trend Loss: 0.0701 | 0.0413
Epoch 115/300, trend Loss: 0.0701 | 0.0413
Epoch 116/300, trend Loss: 0.0701 | 0.0413
Epoch 117/300, trend Loss: 0.0701 | 0.0413
Epoch 118/300, trend Loss: 0.0701 | 0.0413
Epoch 119/300, trend Loss: 0.0701 | 0.0413
Epoch 120/300, trend Loss: 0.0700 | 0.0413
Epoch 121/300, trend Loss: 0.0700 | 0.0413
Epoch 122/300, trend Loss: 0.0700 | 0.0413
Epoch 123/300, trend Loss: 0.0700 | 0.0413
Epoch 124/300, trend Loss: 0.0700 | 0.0413
Epoch 125/300, trend Loss: 0.0700 | 0.0413
Epoch 126/300, trend Loss: 0.0700 | 0.0413
Epoch 127/300, trend Loss: 0.0700 | 0.0413
Epoch 128/300, trend Loss: 0.0700 | 0.0413
Epoch 129/300, trend Loss: 0.0700 | 0.0413
Epoch 130/300, trend Loss: 0.0700 | 0.0413
Epoch 131/300, trend Loss: 0.0700 | 0.0413
Epoch 132/300, trend Loss: 0.0700 | 0.0413
Epoch 133/300, trend Loss: 0.0700 | 0.0413
Epoch 134/300, trend Loss: 0.0700 | 0.0413
Epoch 135/300, trend Loss: 0.0700 | 0.0413
Epoch 136/300, trend Loss: 0.0700 | 0.0413
Epoch 137/300, trend Loss: 0.0700 | 0.0413
Epoch 138/300, trend Loss: 0.0700 | 0.0413
Epoch 139/300, trend Loss: 0.0700 | 0.0413
Epoch 140/300, trend Loss: 0.0700 | 0.0413
Epoch 141/300, trend Loss: 0.0700 | 0.0413
Epoch 142/300, trend Loss: 0.0700 | 0.0413
Epoch 143/300, trend Loss: 0.0700 | 0.0413
Epoch 144/300, trend Loss: 0.0700 | 0.0413
Epoch 145/300, trend Loss: 0.0700 | 0.0413
Epoch 146/300, trend Loss: 0.0700 | 0.0413
Epoch 147/300, trend Loss: 0.0700 | 0.0413
Epoch 148/300, trend Loss: 0.0700 | 0.0413
Epoch 149/300, trend Loss: 0.0700 | 0.0413
Epoch 150/300, trend Loss: 0.0700 | 0.0413
Epoch 151/300, trend Loss: 0.0700 | 0.0413
Epoch 152/300, trend Loss: 0.0700 | 0.0413
Epoch 153/300, trend Loss: 0.0700 | 0.0413
Epoch 154/300, trend Loss: 0.0700 | 0.0413
Epoch 155/300, trend Loss: 0.0700 | 0.0413
Epoch 156/300, trend Loss: 0.0700 | 0.0413
Epoch 157/300, trend Loss: 0.0700 | 0.0413
Epoch 158/300, trend Loss: 0.0700 | 0.0413
Epoch 159/300, trend Loss: 0.0700 | 0.0413
Epoch 160/300, trend Loss: 0.0700 | 0.0413
Epoch 161/300, trend Loss: 0.0700 | 0.0413
Epoch 162/300, trend Loss: 0.0700 | 0.0413
Epoch 163/300, trend Loss: 0.0700 | 0.0413
Epoch 164/300, trend Loss: 0.0700 | 0.0413
Epoch 165/300, trend Loss: 0.0700 | 0.0413
Epoch 166/300, trend Loss: 0.0700 | 0.0413
Epoch 167/300, trend Loss: 0.0700 | 0.0413
Epoch 168/300, trend Loss: 0.0700 | 0.0413
Epoch 169/300, trend Loss: 0.0700 | 0.0413
Epoch 170/300, trend Loss: 0.0700 | 0.0413
Epoch 171/300, trend Loss: 0.0700 | 0.0413
Epoch 172/300, trend Loss: 0.0700 | 0.0413
Epoch 173/300, trend Loss: 0.0700 | 0.0413
Epoch 174/300, trend Loss: 0.0700 | 0.0413
Epoch 175/300, trend Loss: 0.0700 | 0.0413
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.9779112984252135, 'learning_rate': 0.0007598525048823074, 'batch_size': 256, 'step_size': 15, 'gamma': 0.7614232702311378}
Epoch 1/300, seasonal_0 Loss: 0.4369 | 0.2064
Epoch 2/300, seasonal_0 Loss: 0.1916 | 0.1702
Epoch 3/300, seasonal_0 Loss: 0.2624 | 0.1772
Epoch 4/300, seasonal_0 Loss: 0.2930 | 0.4640
Epoch 5/300, seasonal_0 Loss: 0.2107 | 0.1905
Epoch 6/300, seasonal_0 Loss: 0.2715 | 0.3410
Epoch 7/300, seasonal_0 Loss: 0.2875 | 0.2225
Epoch 8/300, seasonal_0 Loss: 0.2044 | 0.1703
Epoch 9/300, seasonal_0 Loss: 0.3082 | 0.1696
Epoch 10/300, seasonal_0 Loss: 0.1960 | 0.1518
Epoch 11/300, seasonal_0 Loss: 0.1748 | 0.1149
Epoch 12/300, seasonal_0 Loss: 0.1583 | 0.1300
Epoch 13/300, seasonal_0 Loss: 0.1565 | 0.1139
Epoch 14/300, seasonal_0 Loss: 0.1571 | 0.1652
Epoch 15/300, seasonal_0 Loss: 0.1435 | 0.1124
Epoch 16/300, seasonal_0 Loss: 0.1260 | 0.1057
Epoch 17/300, seasonal_0 Loss: 0.1163 | 0.0874
Epoch 18/300, seasonal_0 Loss: 0.1101 | 0.0816
Epoch 19/300, seasonal_0 Loss: 0.1068 | 0.0789
Epoch 20/300, seasonal_0 Loss: 0.1070 | 0.0809
Epoch 21/300, seasonal_0 Loss: 0.1051 | 0.0765
Epoch 22/300, seasonal_0 Loss: 0.1048 | 0.0777
Epoch 23/300, seasonal_0 Loss: 0.1040 | 0.0743
Epoch 24/300, seasonal_0 Loss: 0.1038 | 0.0774
Epoch 25/300, seasonal_0 Loss: 0.1040 | 0.0781
Epoch 26/300, seasonal_0 Loss: 0.1049 | 0.0788
Epoch 27/300, seasonal_0 Loss: 0.1049 | 0.0794
Epoch 28/300, seasonal_0 Loss: 0.1046 | 0.0782
Epoch 29/300, seasonal_0 Loss: 0.1033 | 0.0763
Epoch 30/300, seasonal_0 Loss: 0.1015 | 0.0750
Epoch 31/300, seasonal_0 Loss: 0.0985 | 0.0727
Epoch 32/300, seasonal_0 Loss: 0.0960 | 0.0714
Epoch 33/300, seasonal_0 Loss: 0.0937 | 0.0660
Epoch 34/300, seasonal_0 Loss: 0.0921 | 0.0670
Epoch 35/300, seasonal_0 Loss: 0.0900 | 0.0633
Epoch 36/300, seasonal_0 Loss: 0.0890 | 0.0638
Epoch 37/300, seasonal_0 Loss: 0.0879 | 0.0622
Epoch 38/300, seasonal_0 Loss: 0.0873 | 0.0612
Epoch 39/300, seasonal_0 Loss: 0.0868 | 0.0620
Epoch 40/300, seasonal_0 Loss: 0.0865 | 0.0610
Epoch 41/300, seasonal_0 Loss: 0.0863 | 0.0613
Epoch 42/300, seasonal_0 Loss: 0.0863 | 0.0610
Epoch 43/300, seasonal_0 Loss: 0.0861 | 0.0610
Epoch 44/300, seasonal_0 Loss: 0.0860 | 0.0610
Epoch 45/300, seasonal_0 Loss: 0.0857 | 0.0605
Epoch 46/300, seasonal_0 Loss: 0.0850 | 0.0603
Epoch 47/300, seasonal_0 Loss: 0.0842 | 0.0590
Epoch 48/300, seasonal_0 Loss: 0.0836 | 0.0575
Epoch 49/300, seasonal_0 Loss: 0.0831 | 0.0576
Epoch 50/300, seasonal_0 Loss: 0.0828 | 0.0567
Epoch 51/300, seasonal_0 Loss: 0.0826 | 0.0570
Epoch 52/300, seasonal_0 Loss: 0.0825 | 0.0566
Epoch 53/300, seasonal_0 Loss: 0.0825 | 0.0568
Epoch 54/300, seasonal_0 Loss: 0.0828 | 0.0593
Epoch 55/300, seasonal_0 Loss: 0.0846 | 0.0610
Epoch 56/300, seasonal_0 Loss: 0.0866 | 0.0563
Epoch 57/300, seasonal_0 Loss: 0.0840 | 0.0568
Epoch 58/300, seasonal_0 Loss: 0.0822 | 0.0560
Epoch 59/300, seasonal_0 Loss: 0.0814 | 0.0555
Epoch 60/300, seasonal_0 Loss: 0.0813 | 0.0552
Epoch 61/300, seasonal_0 Loss: 0.0811 | 0.0550
Epoch 62/300, seasonal_0 Loss: 0.0809 | 0.0549
Epoch 63/300, seasonal_0 Loss: 0.0808 | 0.0547
Epoch 64/300, seasonal_0 Loss: 0.0807 | 0.0545
Epoch 65/300, seasonal_0 Loss: 0.0806 | 0.0544
Epoch 66/300, seasonal_0 Loss: 0.0805 | 0.0542
Epoch 67/300, seasonal_0 Loss: 0.0803 | 0.0540
Epoch 68/300, seasonal_0 Loss: 0.0802 | 0.0539
Epoch 69/300, seasonal_0 Loss: 0.0801 | 0.0538
Epoch 70/300, seasonal_0 Loss: 0.0800 | 0.0537
Epoch 71/300, seasonal_0 Loss: 0.0799 | 0.0535
Epoch 72/300, seasonal_0 Loss: 0.0798 | 0.0534
Epoch 73/300, seasonal_0 Loss: 0.0797 | 0.0532
Epoch 74/300, seasonal_0 Loss: 0.0796 | 0.0531
Epoch 75/300, seasonal_0 Loss: 0.0795 | 0.0529
Epoch 76/300, seasonal_0 Loss: 0.0794 | 0.0528
Epoch 77/300, seasonal_0 Loss: 0.0793 | 0.0527
Epoch 78/300, seasonal_0 Loss: 0.0792 | 0.0526
Epoch 79/300, seasonal_0 Loss: 0.0791 | 0.0525
Epoch 80/300, seasonal_0 Loss: 0.0790 | 0.0523
Epoch 81/300, seasonal_0 Loss: 0.0789 | 0.0522
Epoch 82/300, seasonal_0 Loss: 0.0788 | 0.0520
Epoch 83/300, seasonal_0 Loss: 0.0787 | 0.0519
Epoch 84/300, seasonal_0 Loss: 0.0786 | 0.0519
Epoch 85/300, seasonal_0 Loss: 0.0785 | 0.0517
Epoch 86/300, seasonal_0 Loss: 0.0785 | 0.0516
Epoch 87/300, seasonal_0 Loss: 0.0784 | 0.0515
Epoch 88/300, seasonal_0 Loss: 0.0783 | 0.0514
Epoch 89/300, seasonal_0 Loss: 0.0783 | 0.0513
Epoch 90/300, seasonal_0 Loss: 0.0782 | 0.0512
Epoch 91/300, seasonal_0 Loss: 0.0781 | 0.0512
Epoch 92/300, seasonal_0 Loss: 0.0781 | 0.0510
Epoch 93/300, seasonal_0 Loss: 0.0780 | 0.0510
Epoch 94/300, seasonal_0 Loss: 0.0780 | 0.0509
Epoch 95/300, seasonal_0 Loss: 0.0779 | 0.0508
Epoch 96/300, seasonal_0 Loss: 0.0778 | 0.0507
Epoch 97/300, seasonal_0 Loss: 0.0778 | 0.0506
Epoch 98/300, seasonal_0 Loss: 0.0777 | 0.0506
Epoch 99/300, seasonal_0 Loss: 0.0777 | 0.0505
Epoch 100/300, seasonal_0 Loss: 0.0776 | 0.0504
Epoch 101/300, seasonal_0 Loss: 0.0776 | 0.0504
Epoch 102/300, seasonal_0 Loss: 0.0775 | 0.0503
Epoch 103/300, seasonal_0 Loss: 0.0775 | 0.0503
Epoch 104/300, seasonal_0 Loss: 0.0775 | 0.0502
Epoch 105/300, seasonal_0 Loss: 0.0774 | 0.0501
Epoch 106/300, seasonal_0 Loss: 0.0774 | 0.0501
Epoch 107/300, seasonal_0 Loss: 0.0773 | 0.0501
Epoch 108/300, seasonal_0 Loss: 0.0773 | 0.0500
Epoch 109/300, seasonal_0 Loss: 0.0773 | 0.0500
Epoch 110/300, seasonal_0 Loss: 0.0772 | 0.0499
Epoch 111/300, seasonal_0 Loss: 0.0772 | 0.0499
Epoch 112/300, seasonal_0 Loss: 0.0772 | 0.0498
Epoch 113/300, seasonal_0 Loss: 0.0771 | 0.0498
Epoch 114/300, seasonal_0 Loss: 0.0771 | 0.0498
Epoch 115/300, seasonal_0 Loss: 0.0771 | 0.0497
Epoch 116/300, seasonal_0 Loss: 0.0771 | 0.0497
Epoch 117/300, seasonal_0 Loss: 0.0770 | 0.0497
Epoch 118/300, seasonal_0 Loss: 0.0770 | 0.0496
Epoch 119/300, seasonal_0 Loss: 0.0770 | 0.0496
Epoch 120/300, seasonal_0 Loss: 0.0770 | 0.0496
Epoch 121/300, seasonal_0 Loss: 0.0769 | 0.0496
Epoch 122/300, seasonal_0 Loss: 0.0769 | 0.0495
Epoch 123/300, seasonal_0 Loss: 0.0769 | 0.0495
Epoch 124/300, seasonal_0 Loss: 0.0769 | 0.0495
Epoch 125/300, seasonal_0 Loss: 0.0769 | 0.0495
Epoch 126/300, seasonal_0 Loss: 0.0768 | 0.0494
Epoch 127/300, seasonal_0 Loss: 0.0768 | 0.0494
Epoch 128/300, seasonal_0 Loss: 0.0768 | 0.0494
Epoch 129/300, seasonal_0 Loss: 0.0768 | 0.0494
Epoch 130/300, seasonal_0 Loss: 0.0768 | 0.0493
Epoch 131/300, seasonal_0 Loss: 0.0767 | 0.0493
Epoch 132/300, seasonal_0 Loss: 0.0767 | 0.0493
Epoch 133/300, seasonal_0 Loss: 0.0767 | 0.0493
Epoch 134/300, seasonal_0 Loss: 0.0767 | 0.0493
Epoch 135/300, seasonal_0 Loss: 0.0767 | 0.0492
Epoch 136/300, seasonal_0 Loss: 0.0767 | 0.0492
Epoch 137/300, seasonal_0 Loss: 0.0767 | 0.0492
Epoch 138/300, seasonal_0 Loss: 0.0766 | 0.0492
Epoch 139/300, seasonal_0 Loss: 0.0766 | 0.0492
Epoch 140/300, seasonal_0 Loss: 0.0766 | 0.0492
Epoch 141/300, seasonal_0 Loss: 0.0766 | 0.0492
Epoch 142/300, seasonal_0 Loss: 0.0766 | 0.0492
Epoch 143/300, seasonal_0 Loss: 0.0766 | 0.0491
Epoch 144/300, seasonal_0 Loss: 0.0766 | 0.0491
Epoch 145/300, seasonal_0 Loss: 0.0766 | 0.0491
Epoch 146/300, seasonal_0 Loss: 0.0766 | 0.0491
Epoch 147/300, seasonal_0 Loss: 0.0766 | 0.0491
Epoch 148/300, seasonal_0 Loss: 0.0765 | 0.0491
Epoch 149/300, seasonal_0 Loss: 0.0765 | 0.0491
Epoch 150/300, seasonal_0 Loss: 0.0765 | 0.0491
Epoch 151/300, seasonal_0 Loss: 0.0765 | 0.0491
Epoch 152/300, seasonal_0 Loss: 0.0765 | 0.0490
Epoch 153/300, seasonal_0 Loss: 0.0765 | 0.0490
Epoch 154/300, seasonal_0 Loss: 0.0765 | 0.0490
Epoch 155/300, seasonal_0 Loss: 0.0765 | 0.0490
Epoch 156/300, seasonal_0 Loss: 0.0765 | 0.0490
Epoch 157/300, seasonal_0 Loss: 0.0765 | 0.0490
Epoch 158/300, seasonal_0 Loss: 0.0765 | 0.0490
Epoch 159/300, seasonal_0 Loss: 0.0765 | 0.0490
Epoch 160/300, seasonal_0 Loss: 0.0765 | 0.0490
Epoch 161/300, seasonal_0 Loss: 0.0764 | 0.0490
Epoch 162/300, seasonal_0 Loss: 0.0764 | 0.0490
Epoch 163/300, seasonal_0 Loss: 0.0764 | 0.0490
Epoch 164/300, seasonal_0 Loss: 0.0764 | 0.0490
Epoch 165/300, seasonal_0 Loss: 0.0764 | 0.0490
Epoch 166/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 167/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 168/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 169/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 170/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 171/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 172/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 173/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 174/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 175/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 176/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 177/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 178/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 179/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 180/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 181/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 182/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 183/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 184/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 185/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 186/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 187/300, seasonal_0 Loss: 0.0763 | 0.0489
Epoch 188/300, seasonal_0 Loss: 0.0763 | 0.0489
Epoch 189/300, seasonal_0 Loss: 0.0763 | 0.0489
Epoch 190/300, seasonal_0 Loss: 0.0763 | 0.0489
Epoch 191/300, seasonal_0 Loss: 0.0763 | 0.0489
Epoch 192/300, seasonal_0 Loss: 0.0763 | 0.0489
Epoch 193/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 194/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 195/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 196/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 197/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 198/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 199/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 200/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 201/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 202/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 203/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 204/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 205/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 206/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 207/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 208/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 209/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 210/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 211/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 212/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 213/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 214/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 215/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 216/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 217/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 218/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 219/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 220/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 221/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 222/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 223/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 224/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 225/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 226/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 227/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 228/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 229/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 230/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 231/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 232/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 233/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 234/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 235/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 236/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 237/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 238/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 239/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 240/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 241/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 242/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 243/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 244/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 245/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 246/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 247/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 248/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 249/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 250/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 251/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 252/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 253/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 254/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 255/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 256/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 257/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 258/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 259/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 260/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 261/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 262/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 263/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 264/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 265/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 266/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 267/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 268/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 269/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 270/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 271/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 272/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 273/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 274/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 275/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 276/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 277/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 278/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 279/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 280/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 281/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 282/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 283/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 284/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 285/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 286/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 287/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 288/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 289/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 290/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 291/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 292/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 293/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 294/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 295/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 296/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 297/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 298/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 299/300, seasonal_0 Loss: 0.0763 | 0.0488
Epoch 300/300, seasonal_0 Loss: 0.0763 | 0.0488
Training seasonal_1 component with params: {'observation_period_num': 10, 'train_rates': 0.8847429453582101, 'learning_rate': 0.00039336064384533397, 'batch_size': 64, 'step_size': 12, 'gamma': 0.7863620498775314}
Epoch 1/300, seasonal_1 Loss: 0.2101 | 0.1299
Epoch 2/300, seasonal_1 Loss: 0.1377 | 0.1117
Epoch 3/300, seasonal_1 Loss: 0.1228 | 0.1081
Epoch 4/300, seasonal_1 Loss: 0.1084 | 0.0735
Epoch 5/300, seasonal_1 Loss: 0.1047 | 0.0683
Epoch 6/300, seasonal_1 Loss: 0.1017 | 0.0626
Epoch 7/300, seasonal_1 Loss: 0.1007 | 0.0586
Epoch 8/300, seasonal_1 Loss: 0.1007 | 0.0555
Epoch 9/300, seasonal_1 Loss: 0.0890 | 0.0499
Epoch 10/300, seasonal_1 Loss: 0.0871 | 0.0491
Epoch 11/300, seasonal_1 Loss: 0.0905 | 0.0458
Epoch 12/300, seasonal_1 Loss: 0.0854 | 0.0453
Epoch 13/300, seasonal_1 Loss: 0.0849 | 0.0440
Epoch 14/300, seasonal_1 Loss: 0.0825 | 0.0458
Epoch 15/300, seasonal_1 Loss: 0.0826 | 0.0470
Epoch 16/300, seasonal_1 Loss: 0.0850 | 0.0410
Epoch 17/300, seasonal_1 Loss: 0.0793 | 0.0447
Epoch 18/300, seasonal_1 Loss: 0.0784 | 0.0450
Epoch 19/300, seasonal_1 Loss: 0.0799 | 0.0413
Epoch 20/300, seasonal_1 Loss: 0.0768 | 0.0508
Epoch 21/300, seasonal_1 Loss: 0.0755 | 0.0527
Epoch 22/300, seasonal_1 Loss: 0.0728 | 0.0394
Epoch 23/300, seasonal_1 Loss: 0.0720 | 0.0411
Epoch 24/300, seasonal_1 Loss: 0.0718 | 0.0426
Epoch 25/300, seasonal_1 Loss: 0.0713 | 0.0408
Epoch 26/300, seasonal_1 Loss: 0.0712 | 0.0404
Epoch 27/300, seasonal_1 Loss: 0.0707 | 0.0397
Epoch 28/300, seasonal_1 Loss: 0.0700 | 0.0386
Epoch 29/300, seasonal_1 Loss: 0.0697 | 0.0391
Epoch 30/300, seasonal_1 Loss: 0.0695 | 0.0390
Epoch 31/300, seasonal_1 Loss: 0.0693 | 0.0356
Epoch 32/300, seasonal_1 Loss: 0.0698 | 0.0342
Epoch 33/300, seasonal_1 Loss: 0.0697 | 0.0339
Epoch 34/300, seasonal_1 Loss: 0.0701 | 0.0343
Epoch 35/300, seasonal_1 Loss: 0.0708 | 0.0346
Epoch 36/300, seasonal_1 Loss: 0.0714 | 0.0348
Epoch 37/300, seasonal_1 Loss: 0.0728 | 0.0376
Epoch 38/300, seasonal_1 Loss: 0.0742 | 0.0374
Epoch 39/300, seasonal_1 Loss: 0.0711 | 0.0362
Epoch 40/300, seasonal_1 Loss: 0.0684 | 0.0367
Epoch 41/300, seasonal_1 Loss: 0.0671 | 0.0341
Epoch 42/300, seasonal_1 Loss: 0.0671 | 0.0331
Epoch 43/300, seasonal_1 Loss: 0.0684 | 0.0382
Epoch 44/300, seasonal_1 Loss: 0.0702 | 0.0370
Epoch 45/300, seasonal_1 Loss: 0.0681 | 0.0342
Epoch 46/300, seasonal_1 Loss: 0.0667 | 0.0335
Epoch 47/300, seasonal_1 Loss: 0.0677 | 0.0345
Epoch 48/300, seasonal_1 Loss: 0.0662 | 0.0340
Epoch 49/300, seasonal_1 Loss: 0.0661 | 0.0373
Epoch 50/300, seasonal_1 Loss: 0.0657 | 0.0352
Epoch 51/300, seasonal_1 Loss: 0.0649 | 0.0334
Epoch 52/300, seasonal_1 Loss: 0.0640 | 0.0319
Epoch 53/300, seasonal_1 Loss: 0.0632 | 0.0308
Epoch 54/300, seasonal_1 Loss: 0.0624 | 0.0299
Epoch 55/300, seasonal_1 Loss: 0.0621 | 0.0297
Epoch 56/300, seasonal_1 Loss: 0.0619 | 0.0289
Epoch 57/300, seasonal_1 Loss: 0.0623 | 0.0287
Epoch 58/300, seasonal_1 Loss: 0.0659 | 0.0351
Epoch 59/300, seasonal_1 Loss: 0.0650 | 0.0343
Epoch 60/300, seasonal_1 Loss: 0.0626 | 0.0299
Epoch 61/300, seasonal_1 Loss: 0.0621 | 0.0291
Epoch 62/300, seasonal_1 Loss: 0.0618 | 0.0287
Epoch 63/300, seasonal_1 Loss: 0.0613 | 0.0283
Epoch 64/300, seasonal_1 Loss: 0.0609 | 0.0280
Epoch 65/300, seasonal_1 Loss: 0.0606 | 0.0277
Epoch 66/300, seasonal_1 Loss: 0.0603 | 0.0275
Epoch 67/300, seasonal_1 Loss: 0.0603 | 0.0274
Epoch 68/300, seasonal_1 Loss: 0.0602 | 0.0273
Epoch 69/300, seasonal_1 Loss: 0.0602 | 0.0272
Epoch 70/300, seasonal_1 Loss: 0.0601 | 0.0271
Epoch 71/300, seasonal_1 Loss: 0.0601 | 0.0271
Epoch 72/300, seasonal_1 Loss: 0.0600 | 0.0270
Epoch 73/300, seasonal_1 Loss: 0.0600 | 0.0268
Epoch 74/300, seasonal_1 Loss: 0.0600 | 0.0266
Epoch 75/300, seasonal_1 Loss: 0.0599 | 0.0267
Epoch 76/300, seasonal_1 Loss: 0.0599 | 0.0267
Epoch 77/300, seasonal_1 Loss: 0.0598 | 0.0267
Epoch 78/300, seasonal_1 Loss: 0.0598 | 0.0267
Epoch 79/300, seasonal_1 Loss: 0.0598 | 0.0266
Epoch 80/300, seasonal_1 Loss: 0.0598 | 0.0266
Epoch 81/300, seasonal_1 Loss: 0.0597 | 0.0266
Epoch 82/300, seasonal_1 Loss: 0.0596 | 0.0266
Epoch 83/300, seasonal_1 Loss: 0.0594 | 0.0265
Epoch 84/300, seasonal_1 Loss: 0.0593 | 0.0265
Epoch 85/300, seasonal_1 Loss: 0.0593 | 0.0266
Epoch 86/300, seasonal_1 Loss: 0.0593 | 0.0266
Epoch 87/300, seasonal_1 Loss: 0.0591 | 0.0265
Epoch 88/300, seasonal_1 Loss: 0.0590 | 0.0265
Epoch 89/300, seasonal_1 Loss: 0.0588 | 0.0265
Epoch 90/300, seasonal_1 Loss: 0.0587 | 0.0265
Epoch 91/300, seasonal_1 Loss: 0.0587 | 0.0266
Epoch 92/300, seasonal_1 Loss: 0.0586 | 0.0265
Epoch 93/300, seasonal_1 Loss: 0.0585 | 0.0265
Epoch 94/300, seasonal_1 Loss: 0.0584 | 0.0265
Epoch 95/300, seasonal_1 Loss: 0.0583 | 0.0264
Epoch 96/300, seasonal_1 Loss: 0.0582 | 0.0264
Epoch 97/300, seasonal_1 Loss: 0.0581 | 0.0264
Epoch 98/300, seasonal_1 Loss: 0.0581 | 0.0263
Epoch 99/300, seasonal_1 Loss: 0.0580 | 0.0263
Epoch 100/300, seasonal_1 Loss: 0.0579 | 0.0262
Epoch 101/300, seasonal_1 Loss: 0.0579 | 0.0262
Epoch 102/300, seasonal_1 Loss: 0.0578 | 0.0262
Epoch 103/300, seasonal_1 Loss: 0.0577 | 0.0261
Epoch 104/300, seasonal_1 Loss: 0.0577 | 0.0261
Epoch 105/300, seasonal_1 Loss: 0.0577 | 0.0260
Epoch 106/300, seasonal_1 Loss: 0.0576 | 0.0260
Epoch 107/300, seasonal_1 Loss: 0.0576 | 0.0260
Epoch 108/300, seasonal_1 Loss: 0.0576 | 0.0260
Epoch 109/300, seasonal_1 Loss: 0.0575 | 0.0260
Epoch 110/300, seasonal_1 Loss: 0.0575 | 0.0259
Epoch 111/300, seasonal_1 Loss: 0.0575 | 0.0259
Epoch 112/300, seasonal_1 Loss: 0.0575 | 0.0259
Epoch 113/300, seasonal_1 Loss: 0.0574 | 0.0259
Epoch 114/300, seasonal_1 Loss: 0.0574 | 0.0259
Epoch 115/300, seasonal_1 Loss: 0.0574 | 0.0259
Epoch 116/300, seasonal_1 Loss: 0.0574 | 0.0259
Epoch 117/300, seasonal_1 Loss: 0.0574 | 0.0259
Epoch 118/300, seasonal_1 Loss: 0.0573 | 0.0259
Epoch 119/300, seasonal_1 Loss: 0.0573 | 0.0258
Epoch 120/300, seasonal_1 Loss: 0.0573 | 0.0258
Epoch 121/300, seasonal_1 Loss: 0.0573 | 0.0258
Epoch 122/300, seasonal_1 Loss: 0.0573 | 0.0258
Epoch 123/300, seasonal_1 Loss: 0.0573 | 0.0258
Epoch 124/300, seasonal_1 Loss: 0.0572 | 0.0258
Epoch 125/300, seasonal_1 Loss: 0.0572 | 0.0258
Epoch 126/300, seasonal_1 Loss: 0.0572 | 0.0258
Epoch 127/300, seasonal_1 Loss: 0.0572 | 0.0258
Epoch 128/300, seasonal_1 Loss: 0.0572 | 0.0258
Epoch 129/300, seasonal_1 Loss: 0.0572 | 0.0258
Epoch 130/300, seasonal_1 Loss: 0.0572 | 0.0258
Epoch 131/300, seasonal_1 Loss: 0.0571 | 0.0257
Epoch 132/300, seasonal_1 Loss: 0.0571 | 0.0257
Epoch 133/300, seasonal_1 Loss: 0.0571 | 0.0257
Epoch 134/300, seasonal_1 Loss: 0.0571 | 0.0257
Epoch 135/300, seasonal_1 Loss: 0.0571 | 0.0257
Epoch 136/300, seasonal_1 Loss: 0.0571 | 0.0257
Epoch 137/300, seasonal_1 Loss: 0.0571 | 0.0257
Epoch 138/300, seasonal_1 Loss: 0.0571 | 0.0257
Epoch 139/300, seasonal_1 Loss: 0.0571 | 0.0257
Epoch 140/300, seasonal_1 Loss: 0.0571 | 0.0257
Epoch 141/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 142/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 143/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 144/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 145/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 146/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 147/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 148/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 149/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 150/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 151/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 152/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 153/300, seasonal_1 Loss: 0.0570 | 0.0257
Epoch 154/300, seasonal_1 Loss: 0.0570 | 0.0256
Epoch 155/300, seasonal_1 Loss: 0.0570 | 0.0256
Epoch 156/300, seasonal_1 Loss: 0.0570 | 0.0256
Epoch 157/300, seasonal_1 Loss: 0.0570 | 0.0256
Epoch 158/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 159/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 160/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 161/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 162/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 163/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 164/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 165/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 166/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 167/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 168/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 169/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 170/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 171/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 172/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 173/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 174/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 175/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 176/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 177/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 178/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 179/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 180/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 181/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 182/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 183/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 184/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 185/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 186/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 187/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 188/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 189/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 190/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 191/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 192/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 193/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 194/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 195/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 196/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 197/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 198/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 199/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 200/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 201/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 202/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 203/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 204/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 205/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 206/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 207/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 208/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 209/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 210/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 211/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 212/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 213/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 214/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 215/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 216/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 217/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 218/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 219/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 220/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 221/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 222/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 223/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 224/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 225/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 226/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 227/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 228/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 229/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 230/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 231/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 232/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 233/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 234/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 235/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 236/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 237/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 238/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 239/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 240/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 241/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 242/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 243/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 244/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 245/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 246/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 247/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 248/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 249/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 250/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 251/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 252/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 253/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 254/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 255/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 256/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 257/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 258/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 259/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 260/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 261/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 262/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 263/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 264/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 265/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 266/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 267/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 268/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 269/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 270/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 271/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 272/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 273/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 274/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 275/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 276/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 277/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 278/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 279/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 280/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 281/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 282/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 283/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 284/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 285/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 286/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 287/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 288/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 289/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 290/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 291/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 292/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 293/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 294/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 295/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 296/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 297/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 298/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 299/300, seasonal_1 Loss: 0.0568 | 0.0256
Epoch 300/300, seasonal_1 Loss: 0.0568 | 0.0256
Training seasonal_2 component with params: {'observation_period_num': 10, 'train_rates': 0.9829782009317956, 'learning_rate': 0.0008824583713776, 'batch_size': 104, 'step_size': 11, 'gamma': 0.7542167200770462}
Epoch 1/300, seasonal_2 Loss: 0.3142 | 0.1742
Epoch 2/300, seasonal_2 Loss: 0.1790 | 0.1135
Epoch 3/300, seasonal_2 Loss: 0.1224 | 0.0854
Epoch 4/300, seasonal_2 Loss: 0.1051 | 0.0778
Epoch 5/300, seasonal_2 Loss: 0.1010 | 0.0813
Epoch 6/300, seasonal_2 Loss: 0.1013 | 0.0906
Epoch 7/300, seasonal_2 Loss: 0.1009 | 0.0656
Epoch 8/300, seasonal_2 Loss: 0.0979 | 0.0644
Epoch 9/300, seasonal_2 Loss: 0.0918 | 0.0575
Epoch 10/300, seasonal_2 Loss: 0.0859 | 0.0556
Epoch 11/300, seasonal_2 Loss: 0.0875 | 0.0534
Epoch 12/300, seasonal_2 Loss: 0.0916 | 0.0580
Epoch 13/300, seasonal_2 Loss: 0.0864 | 0.0526
Epoch 14/300, seasonal_2 Loss: 0.0858 | 0.0514
Epoch 15/300, seasonal_2 Loss: 0.0815 | 0.0482
Epoch 16/300, seasonal_2 Loss: 0.0843 | 0.0533
Epoch 17/300, seasonal_2 Loss: 0.0777 | 0.0459
Epoch 18/300, seasonal_2 Loss: 0.0768 | 0.0446
Epoch 19/300, seasonal_2 Loss: 0.0753 | 0.0432
Epoch 20/300, seasonal_2 Loss: 0.0739 | 0.0422
Epoch 21/300, seasonal_2 Loss: 0.0736 | 0.0410
Epoch 22/300, seasonal_2 Loss: 0.0726 | 0.0401
Epoch 23/300, seasonal_2 Loss: 0.0723 | 0.0406
Epoch 24/300, seasonal_2 Loss: 0.0735 | 0.0407
Epoch 25/300, seasonal_2 Loss: 0.0736 | 0.0423
Epoch 26/300, seasonal_2 Loss: 0.0729 | 0.0427
Epoch 27/300, seasonal_2 Loss: 0.0717 | 0.0424
Epoch 28/300, seasonal_2 Loss: 0.0701 | 0.0416
Epoch 29/300, seasonal_2 Loss: 0.0682 | 0.0397
Epoch 30/300, seasonal_2 Loss: 0.0683 | 0.0399
Epoch 31/300, seasonal_2 Loss: 0.0723 | 0.0401
Epoch 32/300, seasonal_2 Loss: 0.0731 | 0.0400
Epoch 33/300, seasonal_2 Loss: 0.0683 | 0.0402
Epoch 34/300, seasonal_2 Loss: 0.0680 | 0.0404
Epoch 35/300, seasonal_2 Loss: 0.0676 | 0.0410
Epoch 36/300, seasonal_2 Loss: 0.0670 | 0.0410
Epoch 37/300, seasonal_2 Loss: 0.0664 | 0.0404
Epoch 38/300, seasonal_2 Loss: 0.0658 | 0.0398
Epoch 39/300, seasonal_2 Loss: 0.0653 | 0.0391
Epoch 40/300, seasonal_2 Loss: 0.0648 | 0.0374
Epoch 41/300, seasonal_2 Loss: 0.0647 | 0.0369
Epoch 42/300, seasonal_2 Loss: 0.0651 | 0.0367
Epoch 43/300, seasonal_2 Loss: 0.0651 | 0.0370
Epoch 44/300, seasonal_2 Loss: 0.0660 | 0.0386
Epoch 45/300, seasonal_2 Loss: 0.0673 | 0.0455
Epoch 46/300, seasonal_2 Loss: 0.0676 | 0.0486
Epoch 47/300, seasonal_2 Loss: 0.0663 | 0.0487
Epoch 48/300, seasonal_2 Loss: 0.0647 | 0.0454
Epoch 49/300, seasonal_2 Loss: 0.0640 | 0.0407
Epoch 50/300, seasonal_2 Loss: 0.0654 | 0.0372
Epoch 51/300, seasonal_2 Loss: 0.0673 | 0.0405
Epoch 52/300, seasonal_2 Loss: 0.0658 | 0.0362
Epoch 53/300, seasonal_2 Loss: 0.0622 | 0.0353
Epoch 54/300, seasonal_2 Loss: 0.0610 | 0.0349
Epoch 55/300, seasonal_2 Loss: 0.0608 | 0.0343
Epoch 56/300, seasonal_2 Loss: 0.0607 | 0.0344
Epoch 57/300, seasonal_2 Loss: 0.0607 | 0.0348
Epoch 58/300, seasonal_2 Loss: 0.0608 | 0.0353
Epoch 59/300, seasonal_2 Loss: 0.0610 | 0.0357
Epoch 60/300, seasonal_2 Loss: 0.0612 | 0.0358
Epoch 61/300, seasonal_2 Loss: 0.0611 | 0.0360
Epoch 62/300, seasonal_2 Loss: 0.0612 | 0.0350
Epoch 63/300, seasonal_2 Loss: 0.0611 | 0.0345
Epoch 64/300, seasonal_2 Loss: 0.0605 | 0.0339
Epoch 65/300, seasonal_2 Loss: 0.0602 | 0.0335
Epoch 66/300, seasonal_2 Loss: 0.0601 | 0.0332
Epoch 67/300, seasonal_2 Loss: 0.0601 | 0.0325
Epoch 68/300, seasonal_2 Loss: 0.0602 | 0.0326
Epoch 69/300, seasonal_2 Loss: 0.0601 | 0.0327
Epoch 70/300, seasonal_2 Loss: 0.0600 | 0.0330
Epoch 71/300, seasonal_2 Loss: 0.0598 | 0.0332
Epoch 72/300, seasonal_2 Loss: 0.0596 | 0.0334
Epoch 73/300, seasonal_2 Loss: 0.0593 | 0.0333
Epoch 74/300, seasonal_2 Loss: 0.0591 | 0.0336
Epoch 75/300, seasonal_2 Loss: 0.0588 | 0.0336
Epoch 76/300, seasonal_2 Loss: 0.0587 | 0.0334
Epoch 77/300, seasonal_2 Loss: 0.0586 | 0.0331
Epoch 78/300, seasonal_2 Loss: 0.0585 | 0.0330
Epoch 79/300, seasonal_2 Loss: 0.0584 | 0.0329
Epoch 80/300, seasonal_2 Loss: 0.0584 | 0.0328
Epoch 81/300, seasonal_2 Loss: 0.0583 | 0.0327
Epoch 82/300, seasonal_2 Loss: 0.0583 | 0.0326
Epoch 83/300, seasonal_2 Loss: 0.0582 | 0.0325
Epoch 84/300, seasonal_2 Loss: 0.0582 | 0.0325
Epoch 85/300, seasonal_2 Loss: 0.0581 | 0.0325
Epoch 86/300, seasonal_2 Loss: 0.0581 | 0.0324
Epoch 87/300, seasonal_2 Loss: 0.0580 | 0.0323
Epoch 88/300, seasonal_2 Loss: 0.0580 | 0.0322
Epoch 89/300, seasonal_2 Loss: 0.0580 | 0.0322
Epoch 90/300, seasonal_2 Loss: 0.0579 | 0.0321
Epoch 91/300, seasonal_2 Loss: 0.0579 | 0.0321
Epoch 92/300, seasonal_2 Loss: 0.0579 | 0.0320
Epoch 93/300, seasonal_2 Loss: 0.0579 | 0.0319
Epoch 94/300, seasonal_2 Loss: 0.0578 | 0.0319
Epoch 95/300, seasonal_2 Loss: 0.0578 | 0.0319
Epoch 96/300, seasonal_2 Loss: 0.0578 | 0.0318
Epoch 97/300, seasonal_2 Loss: 0.0577 | 0.0318
Epoch 98/300, seasonal_2 Loss: 0.0577 | 0.0317
Epoch 99/300, seasonal_2 Loss: 0.0577 | 0.0317
Epoch 100/300, seasonal_2 Loss: 0.0577 | 0.0317
Epoch 101/300, seasonal_2 Loss: 0.0577 | 0.0316
Epoch 102/300, seasonal_2 Loss: 0.0577 | 0.0316
Epoch 103/300, seasonal_2 Loss: 0.0576 | 0.0316
Epoch 104/300, seasonal_2 Loss: 0.0576 | 0.0315
Epoch 105/300, seasonal_2 Loss: 0.0576 | 0.0315
Epoch 106/300, seasonal_2 Loss: 0.0576 | 0.0315
Epoch 107/300, seasonal_2 Loss: 0.0576 | 0.0315
Epoch 108/300, seasonal_2 Loss: 0.0576 | 0.0314
Epoch 109/300, seasonal_2 Loss: 0.0576 | 0.0314
Epoch 110/300, seasonal_2 Loss: 0.0575 | 0.0314
Epoch 111/300, seasonal_2 Loss: 0.0575 | 0.0314
Epoch 112/300, seasonal_2 Loss: 0.0575 | 0.0314
Epoch 113/300, seasonal_2 Loss: 0.0575 | 0.0313
Epoch 114/300, seasonal_2 Loss: 0.0575 | 0.0313
Epoch 115/300, seasonal_2 Loss: 0.0575 | 0.0313
Epoch 116/300, seasonal_2 Loss: 0.0575 | 0.0313
Epoch 117/300, seasonal_2 Loss: 0.0575 | 0.0313
Epoch 118/300, seasonal_2 Loss: 0.0575 | 0.0313
Epoch 119/300, seasonal_2 Loss: 0.0575 | 0.0312
Epoch 120/300, seasonal_2 Loss: 0.0574 | 0.0312
Epoch 121/300, seasonal_2 Loss: 0.0574 | 0.0312
Epoch 122/300, seasonal_2 Loss: 0.0574 | 0.0312
Epoch 123/300, seasonal_2 Loss: 0.0574 | 0.0312
Epoch 124/300, seasonal_2 Loss: 0.0574 | 0.0312
Epoch 125/300, seasonal_2 Loss: 0.0574 | 0.0312
Epoch 126/300, seasonal_2 Loss: 0.0574 | 0.0312
Epoch 127/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 128/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 129/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 130/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 131/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 132/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 133/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 134/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 135/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 136/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 137/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 138/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 139/300, seasonal_2 Loss: 0.0574 | 0.0311
Epoch 140/300, seasonal_2 Loss: 0.0573 | 0.0311
Epoch 141/300, seasonal_2 Loss: 0.0573 | 0.0311
Epoch 142/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 143/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 144/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 145/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 146/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 147/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 148/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 149/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 150/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 151/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 152/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 153/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 154/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 155/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 156/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 157/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 158/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 159/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 160/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 161/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 162/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 163/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 164/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 165/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 166/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 167/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 168/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 169/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 170/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 171/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 172/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 173/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 174/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 175/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 176/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 177/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 178/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 179/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 180/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 181/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 182/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 183/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 184/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 185/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 186/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 187/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 188/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 189/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 190/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 191/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 192/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 193/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 194/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 195/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 196/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 197/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 198/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 199/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 200/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 201/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 202/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 203/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 204/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 205/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 206/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 207/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 208/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 209/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 210/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 211/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 212/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 213/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 214/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 215/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 216/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 217/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 218/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 219/300, seasonal_2 Loss: 0.0573 | 0.0310
Epoch 220/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 221/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 222/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 223/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 224/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 225/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 226/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 227/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 228/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 229/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 230/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 231/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 232/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 233/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 234/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 235/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 236/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 237/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 238/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 239/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 240/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 241/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 242/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 243/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 244/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 245/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 246/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 247/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 248/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 249/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 250/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 251/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 252/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 253/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 254/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 255/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 256/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 257/300, seasonal_2 Loss: 0.0573 | 0.0309
Epoch 258/300, seasonal_2 Loss: 0.0573 | 0.0309
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 17, 'train_rates': 0.8877015417672267, 'learning_rate': 2.004436287423855e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9656620346567689}
Epoch 1/300, seasonal_3 Loss: 0.3278 | 0.2811
Epoch 2/300, seasonal_3 Loss: 0.1740 | 0.1929
Epoch 3/300, seasonal_3 Loss: 0.1491 | 0.1525
Epoch 4/300, seasonal_3 Loss: 0.1338 | 0.1314
Epoch 5/300, seasonal_3 Loss: 0.1239 | 0.1174
Epoch 6/300, seasonal_3 Loss: 0.1176 | 0.1055
Epoch 7/300, seasonal_3 Loss: 0.1135 | 0.0967
Epoch 8/300, seasonal_3 Loss: 0.1104 | 0.0905
Epoch 9/300, seasonal_3 Loss: 0.1078 | 0.0859
Epoch 10/300, seasonal_3 Loss: 0.1056 | 0.0824
Epoch 11/300, seasonal_3 Loss: 0.1036 | 0.0796
Epoch 12/300, seasonal_3 Loss: 0.1019 | 0.0770
Epoch 13/300, seasonal_3 Loss: 0.1004 | 0.0748
Epoch 14/300, seasonal_3 Loss: 0.0990 | 0.0729
Epoch 15/300, seasonal_3 Loss: 0.0977 | 0.0712
Epoch 16/300, seasonal_3 Loss: 0.0966 | 0.0696
Epoch 17/300, seasonal_3 Loss: 0.0955 | 0.0681
Epoch 18/300, seasonal_3 Loss: 0.0945 | 0.0668
Epoch 19/300, seasonal_3 Loss: 0.0936 | 0.0655
Epoch 20/300, seasonal_3 Loss: 0.0926 | 0.0642
Epoch 21/300, seasonal_3 Loss: 0.0917 | 0.0629
Epoch 22/300, seasonal_3 Loss: 0.0908 | 0.0617
Epoch 23/300, seasonal_3 Loss: 0.0900 | 0.0605
Epoch 24/300, seasonal_3 Loss: 0.0891 | 0.0592
Epoch 25/300, seasonal_3 Loss: 0.0882 | 0.0580
Epoch 26/300, seasonal_3 Loss: 0.0873 | 0.0568
Epoch 27/300, seasonal_3 Loss: 0.0864 | 0.0556
Epoch 28/300, seasonal_3 Loss: 0.0855 | 0.0544
Epoch 29/300, seasonal_3 Loss: 0.0846 | 0.0532
Epoch 30/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 31/300, seasonal_3 Loss: 0.0829 | 0.0509
Epoch 32/300, seasonal_3 Loss: 0.0821 | 0.0499
Epoch 33/300, seasonal_3 Loss: 0.0813 | 0.0490
Epoch 34/300, seasonal_3 Loss: 0.0806 | 0.0480
Epoch 35/300, seasonal_3 Loss: 0.0799 | 0.0472
Epoch 36/300, seasonal_3 Loss: 0.0792 | 0.0463
Epoch 37/300, seasonal_3 Loss: 0.0786 | 0.0456
Epoch 38/300, seasonal_3 Loss: 0.0781 | 0.0449
Epoch 39/300, seasonal_3 Loss: 0.0776 | 0.0442
Epoch 40/300, seasonal_3 Loss: 0.0771 | 0.0436
Epoch 41/300, seasonal_3 Loss: 0.0767 | 0.0430
Epoch 42/300, seasonal_3 Loss: 0.0763 | 0.0425
Epoch 43/300, seasonal_3 Loss: 0.0759 | 0.0421
Epoch 44/300, seasonal_3 Loss: 0.0755 | 0.0416
Epoch 45/300, seasonal_3 Loss: 0.0751 | 0.0412
Epoch 46/300, seasonal_3 Loss: 0.0748 | 0.0408
Epoch 47/300, seasonal_3 Loss: 0.0744 | 0.0404
Epoch 48/300, seasonal_3 Loss: 0.0741 | 0.0401
Epoch 49/300, seasonal_3 Loss: 0.0738 | 0.0398
Epoch 50/300, seasonal_3 Loss: 0.0735 | 0.0395
Epoch 51/300, seasonal_3 Loss: 0.0732 | 0.0392
Epoch 52/300, seasonal_3 Loss: 0.0729 | 0.0389
Epoch 53/300, seasonal_3 Loss: 0.0726 | 0.0386
Epoch 54/300, seasonal_3 Loss: 0.0723 | 0.0384
Epoch 55/300, seasonal_3 Loss: 0.0721 | 0.0381
Epoch 56/300, seasonal_3 Loss: 0.0718 | 0.0379
Epoch 57/300, seasonal_3 Loss: 0.0715 | 0.0377
Epoch 58/300, seasonal_3 Loss: 0.0713 | 0.0375
Epoch 59/300, seasonal_3 Loss: 0.0710 | 0.0372
Epoch 60/300, seasonal_3 Loss: 0.0707 | 0.0370
Epoch 61/300, seasonal_3 Loss: 0.0705 | 0.0368
Epoch 62/300, seasonal_3 Loss: 0.0702 | 0.0366
Epoch 63/300, seasonal_3 Loss: 0.0700 | 0.0364
Epoch 64/300, seasonal_3 Loss: 0.0698 | 0.0362
Epoch 65/300, seasonal_3 Loss: 0.0695 | 0.0360
Epoch 66/300, seasonal_3 Loss: 0.0693 | 0.0359
Epoch 67/300, seasonal_3 Loss: 0.0691 | 0.0357
Epoch 68/300, seasonal_3 Loss: 0.0688 | 0.0355
Epoch 69/300, seasonal_3 Loss: 0.0686 | 0.0354
Epoch 70/300, seasonal_3 Loss: 0.0684 | 0.0352
Epoch 71/300, seasonal_3 Loss: 0.0682 | 0.0351
Epoch 72/300, seasonal_3 Loss: 0.0680 | 0.0349
Epoch 73/300, seasonal_3 Loss: 0.0678 | 0.0348
Epoch 74/300, seasonal_3 Loss: 0.0676 | 0.0346
Epoch 75/300, seasonal_3 Loss: 0.0674 | 0.0345
Epoch 76/300, seasonal_3 Loss: 0.0672 | 0.0344
Epoch 77/300, seasonal_3 Loss: 0.0670 | 0.0343
Epoch 78/300, seasonal_3 Loss: 0.0668 | 0.0341
Epoch 79/300, seasonal_3 Loss: 0.0666 | 0.0340
Epoch 80/300, seasonal_3 Loss: 0.0664 | 0.0339
Epoch 81/300, seasonal_3 Loss: 0.0662 | 0.0338
Epoch 82/300, seasonal_3 Loss: 0.0660 | 0.0337
Epoch 83/300, seasonal_3 Loss: 0.0658 | 0.0336
Epoch 84/300, seasonal_3 Loss: 0.0657 | 0.0335
Epoch 85/300, seasonal_3 Loss: 0.0655 | 0.0334
Epoch 86/300, seasonal_3 Loss: 0.0653 | 0.0334
Epoch 87/300, seasonal_3 Loss: 0.0652 | 0.0333
Epoch 88/300, seasonal_3 Loss: 0.0650 | 0.0332
Epoch 89/300, seasonal_3 Loss: 0.0649 | 0.0332
Epoch 90/300, seasonal_3 Loss: 0.0647 | 0.0331
Epoch 91/300, seasonal_3 Loss: 0.0646 | 0.0330
Epoch 92/300, seasonal_3 Loss: 0.0644 | 0.0330
Epoch 93/300, seasonal_3 Loss: 0.0643 | 0.0329
Epoch 94/300, seasonal_3 Loss: 0.0641 | 0.0329
Epoch 95/300, seasonal_3 Loss: 0.0640 | 0.0328
Epoch 96/300, seasonal_3 Loss: 0.0638 | 0.0328
Epoch 97/300, seasonal_3 Loss: 0.0637 | 0.0328
Epoch 98/300, seasonal_3 Loss: 0.0636 | 0.0327
Epoch 99/300, seasonal_3 Loss: 0.0634 | 0.0327
Epoch 100/300, seasonal_3 Loss: 0.0633 | 0.0326
Epoch 101/300, seasonal_3 Loss: 0.0632 | 0.0326
Epoch 102/300, seasonal_3 Loss: 0.0630 | 0.0326
Epoch 103/300, seasonal_3 Loss: 0.0629 | 0.0325
Epoch 104/300, seasonal_3 Loss: 0.0628 | 0.0325
Epoch 105/300, seasonal_3 Loss: 0.0627 | 0.0324
Epoch 106/300, seasonal_3 Loss: 0.0625 | 0.0324
Epoch 107/300, seasonal_3 Loss: 0.0624 | 0.0324
Epoch 108/300, seasonal_3 Loss: 0.0623 | 0.0324
Epoch 109/300, seasonal_3 Loss: 0.0622 | 0.0323
Epoch 110/300, seasonal_3 Loss: 0.0621 | 0.0323
Epoch 111/300, seasonal_3 Loss: 0.0619 | 0.0323
Epoch 112/300, seasonal_3 Loss: 0.0618 | 0.0322
Epoch 113/300, seasonal_3 Loss: 0.0617 | 0.0322
Epoch 114/300, seasonal_3 Loss: 0.0616 | 0.0322
Epoch 115/300, seasonal_3 Loss: 0.0615 | 0.0322
Epoch 116/300, seasonal_3 Loss: 0.0614 | 0.0322
Epoch 117/300, seasonal_3 Loss: 0.0612 | 0.0321
Epoch 118/300, seasonal_3 Loss: 0.0611 | 0.0321
Epoch 119/300, seasonal_3 Loss: 0.0610 | 0.0321
Epoch 120/300, seasonal_3 Loss: 0.0609 | 0.0321
Epoch 121/300, seasonal_3 Loss: 0.0608 | 0.0321
Epoch 122/300, seasonal_3 Loss: 0.0607 | 0.0321
Epoch 123/300, seasonal_3 Loss: 0.0606 | 0.0320
Epoch 124/300, seasonal_3 Loss: 0.0605 | 0.0320
Epoch 125/300, seasonal_3 Loss: 0.0604 | 0.0320
Epoch 126/300, seasonal_3 Loss: 0.0603 | 0.0320
Epoch 127/300, seasonal_3 Loss: 0.0602 | 0.0320
Epoch 128/300, seasonal_3 Loss: 0.0601 | 0.0320
Epoch 129/300, seasonal_3 Loss: 0.0600 | 0.0320
Epoch 130/300, seasonal_3 Loss: 0.0599 | 0.0320
Epoch 131/300, seasonal_3 Loss: 0.0597 | 0.0320
Epoch 132/300, seasonal_3 Loss: 0.0596 | 0.0320
Epoch 133/300, seasonal_3 Loss: 0.0595 | 0.0320
Epoch 134/300, seasonal_3 Loss: 0.0594 | 0.0320
Epoch 135/300, seasonal_3 Loss: 0.0594 | 0.0320
Epoch 136/300, seasonal_3 Loss: 0.0592 | 0.0320
Epoch 137/300, seasonal_3 Loss: 0.0592 | 0.0320
Epoch 138/300, seasonal_3 Loss: 0.0591 | 0.0320
Epoch 139/300, seasonal_3 Loss: 0.0590 | 0.0320
Epoch 140/300, seasonal_3 Loss: 0.0589 | 0.0320
Epoch 141/300, seasonal_3 Loss: 0.0588 | 0.0320
Epoch 142/300, seasonal_3 Loss: 0.0587 | 0.0320
Epoch 143/300, seasonal_3 Loss: 0.0586 | 0.0321
Epoch 144/300, seasonal_3 Loss: 0.0585 | 0.0321
Epoch 145/300, seasonal_3 Loss: 0.0584 | 0.0321
Epoch 146/300, seasonal_3 Loss: 0.0583 | 0.0321
Epoch 147/300, seasonal_3 Loss: 0.0582 | 0.0321
Epoch 148/300, seasonal_3 Loss: 0.0581 | 0.0321
Epoch 149/300, seasonal_3 Loss: 0.0581 | 0.0321
Epoch 150/300, seasonal_3 Loss: 0.0580 | 0.0322
Epoch 151/300, seasonal_3 Loss: 0.0579 | 0.0322
Epoch 152/300, seasonal_3 Loss: 0.0578 | 0.0322
Epoch 153/300, seasonal_3 Loss: 0.0577 | 0.0322
Epoch 154/300, seasonal_3 Loss: 0.0576 | 0.0322
Epoch 155/300, seasonal_3 Loss: 0.0575 | 0.0323
Epoch 156/300, seasonal_3 Loss: 0.0575 | 0.0323
Epoch 157/300, seasonal_3 Loss: 0.0574 | 0.0323
Epoch 158/300, seasonal_3 Loss: 0.0573 | 0.0323
Epoch 159/300, seasonal_3 Loss: 0.0572 | 0.0324
Epoch 160/300, seasonal_3 Loss: 0.0571 | 0.0324
Epoch 161/300, seasonal_3 Loss: 0.0571 | 0.0324
Epoch 162/300, seasonal_3 Loss: 0.0570 | 0.0324
Epoch 163/300, seasonal_3 Loss: 0.0569 | 0.0324
Epoch 164/300, seasonal_3 Loss: 0.0568 | 0.0325
Epoch 165/300, seasonal_3 Loss: 0.0567 | 0.0325
Epoch 166/300, seasonal_3 Loss: 0.0567 | 0.0325
Epoch 167/300, seasonal_3 Loss: 0.0566 | 0.0325
Epoch 168/300, seasonal_3 Loss: 0.0565 | 0.0325
Epoch 169/300, seasonal_3 Loss: 0.0564 | 0.0326
Epoch 170/300, seasonal_3 Loss: 0.0564 | 0.0326
Epoch 171/300, seasonal_3 Loss: 0.0563 | 0.0326
Epoch 172/300, seasonal_3 Loss: 0.0562 | 0.0326
Epoch 173/300, seasonal_3 Loss: 0.0561 | 0.0327
Epoch 174/300, seasonal_3 Loss: 0.0561 | 0.0327
Epoch 175/300, seasonal_3 Loss: 0.0560 | 0.0327
Epoch 176/300, seasonal_3 Loss: 0.0559 | 0.0327
Epoch 177/300, seasonal_3 Loss: 0.0558 | 0.0327
Epoch 178/300, seasonal_3 Loss: 0.0558 | 0.0328
Epoch 179/300, seasonal_3 Loss: 0.0557 | 0.0328
Epoch 180/300, seasonal_3 Loss: 0.0556 | 0.0328
Epoch 181/300, seasonal_3 Loss: 0.0555 | 0.0328
Epoch 182/300, seasonal_3 Loss: 0.0555 | 0.0328
Epoch 183/300, seasonal_3 Loss: 0.0554 | 0.0328
Epoch 184/300, seasonal_3 Loss: 0.0553 | 0.0329
Epoch 185/300, seasonal_3 Loss: 0.0552 | 0.0329
Epoch 186/300, seasonal_3 Loss: 0.0552 | 0.0329
Epoch 187/300, seasonal_3 Loss: 0.0551 | 0.0329
Epoch 188/300, seasonal_3 Loss: 0.0550 | 0.0329
Epoch 189/300, seasonal_3 Loss: 0.0549 | 0.0330
Epoch 190/300, seasonal_3 Loss: 0.0549 | 0.0330
Epoch 191/300, seasonal_3 Loss: 0.0548 | 0.0329
Epoch 192/300, seasonal_3 Loss: 0.0547 | 0.0330
Epoch 193/300, seasonal_3 Loss: 0.0546 | 0.0330
Epoch 194/300, seasonal_3 Loss: 0.0546 | 0.0330
Epoch 195/300, seasonal_3 Loss: 0.0545 | 0.0331
Epoch 196/300, seasonal_3 Loss: 0.0544 | 0.0330
Epoch 197/300, seasonal_3 Loss: 0.0543 | 0.0331
Epoch 198/300, seasonal_3 Loss: 0.0542 | 0.0331
Epoch 199/300, seasonal_3 Loss: 0.0542 | 0.0331
Epoch 200/300, seasonal_3 Loss: 0.0541 | 0.0331
Epoch 201/300, seasonal_3 Loss: 0.0540 | 0.0331
Epoch 202/300, seasonal_3 Loss: 0.0539 | 0.0331
Epoch 203/300, seasonal_3 Loss: 0.0538 | 0.0332
Epoch 204/300, seasonal_3 Loss: 0.0538 | 0.0332
Epoch 205/300, seasonal_3 Loss: 0.0537 | 0.0332
Epoch 206/300, seasonal_3 Loss: 0.0536 | 0.0332
Epoch 207/300, seasonal_3 Loss: 0.0535 | 0.0332
Epoch 208/300, seasonal_3 Loss: 0.0534 | 0.0333
Epoch 209/300, seasonal_3 Loss: 0.0534 | 0.0333
Epoch 210/300, seasonal_3 Loss: 0.0533 | 0.0333
Epoch 211/300, seasonal_3 Loss: 0.0532 | 0.0333
Epoch 212/300, seasonal_3 Loss: 0.0531 | 0.0333
Epoch 213/300, seasonal_3 Loss: 0.0530 | 0.0334
Epoch 214/300, seasonal_3 Loss: 0.0529 | 0.0334
Epoch 215/300, seasonal_3 Loss: 0.0528 | 0.0334
Epoch 216/300, seasonal_3 Loss: 0.0527 | 0.0334
Epoch 217/300, seasonal_3 Loss: 0.0527 | 0.0334
Epoch 218/300, seasonal_3 Loss: 0.0526 | 0.0335
Epoch 219/300, seasonal_3 Loss: 0.0525 | 0.0335
Epoch 220/300, seasonal_3 Loss: 0.0524 | 0.0335
Epoch 221/300, seasonal_3 Loss: 0.0523 | 0.0335
Epoch 222/300, seasonal_3 Loss: 0.0522 | 0.0336
Epoch 223/300, seasonal_3 Loss: 0.0521 | 0.0336
Epoch 224/300, seasonal_3 Loss: 0.0520 | 0.0336
Epoch 225/300, seasonal_3 Loss: 0.0519 | 0.0336
Epoch 226/300, seasonal_3 Loss: 0.0518 | 0.0336
Epoch 227/300, seasonal_3 Loss: 0.0517 | 0.0337
Epoch 228/300, seasonal_3 Loss: 0.0516 | 0.0337
Epoch 229/300, seasonal_3 Loss: 0.0515 | 0.0337
Epoch 230/300, seasonal_3 Loss: 0.0514 | 0.0337
Epoch 231/300, seasonal_3 Loss: 0.0513 | 0.0337
Epoch 232/300, seasonal_3 Loss: 0.0512 | 0.0338
Epoch 233/300, seasonal_3 Loss: 0.0511 | 0.0338
Epoch 234/300, seasonal_3 Loss: 0.0510 | 0.0338
Epoch 235/300, seasonal_3 Loss: 0.0509 | 0.0338
Epoch 236/300, seasonal_3 Loss: 0.0508 | 0.0338
Epoch 237/300, seasonal_3 Loss: 0.0507 | 0.0338
Epoch 238/300, seasonal_3 Loss: 0.0506 | 0.0338
Epoch 239/300, seasonal_3 Loss: 0.0505 | 0.0339
Epoch 240/300, seasonal_3 Loss: 0.0504 | 0.0339
Epoch 241/300, seasonal_3 Loss: 0.0503 | 0.0338
Epoch 242/300, seasonal_3 Loss: 0.0501 | 0.0338
Epoch 243/300, seasonal_3 Loss: 0.0500 | 0.0339
Epoch 244/300, seasonal_3 Loss: 0.0499 | 0.0339
Epoch 245/300, seasonal_3 Loss: 0.0498 | 0.0339
Epoch 246/300, seasonal_3 Loss: 0.0497 | 0.0338
Epoch 247/300, seasonal_3 Loss: 0.0496 | 0.0338
Epoch 248/300, seasonal_3 Loss: 0.0495 | 0.0338
Epoch 249/300, seasonal_3 Loss: 0.0494 | 0.0338
Epoch 250/300, seasonal_3 Loss: 0.0493 | 0.0338
Epoch 251/300, seasonal_3 Loss: 0.0492 | 0.0338
Epoch 252/300, seasonal_3 Loss: 0.0491 | 0.0338
Epoch 253/300, seasonal_3 Loss: 0.0490 | 0.0338
Epoch 254/300, seasonal_3 Loss: 0.0489 | 0.0338
Epoch 255/300, seasonal_3 Loss: 0.0488 | 0.0338
Epoch 256/300, seasonal_3 Loss: 0.0487 | 0.0337
Epoch 257/300, seasonal_3 Loss: 0.0486 | 0.0337
Epoch 258/300, seasonal_3 Loss: 0.0485 | 0.0337
Epoch 259/300, seasonal_3 Loss: 0.0484 | 0.0337
Epoch 260/300, seasonal_3 Loss: 0.0483 | 0.0337
Epoch 261/300, seasonal_3 Loss: 0.0482 | 0.0336
Epoch 262/300, seasonal_3 Loss: 0.0481 | 0.0336
Epoch 263/300, seasonal_3 Loss: 0.0480 | 0.0336
Epoch 264/300, seasonal_3 Loss: 0.0480 | 0.0335
Epoch 265/300, seasonal_3 Loss: 0.0479 | 0.0335
Epoch 266/300, seasonal_3 Loss: 0.0478 | 0.0335
Epoch 267/300, seasonal_3 Loss: 0.0477 | 0.0334
Epoch 268/300, seasonal_3 Loss: 0.0476 | 0.0334
Epoch 269/300, seasonal_3 Loss: 0.0475 | 0.0334
Epoch 270/300, seasonal_3 Loss: 0.0475 | 0.0334
Epoch 271/300, seasonal_3 Loss: 0.0474 | 0.0333
Epoch 272/300, seasonal_3 Loss: 0.0473 | 0.0333
Epoch 273/300, seasonal_3 Loss: 0.0472 | 0.0332
Epoch 274/300, seasonal_3 Loss: 0.0471 | 0.0332
Epoch 275/300, seasonal_3 Loss: 0.0471 | 0.0332
Epoch 276/300, seasonal_3 Loss: 0.0470 | 0.0331
Epoch 277/300, seasonal_3 Loss: 0.0469 | 0.0331
Epoch 278/300, seasonal_3 Loss: 0.0469 | 0.0330
Epoch 279/300, seasonal_3 Loss: 0.0468 | 0.0330
Epoch 280/300, seasonal_3 Loss: 0.0467 | 0.0330
Epoch 281/300, seasonal_3 Loss: 0.0466 | 0.0329
Epoch 282/300, seasonal_3 Loss: 0.0466 | 0.0329
Epoch 283/300, seasonal_3 Loss: 0.0465 | 0.0329
Epoch 284/300, seasonal_3 Loss: 0.0465 | 0.0328
Epoch 285/300, seasonal_3 Loss: 0.0464 | 0.0328
Epoch 286/300, seasonal_3 Loss: 0.0463 | 0.0327
Epoch 287/300, seasonal_3 Loss: 0.0463 | 0.0327
Epoch 288/300, seasonal_3 Loss: 0.0462 | 0.0327
Epoch 289/300, seasonal_3 Loss: 0.0461 | 0.0326
Epoch 290/300, seasonal_3 Loss: 0.0461 | 0.0326
Epoch 291/300, seasonal_3 Loss: 0.0460 | 0.0326
Epoch 292/300, seasonal_3 Loss: 0.0460 | 0.0325
Epoch 293/300, seasonal_3 Loss: 0.0459 | 0.0325
Epoch 294/300, seasonal_3 Loss: 0.0459 | 0.0325
Epoch 295/300, seasonal_3 Loss: 0.0458 | 0.0324
Epoch 296/300, seasonal_3 Loss: 0.0458 | 0.0324
Epoch 297/300, seasonal_3 Loss: 0.0457 | 0.0324
Epoch 298/300, seasonal_3 Loss: 0.0457 | 0.0323
Epoch 299/300, seasonal_3 Loss: 0.0456 | 0.0323
Epoch 300/300, seasonal_3 Loss: 0.0456 | 0.0323
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.9533132134579991, 'learning_rate': 0.0004139196016946088, 'batch_size': 76, 'step_size': 3, 'gamma': 0.9407542909309634}
Epoch 1/300, resid Loss: 0.2103 | 0.1773
Epoch 2/300, resid Loss: 0.1254 | 0.1089
Epoch 3/300, resid Loss: 0.1167 | 0.1231
Epoch 4/300, resid Loss: 0.1078 | 0.1057
Epoch 5/300, resid Loss: 0.1040 | 0.1078
Epoch 6/300, resid Loss: 0.1040 | 0.0983
Epoch 7/300, resid Loss: 0.1090 | 0.0888
Epoch 8/300, resid Loss: 0.1171 | 0.0943
Epoch 9/300, resid Loss: 0.0955 | 0.0666
Epoch 10/300, resid Loss: 0.0871 | 0.0592
Epoch 11/300, resid Loss: 0.0837 | 0.0547
Epoch 12/300, resid Loss: 0.0812 | 0.0511
Epoch 13/300, resid Loss: 0.0791 | 0.0479
Epoch 14/300, resid Loss: 0.0780 | 0.0461
Epoch 15/300, resid Loss: 0.0778 | 0.0454
Epoch 16/300, resid Loss: 0.0783 | 0.0464
Epoch 17/300, resid Loss: 0.0791 | 0.0470
Epoch 18/300, resid Loss: 0.0792 | 0.0492
Epoch 19/300, resid Loss: 0.0787 | 0.0505
Epoch 20/300, resid Loss: 0.0765 | 0.0459
Epoch 21/300, resid Loss: 0.0743 | 0.0426
Epoch 22/300, resid Loss: 0.0737 | 0.0426
Epoch 23/300, resid Loss: 0.0770 | 0.0463
Epoch 24/300, resid Loss: 0.0786 | 0.0429
Epoch 25/300, resid Loss: 0.0746 | 0.0425
Epoch 26/300, resid Loss: 0.0734 | 0.0411
Epoch 27/300, resid Loss: 0.0736 | 0.0470
Epoch 28/300, resid Loss: 0.0738 | 0.0471
Epoch 29/300, resid Loss: 0.0711 | 0.0400
Epoch 30/300, resid Loss: 0.0692 | 0.0398
Epoch 31/300, resid Loss: 0.0692 | 0.0410
Epoch 32/300, resid Loss: 0.0689 | 0.0400
Epoch 33/300, resid Loss: 0.0687 | 0.0396
Epoch 34/300, resid Loss: 0.0688 | 0.0401
Epoch 35/300, resid Loss: 0.0692 | 0.0398
Epoch 36/300, resid Loss: 0.0694 | 0.0383
Epoch 37/300, resid Loss: 0.0692 | 0.0381
Epoch 38/300, resid Loss: 0.0686 | 0.0380
Epoch 39/300, resid Loss: 0.0679 | 0.0374
Epoch 40/300, resid Loss: 0.0675 | 0.0368
Epoch 41/300, resid Loss: 0.0677 | 0.0364
Epoch 42/300, resid Loss: 0.0681 | 0.0363
Epoch 43/300, resid Loss: 0.0688 | 0.0369
Epoch 44/300, resid Loss: 0.0692 | 0.0372
Epoch 45/300, resid Loss: 0.0689 | 0.0372
Epoch 46/300, resid Loss: 0.0685 | 0.0374
Epoch 47/300, resid Loss: 0.0685 | 0.0376
Epoch 48/300, resid Loss: 0.0684 | 0.0375
Epoch 49/300, resid Loss: 0.0685 | 0.0377
Epoch 50/300, resid Loss: 0.0688 | 0.0380
Epoch 51/300, resid Loss: 0.0695 | 0.0394
Epoch 52/300, resid Loss: 0.0701 | 0.0416
Epoch 53/300, resid Loss: 0.0698 | 0.0414
Epoch 54/300, resid Loss: 0.0687 | 0.0398
Epoch 55/300, resid Loss: 0.0679 | 0.0381
Epoch 56/300, resid Loss: 0.0676 | 0.0373
Epoch 57/300, resid Loss: 0.0678 | 0.0370
Epoch 58/300, resid Loss: 0.0680 | 0.0370
Epoch 59/300, resid Loss: 0.0679 | 0.0368
Epoch 60/300, resid Loss: 0.0675 | 0.0366
Epoch 61/300, resid Loss: 0.0669 | 0.0364
Epoch 62/300, resid Loss: 0.0664 | 0.0361
Epoch 63/300, resid Loss: 0.0661 | 0.0362
Epoch 64/300, resid Loss: 0.0660 | 0.0363
Epoch 65/300, resid Loss: 0.0658 | 0.0362
Epoch 66/300, resid Loss: 0.0656 | 0.0363
Epoch 67/300, resid Loss: 0.0653 | 0.0361
Epoch 68/300, resid Loss: 0.0648 | 0.0357
Epoch 69/300, resid Loss: 0.0645 | 0.0353
Epoch 70/300, resid Loss: 0.0646 | 0.0350
Epoch 71/300, resid Loss: 0.0649 | 0.0350
Epoch 72/300, resid Loss: 0.0657 | 0.0351
Epoch 73/300, resid Loss: 0.0667 | 0.0354
Epoch 74/300, resid Loss: 0.0677 | 0.0357
Epoch 75/300, resid Loss: 0.0686 | 0.0364
Epoch 76/300, resid Loss: 0.0690 | 0.0368
Epoch 77/300, resid Loss: 0.0676 | 0.0362
Epoch 78/300, resid Loss: 0.0657 | 0.0356
Epoch 79/300, resid Loss: 0.0644 | 0.0352
Epoch 80/300, resid Loss: 0.0637 | 0.0350
Epoch 81/300, resid Loss: 0.0634 | 0.0351
Epoch 82/300, resid Loss: 0.0632 | 0.0350
Epoch 83/300, resid Loss: 0.0631 | 0.0350
Epoch 84/300, resid Loss: 0.0629 | 0.0349
Epoch 85/300, resid Loss: 0.0628 | 0.0347
Epoch 86/300, resid Loss: 0.0627 | 0.0346
Epoch 87/300, resid Loss: 0.0626 | 0.0345
Epoch 88/300, resid Loss: 0.0625 | 0.0344
Epoch 89/300, resid Loss: 0.0625 | 0.0343
Epoch 90/300, resid Loss: 0.0624 | 0.0343
Epoch 91/300, resid Loss: 0.0624 | 0.0342
Epoch 92/300, resid Loss: 0.0623 | 0.0342
Epoch 93/300, resid Loss: 0.0623 | 0.0341
Epoch 94/300, resid Loss: 0.0622 | 0.0341
Epoch 95/300, resid Loss: 0.0622 | 0.0340
Epoch 96/300, resid Loss: 0.0622 | 0.0340
Epoch 97/300, resid Loss: 0.0621 | 0.0339
Epoch 98/300, resid Loss: 0.0621 | 0.0339
Epoch 99/300, resid Loss: 0.0621 | 0.0339
Epoch 100/300, resid Loss: 0.0621 | 0.0338
Epoch 101/300, resid Loss: 0.0620 | 0.0338
Epoch 102/300, resid Loss: 0.0620 | 0.0338
Epoch 103/300, resid Loss: 0.0620 | 0.0337
Epoch 104/300, resid Loss: 0.0619 | 0.0337
Epoch 105/300, resid Loss: 0.0619 | 0.0337
Epoch 106/300, resid Loss: 0.0619 | 0.0336
Epoch 107/300, resid Loss: 0.0619 | 0.0336
Epoch 108/300, resid Loss: 0.0618 | 0.0336
Epoch 109/300, resid Loss: 0.0618 | 0.0336
Epoch 110/300, resid Loss: 0.0618 | 0.0335
Epoch 111/300, resid Loss: 0.0618 | 0.0335
Epoch 112/300, resid Loss: 0.0617 | 0.0335
Epoch 113/300, resid Loss: 0.0617 | 0.0335
Epoch 114/300, resid Loss: 0.0617 | 0.0335
Epoch 115/300, resid Loss: 0.0617 | 0.0334
Epoch 116/300, resid Loss: 0.0617 | 0.0334
Epoch 117/300, resid Loss: 0.0616 | 0.0334
Epoch 118/300, resid Loss: 0.0616 | 0.0334
Epoch 119/300, resid Loss: 0.0616 | 0.0334
Epoch 120/300, resid Loss: 0.0616 | 0.0333
Epoch 121/300, resid Loss: 0.0616 | 0.0333
Epoch 122/300, resid Loss: 0.0615 | 0.0333
Epoch 123/300, resid Loss: 0.0615 | 0.0333
Epoch 124/300, resid Loss: 0.0615 | 0.0333
Epoch 125/300, resid Loss: 0.0615 | 0.0333
Epoch 126/300, resid Loss: 0.0615 | 0.0332
Epoch 127/300, resid Loss: 0.0615 | 0.0332
Epoch 128/300, resid Loss: 0.0615 | 0.0332
Epoch 129/300, resid Loss: 0.0614 | 0.0332
Epoch 130/300, resid Loss: 0.0614 | 0.0332
Epoch 131/300, resid Loss: 0.0614 | 0.0332
Epoch 132/300, resid Loss: 0.0614 | 0.0332
Epoch 133/300, resid Loss: 0.0614 | 0.0332
Epoch 134/300, resid Loss: 0.0614 | 0.0331
Epoch 135/300, resid Loss: 0.0614 | 0.0331
Epoch 136/300, resid Loss: 0.0614 | 0.0331
Epoch 137/300, resid Loss: 0.0614 | 0.0331
Epoch 138/300, resid Loss: 0.0613 | 0.0331
Epoch 139/300, resid Loss: 0.0613 | 0.0331
Epoch 140/300, resid Loss: 0.0613 | 0.0331
Epoch 141/300, resid Loss: 0.0613 | 0.0331
Epoch 142/300, resid Loss: 0.0613 | 0.0331
Epoch 143/300, resid Loss: 0.0613 | 0.0331
Epoch 144/300, resid Loss: 0.0613 | 0.0331
Epoch 145/300, resid Loss: 0.0613 | 0.0330
Epoch 146/300, resid Loss: 0.0613 | 0.0330
Epoch 147/300, resid Loss: 0.0613 | 0.0330
Epoch 148/300, resid Loss: 0.0613 | 0.0330
Epoch 149/300, resid Loss: 0.0613 | 0.0330
Epoch 150/300, resid Loss: 0.0612 | 0.0330
Epoch 151/300, resid Loss: 0.0612 | 0.0330
Epoch 152/300, resid Loss: 0.0612 | 0.0330
Epoch 153/300, resid Loss: 0.0612 | 0.0330
Epoch 154/300, resid Loss: 0.0612 | 0.0330
Epoch 155/300, resid Loss: 0.0612 | 0.0330
Epoch 156/300, resid Loss: 0.0612 | 0.0330
Epoch 157/300, resid Loss: 0.0612 | 0.0330
Epoch 158/300, resid Loss: 0.0612 | 0.0330
Epoch 159/300, resid Loss: 0.0612 | 0.0330
Epoch 160/300, resid Loss: 0.0612 | 0.0330
Epoch 161/300, resid Loss: 0.0612 | 0.0330
Epoch 162/300, resid Loss: 0.0612 | 0.0330
Epoch 163/300, resid Loss: 0.0612 | 0.0329
Epoch 164/300, resid Loss: 0.0612 | 0.0329
Epoch 165/300, resid Loss: 0.0612 | 0.0329
Epoch 166/300, resid Loss: 0.0612 | 0.0329
Epoch 167/300, resid Loss: 0.0612 | 0.0329
Epoch 168/300, resid Loss: 0.0612 | 0.0329
Epoch 169/300, resid Loss: 0.0612 | 0.0329
Epoch 170/300, resid Loss: 0.0612 | 0.0329
Epoch 171/300, resid Loss: 0.0612 | 0.0329
Epoch 172/300, resid Loss: 0.0611 | 0.0329
Epoch 173/300, resid Loss: 0.0611 | 0.0329
Epoch 174/300, resid Loss: 0.0611 | 0.0329
Epoch 175/300, resid Loss: 0.0611 | 0.0329
Epoch 176/300, resid Loss: 0.0611 | 0.0329
Epoch 177/300, resid Loss: 0.0611 | 0.0329
Epoch 178/300, resid Loss: 0.0611 | 0.0329
Epoch 179/300, resid Loss: 0.0611 | 0.0329
Epoch 180/300, resid Loss: 0.0611 | 0.0329
Epoch 181/300, resid Loss: 0.0611 | 0.0329
Epoch 182/300, resid Loss: 0.0611 | 0.0329
Epoch 183/300, resid Loss: 0.0611 | 0.0329
Epoch 184/300, resid Loss: 0.0611 | 0.0329
Epoch 185/300, resid Loss: 0.0611 | 0.0329
Epoch 186/300, resid Loss: 0.0611 | 0.0329
Epoch 187/300, resid Loss: 0.0611 | 0.0329
Epoch 188/300, resid Loss: 0.0611 | 0.0329
Epoch 189/300, resid Loss: 0.0611 | 0.0329
Epoch 190/300, resid Loss: 0.0611 | 0.0329
Epoch 191/300, resid Loss: 0.0611 | 0.0329
Epoch 192/300, resid Loss: 0.0611 | 0.0329
Epoch 193/300, resid Loss: 0.0611 | 0.0329
Epoch 194/300, resid Loss: 0.0611 | 0.0329
Epoch 195/300, resid Loss: 0.0611 | 0.0329
Epoch 196/300, resid Loss: 0.0611 | 0.0329
Epoch 197/300, resid Loss: 0.0611 | 0.0329
Epoch 198/300, resid Loss: 0.0611 | 0.0329
Epoch 199/300, resid Loss: 0.0611 | 0.0329
Epoch 200/300, resid Loss: 0.0611 | 0.0329
Epoch 201/300, resid Loss: 0.0611 | 0.0329
Epoch 202/300, resid Loss: 0.0611 | 0.0329
Epoch 203/300, resid Loss: 0.0611 | 0.0329
Epoch 204/300, resid Loss: 0.0611 | 0.0329
Epoch 205/300, resid Loss: 0.0611 | 0.0329
Epoch 206/300, resid Loss: 0.0611 | 0.0329
Epoch 207/300, resid Loss: 0.0611 | 0.0329
Epoch 208/300, resid Loss: 0.0611 | 0.0329
Epoch 209/300, resid Loss: 0.0611 | 0.0329
Epoch 210/300, resid Loss: 0.0611 | 0.0329
Epoch 211/300, resid Loss: 0.0611 | 0.0329
Epoch 212/300, resid Loss: 0.0611 | 0.0329
Epoch 213/300, resid Loss: 0.0611 | 0.0329
Epoch 214/300, resid Loss: 0.0611 | 0.0329
Epoch 215/300, resid Loss: 0.0611 | 0.0329
Epoch 216/300, resid Loss: 0.0611 | 0.0329
Epoch 217/300, resid Loss: 0.0611 | 0.0329
Epoch 218/300, resid Loss: 0.0611 | 0.0329
Epoch 219/300, resid Loss: 0.0611 | 0.0329
Epoch 220/300, resid Loss: 0.0611 | 0.0329
Epoch 221/300, resid Loss: 0.0611 | 0.0329
Epoch 222/300, resid Loss: 0.0611 | 0.0329
Epoch 223/300, resid Loss: 0.0611 | 0.0329
Epoch 224/300, resid Loss: 0.0611 | 0.0329
Epoch 225/300, resid Loss: 0.0611 | 0.0329
Epoch 226/300, resid Loss: 0.0611 | 0.0329
Epoch 227/300, resid Loss: 0.0611 | 0.0329
Epoch 228/300, resid Loss: 0.0611 | 0.0329
Epoch 229/300, resid Loss: 0.0611 | 0.0329
Epoch 230/300, resid Loss: 0.0611 | 0.0329
Epoch 231/300, resid Loss: 0.0611 | 0.0329
Epoch 232/300, resid Loss: 0.0611 | 0.0329
Epoch 233/300, resid Loss: 0.0611 | 0.0329
Epoch 234/300, resid Loss: 0.0611 | 0.0329
Epoch 235/300, resid Loss: 0.0611 | 0.0329
Epoch 236/300, resid Loss: 0.0611 | 0.0329
Epoch 237/300, resid Loss: 0.0611 | 0.0329
Epoch 238/300, resid Loss: 0.0611 | 0.0329
Epoch 239/300, resid Loss: 0.0611 | 0.0328
Epoch 240/300, resid Loss: 0.0611 | 0.0328
Epoch 241/300, resid Loss: 0.0611 | 0.0328
Epoch 242/300, resid Loss: 0.0611 | 0.0328
Epoch 243/300, resid Loss: 0.0611 | 0.0328
Epoch 244/300, resid Loss: 0.0611 | 0.0328
Epoch 245/300, resid Loss: 0.0611 | 0.0328
Epoch 246/300, resid Loss: 0.0611 | 0.0328
Epoch 247/300, resid Loss: 0.0611 | 0.0328
Epoch 248/300, resid Loss: 0.0611 | 0.0328
Epoch 249/300, resid Loss: 0.0611 | 0.0328
Epoch 250/300, resid Loss: 0.0611 | 0.0328
Epoch 251/300, resid Loss: 0.0611 | 0.0328
Epoch 252/300, resid Loss: 0.0611 | 0.0328
Epoch 253/300, resid Loss: 0.0611 | 0.0328
Epoch 254/300, resid Loss: 0.0611 | 0.0328
Epoch 255/300, resid Loss: 0.0611 | 0.0328
Epoch 256/300, resid Loss: 0.0611 | 0.0328
Epoch 257/300, resid Loss: 0.0611 | 0.0328
Epoch 258/300, resid Loss: 0.0611 | 0.0328
Epoch 259/300, resid Loss: 0.0611 | 0.0328
Epoch 260/300, resid Loss: 0.0611 | 0.0328
Epoch 261/300, resid Loss: 0.0611 | 0.0328
Epoch 262/300, resid Loss: 0.0611 | 0.0328
Epoch 263/300, resid Loss: 0.0611 | 0.0328
Epoch 264/300, resid Loss: 0.0611 | 0.0328
Epoch 265/300, resid Loss: 0.0611 | 0.0328
Epoch 266/300, resid Loss: 0.0611 | 0.0328
Epoch 267/300, resid Loss: 0.0611 | 0.0328
Epoch 268/300, resid Loss: 0.0611 | 0.0328
Epoch 269/300, resid Loss: 0.0611 | 0.0328
Epoch 270/300, resid Loss: 0.0611 | 0.0328
Epoch 271/300, resid Loss: 0.0611 | 0.0328
Epoch 272/300, resid Loss: 0.0611 | 0.0328
Epoch 273/300, resid Loss: 0.0611 | 0.0328
Epoch 274/300, resid Loss: 0.0611 | 0.0328
Epoch 275/300, resid Loss: 0.0611 | 0.0328
Epoch 276/300, resid Loss: 0.0611 | 0.0328
Epoch 277/300, resid Loss: 0.0611 | 0.0328
Epoch 278/300, resid Loss: 0.0611 | 0.0328
Epoch 279/300, resid Loss: 0.0611 | 0.0328
Epoch 280/300, resid Loss: 0.0611 | 0.0328
Epoch 281/300, resid Loss: 0.0611 | 0.0328
Epoch 282/300, resid Loss: 0.0611 | 0.0328
Epoch 283/300, resid Loss: 0.0611 | 0.0328
Epoch 284/300, resid Loss: 0.0611 | 0.0328
Epoch 285/300, resid Loss: 0.0611 | 0.0328
Epoch 286/300, resid Loss: 0.0611 | 0.0328
Epoch 287/300, resid Loss: 0.0611 | 0.0328
Epoch 288/300, resid Loss: 0.0611 | 0.0328
Epoch 289/300, resid Loss: 0.0611 | 0.0328
Epoch 290/300, resid Loss: 0.0611 | 0.0328
Epoch 291/300, resid Loss: 0.0611 | 0.0328
Epoch 292/300, resid Loss: 0.0611 | 0.0328
Epoch 293/300, resid Loss: 0.0611 | 0.0328
Epoch 294/300, resid Loss: 0.0611 | 0.0328
Epoch 295/300, resid Loss: 0.0611 | 0.0328
Epoch 296/300, resid Loss: 0.0611 | 0.0328
Epoch 297/300, resid Loss: 0.0611 | 0.0328
Epoch 298/300, resid Loss: 0.0611 | 0.0328
Epoch 299/300, resid Loss: 0.0611 | 0.0328
Epoch 300/300, resid Loss: 0.0611 | 0.0328
Runtime (seconds): 1960.2297027111053
0.00025142320948791635
[155.95401]
[-1.462216]
[-4.139088]
[10.682605]
[1.7902629]
[8.005288]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 3.8146376612130553
RMSE: 1.9531097412109375
MAE: 1.9531097412109375
R-squared: nan
[170.83087]
