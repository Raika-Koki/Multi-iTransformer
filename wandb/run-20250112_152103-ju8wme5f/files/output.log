ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-12 15:21:07,623][0m A new study created in memory with name: no-name-f9096911-4f2b-4e7f-b61a-f4d3a9e0e656[0m
[32m[I 2025-01-12 15:21:39,778][0m Trial 0 finished with value: 0.8903569269945277 and parameters: {'observation_period_num': 61, 'train_rates': 0.7413796150429948, 'learning_rate': 1.574471939465658e-06, 'batch_size': 158, 'step_size': 7, 'gamma': 0.8204318687175792}. Best is trial 0 with value: 0.8903569269945277.[0m
Early stopping at epoch 99
[32m[I 2025-01-12 15:22:21,381][0m Trial 1 finished with value: 1.2978747203102652 and parameters: {'observation_period_num': 102, 'train_rates': 0.7772451946834704, 'learning_rate': 4.485633309714942e-06, 'batch_size': 123, 'step_size': 2, 'gamma': 0.807391744898765}. Best is trial 0 with value: 0.8903569269945277.[0m
[32m[I 2025-01-12 15:22:43,147][0m Trial 2 finished with value: 1.0995619235028935 and parameters: {'observation_period_num': 226, 'train_rates': 0.6441659098404225, 'learning_rate': 5.726746357605177e-06, 'batch_size': 213, 'step_size': 15, 'gamma': 0.8658205367650953}. Best is trial 0 with value: 0.8903569269945277.[0m
[32m[I 2025-01-12 15:23:25,887][0m Trial 3 finished with value: 0.04835386936889684 and parameters: {'observation_period_num': 24, 'train_rates': 0.8716379462196764, 'learning_rate': 0.00027169814765535876, 'batch_size': 134, 'step_size': 12, 'gamma': 0.8088058958795038}. Best is trial 3 with value: 0.04835386936889684.[0m
[32m[I 2025-01-12 15:24:08,062][0m Trial 4 finished with value: 0.10178092620532278 and parameters: {'observation_period_num': 127, 'train_rates': 0.9499333043523563, 'learning_rate': 0.00010551038677329392, 'batch_size': 139, 'step_size': 12, 'gamma': 0.9476906534416636}. Best is trial 3 with value: 0.04835386936889684.[0m
Early stopping at epoch 67
[32m[I 2025-01-12 15:25:12,488][0m Trial 5 finished with value: 0.640346322156174 and parameters: {'observation_period_num': 73, 'train_rates': 0.6055737211147394, 'learning_rate': 4.97542947531592e-05, 'batch_size': 43, 'step_size': 1, 'gamma': 0.802171558256165}. Best is trial 3 with value: 0.04835386936889684.[0m
[32m[I 2025-01-12 15:25:36,912][0m Trial 6 finished with value: 0.4493955709770614 and parameters: {'observation_period_num': 28, 'train_rates': 0.6512941357513442, 'learning_rate': 1.611100789822169e-05, 'batch_size': 194, 'step_size': 9, 'gamma': 0.9171752629098664}. Best is trial 3 with value: 0.04835386936889684.[0m
[32m[I 2025-01-12 15:26:03,407][0m Trial 7 finished with value: 1.9094178077100092 and parameters: {'observation_period_num': 186, 'train_rates': 0.6263122405336967, 'learning_rate': 2.06039310718971e-06, 'batch_size': 169, 'step_size': 2, 'gamma': 0.9031576553059487}. Best is trial 3 with value: 0.04835386936889684.[0m
[32m[I 2025-01-12 15:27:16,038][0m Trial 8 finished with value: 0.2063642239795541 and parameters: {'observation_period_num': 90, 'train_rates': 0.74103321717703, 'learning_rate': 0.0008970520723930053, 'batch_size': 65, 'step_size': 10, 'gamma': 0.8972496920023405}. Best is trial 3 with value: 0.04835386936889684.[0m
[32m[I 2025-01-12 15:30:53,613][0m Trial 9 finished with value: 0.10072063984440975 and parameters: {'observation_period_num': 172, 'train_rates': 0.8245479647998444, 'learning_rate': 6.920803407732238e-05, 'batch_size': 22, 'step_size': 3, 'gamma': 0.883186505635188}. Best is trial 3 with value: 0.04835386936889684.[0m
[32m[I 2025-01-12 15:31:17,538][0m Trial 10 finished with value: 0.05756497010588646 and parameters: {'observation_period_num': 23, 'train_rates': 0.9511572238677308, 'learning_rate': 0.0004982624380526739, 'batch_size': 255, 'step_size': 15, 'gamma': 0.7613909185569371}. Best is trial 3 with value: 0.04835386936889684.[0m
[32m[I 2025-01-12 15:32:15,179][0m Trial 11 finished with value: 0.05388637169681747 and parameters: {'observation_period_num': 40, 'train_rates': 0.9605213227354948, 'learning_rate': 0.000626071498734426, 'batch_size': 102, 'step_size': 15, 'gamma': 0.7514881030953487}. Best is trial 3 with value: 0.04835386936889684.[0m
[32m[I 2025-01-12 15:33:13,949][0m Trial 12 finished with value: 0.04565847897512999 and parameters: {'observation_period_num': 23, 'train_rates': 0.8777722889678103, 'learning_rate': 0.0003890133817119117, 'batch_size': 92, 'step_size': 12, 'gamma': 0.7536127436083402}. Best is trial 12 with value: 0.04565847897512999.[0m
[32m[I 2025-01-12 15:34:17,294][0m Trial 13 finished with value: 0.03686086004038891 and parameters: {'observation_period_num': 11, 'train_rates': 0.8591185742794396, 'learning_rate': 0.00030058802861210185, 'batch_size': 86, 'step_size': 12, 'gamma': 0.8393571266042812}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:35:28,385][0m Trial 14 finished with value: 0.03959025487820537 and parameters: {'observation_period_num': 6, 'train_rates': 0.8828084020959908, 'learning_rate': 0.0001715823083474723, 'batch_size': 78, 'step_size': 12, 'gamma': 0.8578659627791914}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:36:37,572][0m Trial 15 finished with value: 0.06597453012522254 and parameters: {'observation_period_num': 54, 'train_rates': 0.886156092835966, 'learning_rate': 0.00016349870362705442, 'batch_size': 79, 'step_size': 6, 'gamma': 0.8460272109498361}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:38:12,766][0m Trial 16 finished with value: 0.05789009473521714 and parameters: {'observation_period_num': 8, 'train_rates': 0.8292420281984939, 'learning_rate': 1.968141213108283e-05, 'batch_size': 55, 'step_size': 11, 'gamma': 0.9776426669647852}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:43:39,876][0m Trial 17 finished with value: 0.11490609099467596 and parameters: {'observation_period_num': 135, 'train_rates': 0.9894617383197221, 'learning_rate': 0.00018381784394865086, 'batch_size': 17, 'step_size': 8, 'gamma': 0.8429499473937817}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:44:46,417][0m Trial 18 finished with value: 0.06253737802181741 and parameters: {'observation_period_num': 5, 'train_rates': 0.9059153404067306, 'learning_rate': 3.854602742178956e-05, 'batch_size': 85, 'step_size': 13, 'gamma': 0.8509304178695671}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:45:27,648][0m Trial 19 finished with value: 0.22703655889159755 and parameters: {'observation_period_num': 239, 'train_rates': 0.8250547325168709, 'learning_rate': 0.00012471187659603304, 'batch_size': 118, 'step_size': 5, 'gamma': 0.7852365593842553}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:47:47,586][0m Trial 20 finished with value: 0.09188504654243901 and parameters: {'observation_period_num': 92, 'train_rates': 0.9167403904158022, 'learning_rate': 0.0003181618989971257, 'batch_size': 39, 'step_size': 13, 'gamma': 0.9274402783292535}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:48:44,749][0m Trial 21 finished with value: 0.05741727507055396 and parameters: {'observation_period_num': 43, 'train_rates': 0.8613769761000445, 'learning_rate': 0.00038187436961539004, 'batch_size': 96, 'step_size': 10, 'gamma': 0.7718503136879818}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:50:00,682][0m Trial 22 finished with value: 0.16849005742678566 and parameters: {'observation_period_num': 6, 'train_rates': 0.7862846730182502, 'learning_rate': 0.0009275131550764569, 'batch_size': 68, 'step_size': 12, 'gamma': 0.8324020659592319}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:50:56,109][0m Trial 23 finished with value: 0.0565318553934334 and parameters: {'observation_period_num': 67, 'train_rates': 0.8502427578531495, 'learning_rate': 0.0002357372894242561, 'batch_size': 97, 'step_size': 13, 'gamma': 0.867811730676653}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:51:48,438][0m Trial 24 finished with value: 0.07167588895269733 and parameters: {'observation_period_num': 38, 'train_rates': 0.915379090384116, 'learning_rate': 9.829864417961273e-05, 'batch_size': 111, 'step_size': 10, 'gamma': 0.7845139311460232}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:52:57,543][0m Trial 25 finished with value: 0.062173477200679596 and parameters: {'observation_period_num': 22, 'train_rates': 0.8919651309721388, 'learning_rate': 0.0004859096263671241, 'batch_size': 80, 'step_size': 14, 'gamma': 0.8863391741498154}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:54:36,146][0m Trial 26 finished with value: 0.24112622365247355 and parameters: {'observation_period_num': 111, 'train_rates': 0.7579076465615111, 'learning_rate': 6.767200928828944e-05, 'batch_size': 47, 'step_size': 11, 'gamma': 0.9502350264880657}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:55:08,361][0m Trial 27 finished with value: 0.5457366724134329 and parameters: {'observation_period_num': 79, 'train_rates': 0.7030560759695108, 'learning_rate': 2.417878167443968e-05, 'batch_size': 153, 'step_size': 8, 'gamma': 0.8283485325065821}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:56:21,170][0m Trial 28 finished with value: 0.05900084803611534 and parameters: {'observation_period_num': 51, 'train_rates': 0.8130360398467349, 'learning_rate': 0.0002294195703746273, 'batch_size': 70, 'step_size': 14, 'gamma': 0.7908268906344524}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:56:53,937][0m Trial 29 finished with value: 0.07099169052459976 and parameters: {'observation_period_num': 61, 'train_rates': 0.8518491556671607, 'learning_rate': 0.0006818751763731182, 'batch_size': 169, 'step_size': 9, 'gamma': 0.819823449128519}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:57:54,476][0m Trial 30 finished with value: 0.0858206542740975 and parameters: {'observation_period_num': 160, 'train_rates': 0.929843088817552, 'learning_rate': 0.00014649258754539986, 'batch_size': 91, 'step_size': 7, 'gamma': 0.8596731713156287}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:58:36,937][0m Trial 31 finished with value: 0.04886694790011993 and parameters: {'observation_period_num': 24, 'train_rates': 0.8774815807363859, 'learning_rate': 0.0003034510892213714, 'batch_size': 134, 'step_size': 12, 'gamma': 0.8132461307023914}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 15:59:19,117][0m Trial 32 finished with value: 0.0487386919361463 and parameters: {'observation_period_num': 19, 'train_rates': 0.8709780437001909, 'learning_rate': 0.000323220964260945, 'batch_size': 131, 'step_size': 11, 'gamma': 0.8343994254708613}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:00:05,981][0m Trial 33 finished with value: 0.08500834407603687 and parameters: {'observation_period_num': 46, 'train_rates': 0.8474990164562579, 'learning_rate': 0.00022340490157627427, 'batch_size': 113, 'step_size': 14, 'gamma': 0.8000713880863823}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:00:41,372][0m Trial 34 finished with value: 0.10227834358937446 and parameters: {'observation_period_num': 35, 'train_rates': 0.797683216559635, 'learning_rate': 8.166514419772381e-05, 'batch_size': 148, 'step_size': 12, 'gamma': 0.7731343929184639}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:01:27,997][0m Trial 35 finished with value: 0.17088222191447303 and parameters: {'observation_period_num': 5, 'train_rates': 0.8933767878227036, 'learning_rate': 1.1585301432400923e-05, 'batch_size': 124, 'step_size': 11, 'gamma': 0.8144154386084353}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:02:00,087][0m Trial 36 finished with value: 0.05176412686705589 and parameters: {'observation_period_num': 18, 'train_rates': 0.9421375577462361, 'learning_rate': 0.00044502761716460313, 'batch_size': 194, 'step_size': 13, 'gamma': 0.8812919595938982}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:03:36,032][0m Trial 37 finished with value: 0.8587383698543598 and parameters: {'observation_period_num': 78, 'train_rates': 0.83880083601708, 'learning_rate': 1.0180025836605379e-06, 'batch_size': 53, 'step_size': 9, 'gamma': 0.8004474001660417}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:04:20,269][0m Trial 38 finished with value: 0.4640618465795107 and parameters: {'observation_period_num': 194, 'train_rates': 0.7639616385311149, 'learning_rate': 4.0130818815205747e-05, 'batch_size': 110, 'step_size': 12, 'gamma': 0.7502580134661854}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:06:51,121][0m Trial 39 finished with value: 0.18892087397368057 and parameters: {'observation_period_num': 32, 'train_rates': 0.8033300516142136, 'learning_rate': 4.072422112015109e-06, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8645915540476702}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:07:25,334][0m Trial 40 finished with value: 0.07777182757854462 and parameters: {'observation_period_num': 56, 'train_rates': 0.9689930866598053, 'learning_rate': 0.0006338596904282288, 'batch_size': 176, 'step_size': 10, 'gamma': 0.8538136169864219}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:08:09,955][0m Trial 41 finished with value: 0.05420827231999999 and parameters: {'observation_period_num': 19, 'train_rates': 0.8736131487102474, 'learning_rate': 0.00031031636001792995, 'batch_size': 127, 'step_size': 11, 'gamma': 0.8337187105869346}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:08:53,267][0m Trial 42 finished with value: 0.052334217578172686 and parameters: {'observation_period_num': 30, 'train_rates': 0.8633246764699808, 'learning_rate': 0.0002466814499189609, 'batch_size': 131, 'step_size': 11, 'gamma': 0.8376802791026744}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:09:46,874][0m Trial 43 finished with value: 0.04676496063582988 and parameters: {'observation_period_num': 15, 'train_rates': 0.9023713232524648, 'learning_rate': 0.00011687816420902202, 'batch_size': 104, 'step_size': 12, 'gamma': 0.8785909263568644}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:10:43,023][0m Trial 44 finished with value: 0.05029653138659557 and parameters: {'observation_period_num': 13, 'train_rates': 0.9026799230201812, 'learning_rate': 0.00013272176303030127, 'batch_size': 102, 'step_size': 13, 'gamma': 0.903279731652625}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:11:25,099][0m Trial 45 finished with value: 0.06011206061788359 and parameters: {'observation_period_num': 37, 'train_rates': 0.932820398185533, 'learning_rate': 9.824583651174389e-05, 'batch_size': 143, 'step_size': 12, 'gamma': 0.8778538697437507}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:12:39,573][0m Trial 46 finished with value: 0.08307977867701129 and parameters: {'observation_period_num': 66, 'train_rates': 0.9228611438429324, 'learning_rate': 0.00018241804748669112, 'batch_size': 75, 'step_size': 15, 'gamma': 0.8934308985072235}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:13:40,373][0m Trial 47 finished with value: 0.12323163903277853 and parameters: {'observation_period_num': 210, 'train_rates': 0.8911525841825302, 'learning_rate': 0.0006922421427239184, 'batch_size': 85, 'step_size': 10, 'gamma': 0.9129730840438822}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:15:16,208][0m Trial 48 finished with value: 0.10885348012953093 and parameters: {'observation_period_num': 117, 'train_rates': 0.9768683490281854, 'learning_rate': 5.641772277704457e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.8743305148914206}. Best is trial 13 with value: 0.03686086004038891.[0m
[32m[I 2025-01-12 16:15:39,151][0m Trial 49 finished with value: 0.1183975638081574 and parameters: {'observation_period_num': 49, 'train_rates': 0.8172739596533027, 'learning_rate': 0.0004377606335791762, 'batch_size': 250, 'step_size': 5, 'gamma': 0.7681320941040243}. Best is trial 13 with value: 0.03686086004038891.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-12 16:15:39,161][0m A new study created in memory with name: no-name-6fa0ef38-49de-46a0-9ed9-2c4ac0214903[0m
[32m[I 2025-01-12 16:16:06,702][0m Trial 0 finished with value: 0.4198831540802342 and parameters: {'observation_period_num': 226, 'train_rates': 0.7080789871580984, 'learning_rate': 3.975186981260046e-05, 'batch_size': 170, 'step_size': 7, 'gamma': 0.9530900950299266}. Best is trial 0 with value: 0.4198831540802342.[0m
[32m[I 2025-01-12 16:16:28,401][0m Trial 1 finished with value: 0.18956038336862216 and parameters: {'observation_period_num': 28, 'train_rates': 0.6239348678078207, 'learning_rate': 0.0007812413103795738, 'batch_size': 223, 'step_size': 3, 'gamma': 0.9158632950952591}. Best is trial 1 with value: 0.18956038336862216.[0m
[32m[I 2025-01-12 16:17:14,579][0m Trial 2 finished with value: 0.29383352232431587 and parameters: {'observation_period_num': 244, 'train_rates': 0.649841953701742, 'learning_rate': 0.00044549401256233805, 'batch_size': 89, 'step_size': 4, 'gamma': 0.8580080198423854}. Best is trial 1 with value: 0.18956038336862216.[0m
[32m[I 2025-01-12 16:17:41,251][0m Trial 3 finished with value: 0.08880476786358522 and parameters: {'observation_period_num': 95, 'train_rates': 0.8235222628203847, 'learning_rate': 0.0005853785847578037, 'batch_size': 200, 'step_size': 12, 'gamma': 0.957930711209563}. Best is trial 3 with value: 0.08880476786358522.[0m
[32m[I 2025-01-12 16:18:17,628][0m Trial 4 finished with value: 0.4694448047214084 and parameters: {'observation_period_num': 15, 'train_rates': 0.9111951781031886, 'learning_rate': 2.106198844457562e-06, 'batch_size': 160, 'step_size': 14, 'gamma': 0.7941089243425007}. Best is trial 3 with value: 0.08880476786358522.[0m
[32m[I 2025-01-12 16:18:49,942][0m Trial 5 finished with value: 0.06377311116632293 and parameters: {'observation_period_num': 14, 'train_rates': 0.8499362583259351, 'learning_rate': 5.3548937805871896e-05, 'batch_size': 177, 'step_size': 14, 'gamma': 0.9640407423213873}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:19:16,245][0m Trial 6 finished with value: 0.06541012424645544 and parameters: {'observation_period_num': 54, 'train_rates': 0.8640080606435723, 'learning_rate': 0.0006953630890915088, 'batch_size': 223, 'step_size': 12, 'gamma': 0.9047944752743287}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:20:52,842][0m Trial 7 finished with value: 0.411609286590597 and parameters: {'observation_period_num': 205, 'train_rates': 0.766799510266498, 'learning_rate': 1.6966657830114847e-05, 'batch_size': 47, 'step_size': 12, 'gamma': 0.9036144052777754}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:22:14,976][0m Trial 8 finished with value: 0.09021828470220528 and parameters: {'observation_period_num': 160, 'train_rates': 0.821794895937344, 'learning_rate': 0.0005266386204941299, 'batch_size': 59, 'step_size': 11, 'gamma': 0.7576321327768718}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:23:18,958][0m Trial 9 finished with value: 0.40389474038562523 and parameters: {'observation_period_num': 225, 'train_rates': 0.6405848294838936, 'learning_rate': 0.00014798671817419073, 'batch_size': 64, 'step_size': 3, 'gamma': 0.8497127023777115}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:24:10,290][0m Trial 10 finished with value: 0.3681870102882385 and parameters: {'observation_period_num': 107, 'train_rates': 0.9870595630436029, 'learning_rate': 9.028549488723386e-06, 'batch_size': 116, 'step_size': 15, 'gamma': 0.986299208583954}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:24:34,384][0m Trial 11 finished with value: 0.11694063983325448 and parameters: {'observation_period_num': 57, 'train_rates': 0.9035530833455926, 'learning_rate': 8.22088974102388e-05, 'batch_size': 251, 'step_size': 9, 'gamma': 0.9070153619404702}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:25:04,834][0m Trial 12 finished with value: 0.07158497264308314 and parameters: {'observation_period_num': 64, 'train_rates': 0.8927867154188803, 'learning_rate': 0.00015394797091803703, 'batch_size': 193, 'step_size': 8, 'gamma': 0.9424874405400859}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:25:25,540][0m Trial 13 finished with value: 0.6633352650118353 and parameters: {'observation_period_num': 11, 'train_rates': 0.7633422288873676, 'learning_rate': 4.189926234572117e-06, 'batch_size': 252, 'step_size': 14, 'gamma': 0.987571474165578}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:26:06,412][0m Trial 14 finished with value: 0.11710752610697434 and parameters: {'observation_period_num': 60, 'train_rates': 0.8468949467428291, 'learning_rate': 4.136167537463585e-05, 'batch_size': 136, 'step_size': 10, 'gamma': 0.880642274003203}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:26:34,434][0m Trial 15 finished with value: 2.278563976287842 and parameters: {'observation_period_num': 155, 'train_rates': 0.9734968825143158, 'learning_rate': 1.1355540288925623e-06, 'batch_size': 215, 'step_size': 6, 'gamma': 0.8270019047898072}. Best is trial 5 with value: 0.06377311116632293.[0m
[32m[I 2025-01-12 16:27:04,728][0m Trial 16 finished with value: 0.06090994599415823 and parameters: {'observation_period_num': 41, 'train_rates': 0.8659335580328771, 'learning_rate': 0.00024106266456754051, 'batch_size': 181, 'step_size': 13, 'gamma': 0.9268384420157649}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:27:41,282][0m Trial 17 finished with value: 0.1591659326763714 and parameters: {'observation_period_num': 93, 'train_rates': 0.9408726977657978, 'learning_rate': 0.00019004645990214792, 'batch_size': 164, 'step_size': 1, 'gamma': 0.9439010923612867}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:28:21,826][0m Trial 18 finished with value: 0.1988489719764363 and parameters: {'observation_period_num': 34, 'train_rates': 0.7145943347382326, 'learning_rate': 8.14806483879431e-05, 'batch_size': 122, 'step_size': 15, 'gamma': 0.9270326650024159}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:28:49,179][0m Trial 19 finished with value: 0.552407162510641 and parameters: {'observation_period_num': 123, 'train_rates': 0.7679519410432136, 'learning_rate': 1.9148666150597287e-05, 'batch_size': 182, 'step_size': 13, 'gamma': 0.972327576957023}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:32:39,827][0m Trial 20 finished with value: 0.08062225518127282 and parameters: {'observation_period_num': 79, 'train_rates': 0.7967397146046064, 'learning_rate': 0.000324762851148851, 'batch_size': 21, 'step_size': 10, 'gamma': 0.884881141380412}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:33:05,822][0m Trial 21 finished with value: 0.07878919069084997 and parameters: {'observation_period_num': 39, 'train_rates': 0.8551039720515532, 'learning_rate': 0.0002616069790987512, 'batch_size': 224, 'step_size': 13, 'gamma': 0.9324980544900263}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:33:44,667][0m Trial 22 finished with value: 0.07270027620150991 and parameters: {'observation_period_num': 6, 'train_rates': 0.87534534714933, 'learning_rate': 6.676323205261326e-05, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8952789942095395}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:34:14,376][0m Trial 23 finished with value: 0.07419325411319733 and parameters: {'observation_period_num': 45, 'train_rates': 0.9377816153613612, 'learning_rate': 0.0001331517580197736, 'batch_size': 207, 'step_size': 14, 'gamma': 0.9678177878241446}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:34:38,621][0m Trial 24 finished with value: 0.07886595069774853 and parameters: {'observation_period_num': 72, 'train_rates': 0.8592578954937384, 'learning_rate': 0.0009181842248311444, 'batch_size': 236, 'step_size': 11, 'gamma': 0.9240142013694309}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:35:07,385][0m Trial 25 finished with value: 0.07023068775242272 and parameters: {'observation_period_num': 43, 'train_rates': 0.8194302953055166, 'learning_rate': 0.00030445172151754923, 'batch_size': 186, 'step_size': 13, 'gamma': 0.8674640059927622}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:35:41,637][0m Trial 26 finished with value: 0.18614345788955688 and parameters: {'observation_period_num': 20, 'train_rates': 0.9478468859718184, 'learning_rate': 2.03298498788294e-05, 'batch_size': 176, 'step_size': 15, 'gamma': 0.9151064684374588}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:36:18,086][0m Trial 27 finished with value: 0.47714114741042807 and parameters: {'observation_period_num': 130, 'train_rates': 0.8853814948455552, 'learning_rate': 9.231054822325047e-06, 'batch_size': 148, 'step_size': 10, 'gamma': 0.8316127043688879}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:36:40,815][0m Trial 28 finished with value: 0.15226806116368133 and parameters: {'observation_period_num': 81, 'train_rates': 0.7958444751897757, 'learning_rate': 6.348426178008889e-05, 'batch_size': 238, 'step_size': 11, 'gamma': 0.9411109476762198}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:37:06,314][0m Trial 29 finished with value: 0.34263871965638126 and parameters: {'observation_period_num': 54, 'train_rates': 0.7137183678552269, 'learning_rate': 3.424691019745101e-05, 'batch_size': 204, 'step_size': 7, 'gamma': 0.9572942600509423}. Best is trial 16 with value: 0.06090994599415823.[0m
[32m[I 2025-01-12 16:37:58,824][0m Trial 30 finished with value: 0.053975819893505264 and parameters: {'observation_period_num': 28, 'train_rates': 0.9214179863133912, 'learning_rate': 0.00036380708867483463, 'batch_size': 110, 'step_size': 13, 'gamma': 0.8914547704485994}. Best is trial 30 with value: 0.053975819893505264.[0m
[32m[I 2025-01-12 16:38:55,204][0m Trial 31 finished with value: 0.055378383782699825 and parameters: {'observation_period_num': 26, 'train_rates': 0.9160300711720724, 'learning_rate': 0.00040589702502769927, 'batch_size': 101, 'step_size': 13, 'gamma': 0.8811891161919576}. Best is trial 30 with value: 0.053975819893505264.[0m
[32m[I 2025-01-12 16:39:50,723][0m Trial 32 finished with value: 0.043927253469965365 and parameters: {'observation_period_num': 24, 'train_rates': 0.9278327689311053, 'learning_rate': 0.00038727381524878606, 'batch_size': 103, 'step_size': 14, 'gamma': 0.8846620692411822}. Best is trial 32 with value: 0.043927253469965365.[0m
[32m[I 2025-01-12 16:40:51,434][0m Trial 33 finished with value: 0.05807105461582867 and parameters: {'observation_period_num': 35, 'train_rates': 0.9216924090975201, 'learning_rate': 0.00043067175733728374, 'batch_size': 94, 'step_size': 13, 'gamma': 0.8819052255751778}. Best is trial 32 with value: 0.043927253469965365.[0m
[32m[I 2025-01-12 16:41:52,544][0m Trial 34 finished with value: 0.05050624760895454 and parameters: {'observation_period_num': 29, 'train_rates': 0.9206978415510224, 'learning_rate': 0.0004540922257685832, 'batch_size': 95, 'step_size': 15, 'gamma': 0.8493313350793562}. Best is trial 32 with value: 0.043927253469965365.[0m
[32m[I 2025-01-12 16:42:56,508][0m Trial 35 finished with value: 0.04348844561342881 and parameters: {'observation_period_num': 21, 'train_rates': 0.9635220533025977, 'learning_rate': 0.000939239308839558, 'batch_size': 92, 'step_size': 15, 'gamma': 0.8380574290116278}. Best is trial 35 with value: 0.04348844561342881.[0m
[32m[I 2025-01-12 16:44:12,308][0m Trial 36 finished with value: 0.04442064378942762 and parameters: {'observation_period_num': 21, 'train_rates': 0.9644998676867185, 'learning_rate': 0.0009496042860838183, 'batch_size': 78, 'step_size': 15, 'gamma': 0.8499351699707532}. Best is trial 35 with value: 0.04348844561342881.[0m
[32m[I 2025-01-12 16:45:24,330][0m Trial 37 finished with value: 0.04130635991692543 and parameters: {'observation_period_num': 5, 'train_rates': 0.9662419625741364, 'learning_rate': 0.0008990871841928847, 'batch_size': 82, 'step_size': 15, 'gamma': 0.8103769268497819}. Best is trial 37 with value: 0.04130635991692543.[0m
[32m[I 2025-01-12 16:46:38,458][0m Trial 38 finished with value: 0.036370718208226294 and parameters: {'observation_period_num': 9, 'train_rates': 0.9628948559377711, 'learning_rate': 0.0009341274656435258, 'batch_size': 79, 'step_size': 14, 'gamma': 0.7990146823230272}. Best is trial 38 with value: 0.036370718208226294.[0m
[32m[I 2025-01-12 16:47:59,082][0m Trial 39 finished with value: 0.039805538952350616 and parameters: {'observation_period_num': 16, 'train_rates': 0.9883638699004521, 'learning_rate': 0.000689517216794125, 'batch_size': 74, 'step_size': 14, 'gamma': 0.7792792896142083}. Best is trial 38 with value: 0.036370718208226294.[0m
[32m[I 2025-01-12 16:49:14,777][0m Trial 40 finished with value: 0.04536666105999503 and parameters: {'observation_period_num': 5, 'train_rates': 0.9563422373157061, 'learning_rate': 0.000677105496397018, 'batch_size': 77, 'step_size': 5, 'gamma': 0.7892288706619842}. Best is trial 38 with value: 0.036370718208226294.[0m
[32m[I 2025-01-12 16:51:32,583][0m Trial 41 finished with value: 0.03603026866912842 and parameters: {'observation_period_num': 16, 'train_rates': 0.9848286955763662, 'learning_rate': 0.0006064297710302031, 'batch_size': 42, 'step_size': 14, 'gamma': 0.7908288789003469}. Best is trial 41 with value: 0.03603026866912842.[0m
[32m[I 2025-01-12 16:53:59,874][0m Trial 42 finished with value: 0.037750723461310066 and parameters: {'observation_period_num': 13, 'train_rates': 0.9849846485329088, 'learning_rate': 0.0006561982762679763, 'batch_size': 40, 'step_size': 14, 'gamma': 0.7871641167592917}. Best is trial 41 with value: 0.03603026866912842.[0m
[32m[I 2025-01-12 16:57:41,778][0m Trial 43 finished with value: 0.03289005818755128 and parameters: {'observation_period_num': 6, 'train_rates': 0.9856239085242493, 'learning_rate': 0.0006192760326421162, 'batch_size': 26, 'step_size': 14, 'gamma': 0.7926101349551267}. Best is trial 43 with value: 0.03289005818755128.[0m
[32m[I 2025-01-12 17:00:38,319][0m Trial 44 finished with value: 0.039696794003248215 and parameters: {'observation_period_num': 14, 'train_rates': 0.9898543056253224, 'learning_rate': 0.00069194030964841, 'batch_size': 30, 'step_size': 12, 'gamma': 0.7667962775668231}. Best is trial 43 with value: 0.03289005818755128.[0m
[32m[I 2025-01-12 17:03:30,523][0m Trial 45 finished with value: 0.1115750688495058 and parameters: {'observation_period_num': 179, 'train_rates': 0.988298367937373, 'learning_rate': 0.000622921948222145, 'batch_size': 31, 'step_size': 12, 'gamma': 0.7534695228832858}. Best is trial 43 with value: 0.03289005818755128.[0m
[32m[I 2025-01-12 17:05:38,230][0m Trial 46 finished with value: 0.054672286787535995 and parameters: {'observation_period_num': 51, 'train_rates': 0.9781311467308312, 'learning_rate': 0.0005711018562275712, 'batch_size': 45, 'step_size': 12, 'gamma': 0.7723871338165528}. Best is trial 43 with value: 0.03289005818755128.[0m
[32m[I 2025-01-12 17:07:54,949][0m Trial 47 finished with value: 0.03903507856521637 and parameters: {'observation_period_num': 18, 'train_rates': 0.9465293622313782, 'learning_rate': 0.00019810264433329182, 'batch_size': 41, 'step_size': 14, 'gamma': 0.805964395619587}. Best is trial 43 with value: 0.03289005818755128.[0m
[32m[I 2025-01-12 17:09:42,894][0m Trial 48 finished with value: 0.07188108357889898 and parameters: {'observation_period_num': 68, 'train_rates': 0.9498918980214535, 'learning_rate': 0.00011732899138094123, 'batch_size': 52, 'step_size': 14, 'gamma': 0.8108862934698626}. Best is trial 43 with value: 0.03289005818755128.[0m
[32m[I 2025-01-12 17:14:34,290][0m Trial 49 finished with value: 0.10301947943272366 and parameters: {'observation_period_num': 204, 'train_rates': 0.8998975108068046, 'learning_rate': 0.00020174196390655453, 'batch_size': 17, 'step_size': 11, 'gamma': 0.807205229093108}. Best is trial 43 with value: 0.03289005818755128.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-12 17:14:34,299][0m A new study created in memory with name: no-name-fe02da4c-40ad-4d6e-a397-c2f93f0df417[0m
[32m[I 2025-01-12 17:14:56,964][0m Trial 0 finished with value: 1.954789613976198 and parameters: {'observation_period_num': 66, 'train_rates': 0.7999666608588276, 'learning_rate': 1.406475663776681e-06, 'batch_size': 243, 'step_size': 8, 'gamma': 0.9352188306242798}. Best is trial 0 with value: 1.954789613976198.[0m
[32m[I 2025-01-12 17:15:20,013][0m Trial 1 finished with value: 0.9837975673773686 and parameters: {'observation_period_num': 152, 'train_rates': 0.7918621374138972, 'learning_rate': 3.1533852596924555e-06, 'batch_size': 223, 'step_size': 4, 'gamma': 0.9596778585709878}. Best is trial 1 with value: 0.9837975673773686.[0m
[32m[I 2025-01-12 17:17:08,383][0m Trial 2 finished with value: 0.07121435552835464 and parameters: {'observation_period_num': 75, 'train_rates': 0.985100827992224, 'learning_rate': 0.00031980304613334184, 'batch_size': 53, 'step_size': 1, 'gamma': 0.9681169494527102}. Best is trial 2 with value: 0.07121435552835464.[0m
[32m[I 2025-01-12 17:17:39,045][0m Trial 3 finished with value: 0.31877822081247964 and parameters: {'observation_period_num': 200, 'train_rates': 0.9183216003604001, 'learning_rate': 1.099307101604089e-05, 'batch_size': 169, 'step_size': 13, 'gamma': 0.9748993742738439}. Best is trial 2 with value: 0.07121435552835464.[0m
[32m[I 2025-01-12 17:21:05,662][0m Trial 4 finished with value: 0.41405565394123245 and parameters: {'observation_period_num': 126, 'train_rates': 0.7147841704390874, 'learning_rate': 6.384022630636615e-06, 'batch_size': 21, 'step_size': 5, 'gamma': 0.9062363278359001}. Best is trial 2 with value: 0.07121435552835464.[0m
[32m[I 2025-01-12 17:21:46,425][0m Trial 5 finished with value: 0.4209150685398528 and parameters: {'observation_period_num': 118, 'train_rates': 0.6873092988982982, 'learning_rate': 2.1821768645228958e-05, 'batch_size': 110, 'step_size': 15, 'gamma': 0.908102682994362}. Best is trial 2 with value: 0.07121435552835464.[0m
[32m[I 2025-01-12 17:22:40,063][0m Trial 6 finished with value: 0.16431868010390122 and parameters: {'observation_period_num': 193, 'train_rates': 0.9184137345322573, 'learning_rate': 0.00025841318995239196, 'batch_size': 98, 'step_size': 3, 'gamma': 0.7547390321064098}. Best is trial 2 with value: 0.07121435552835464.[0m
[32m[I 2025-01-12 17:23:08,931][0m Trial 7 finished with value: 0.10700138466376247 and parameters: {'observation_period_num': 57, 'train_rates': 0.8178133395833598, 'learning_rate': 0.000258789196517861, 'batch_size': 194, 'step_size': 8, 'gamma': 0.9836543551139434}. Best is trial 2 with value: 0.07121435552835464.[0m
[32m[I 2025-01-12 17:23:34,129][0m Trial 8 finished with value: 0.2899594108917211 and parameters: {'observation_period_num': 121, 'train_rates': 0.7183990123258055, 'learning_rate': 0.00010509919017293922, 'batch_size': 189, 'step_size': 14, 'gamma': 0.9098172699322796}. Best is trial 2 with value: 0.07121435552835464.[0m
[32m[I 2025-01-12 17:23:53,072][0m Trial 9 finished with value: 1.178871328779308 and parameters: {'observation_period_num': 224, 'train_rates': 0.7283103056693689, 'learning_rate': 1.5996695688851419e-06, 'batch_size': 251, 'step_size': 5, 'gamma': 0.867406653434857}. Best is trial 2 with value: 0.07121435552835464.[0m
Early stopping at epoch 66
[32m[I 2025-01-12 17:27:21,420][0m Trial 10 finished with value: 0.04198554373131348 and parameters: {'observation_period_num': 7, 'train_rates': 0.982657579468666, 'learning_rate': 0.0009189535315795286, 'batch_size': 18, 'step_size': 1, 'gamma': 0.7939795784101115}. Best is trial 10 with value: 0.04198554373131348.[0m
Early stopping at epoch 74
[32m[I 2025-01-12 17:30:25,251][0m Trial 11 finished with value: 0.042265002305308975 and parameters: {'observation_period_num': 5, 'train_rates': 0.982825648578198, 'learning_rate': 0.0009276021675856119, 'batch_size': 23, 'step_size': 1, 'gamma': 0.8157054630540574}. Best is trial 10 with value: 0.04198554373131348.[0m
Early stopping at epoch 65
[32m[I 2025-01-12 17:32:14,669][0m Trial 12 finished with value: 0.05801315548328253 and parameters: {'observation_period_num': 6, 'train_rates': 0.9737709901645895, 'learning_rate': 0.0008732962908735379, 'batch_size': 34, 'step_size': 1, 'gamma': 0.7961885973187961}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:33:15,196][0m Trial 13 finished with value: 0.18464042146659837 and parameters: {'observation_period_num': 5, 'train_rates': 0.6044808700362209, 'learning_rate': 6.557661647499536e-05, 'batch_size': 70, 'step_size': 11, 'gamma': 0.8262277522106658}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:34:26,582][0m Trial 14 finished with value: 0.05758656152224136 and parameters: {'observation_period_num': 39, 'train_rates': 0.8988805063301558, 'learning_rate': 0.000990903981912505, 'batch_size': 76, 'step_size': 2, 'gamma': 0.8158458945545779}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:38:52,704][0m Trial 15 finished with value: 0.0480149077797268 and parameters: {'observation_period_num': 35, 'train_rates': 0.8585471728434944, 'learning_rate': 0.0004572666679164069, 'batch_size': 19, 'step_size': 6, 'gamma': 0.7623094048982035}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:39:36,620][0m Trial 16 finished with value: 0.10528992427742645 and parameters: {'observation_period_num': 97, 'train_rates': 0.9491615283790288, 'learning_rate': 9.387615560166841e-05, 'batch_size': 133, 'step_size': 11, 'gamma': 0.8476213626602853}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:41:17,650][0m Trial 17 finished with value: 0.06499848674578838 and parameters: {'observation_period_num': 30, 'train_rates': 0.858151957709209, 'learning_rate': 0.00014805726946229806, 'batch_size': 52, 'step_size': 3, 'gamma': 0.7835469667476395}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:42:15,020][0m Trial 18 finished with value: 0.2781587090707363 and parameters: {'observation_period_num': 155, 'train_rates': 0.9526698750749174, 'learning_rate': 3.485754207226814e-05, 'batch_size': 97, 'step_size': 7, 'gamma': 0.8348518311989237}. Best is trial 10 with value: 0.04198554373131348.[0m
Early stopping at epoch 61
[32m[I 2025-01-12 17:42:38,425][0m Trial 19 finished with value: 0.24196311355995656 and parameters: {'observation_period_num': 96, 'train_rates': 0.8772040422491227, 'learning_rate': 0.0005515133782978155, 'batch_size': 142, 'step_size': 1, 'gamma': 0.8001041090371042}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:44:40,515][0m Trial 20 finished with value: 0.11862005293369293 and parameters: {'observation_period_num': 251, 'train_rates': 0.9885998502310276, 'learning_rate': 0.00018126873975058235, 'batch_size': 44, 'step_size': 10, 'gamma': 0.8550392245083734}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:49:41,688][0m Trial 21 finished with value: 0.05181839486843423 and parameters: {'observation_period_num': 27, 'train_rates': 0.8381150611689747, 'learning_rate': 0.0004535618986158983, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7676810191871618}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:54:56,304][0m Trial 22 finished with value: 0.05121710330290212 and parameters: {'observation_period_num': 41, 'train_rates': 0.9396431743678254, 'learning_rate': 0.0006575783931725288, 'batch_size': 17, 'step_size': 3, 'gamma': 0.7773016642804403}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:56:16,494][0m Trial 23 finished with value: 0.04825190748593325 and parameters: {'observation_period_num': 9, 'train_rates': 0.884314606050453, 'learning_rate': 0.00044164016542076886, 'batch_size': 67, 'step_size': 5, 'gamma': 0.7528227164503231}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 17:58:05,628][0m Trial 24 finished with value: 0.20706397171569876 and parameters: {'observation_period_num': 56, 'train_rates': 0.7710426301267914, 'learning_rate': 0.0008668857252813331, 'batch_size': 44, 'step_size': 2, 'gamma': 0.8111356597902314}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:01:01,258][0m Trial 25 finished with value: 0.055536593998903816 and parameters: {'observation_period_num': 24, 'train_rates': 0.9214807385385939, 'learning_rate': 0.0003626439579981855, 'batch_size': 31, 'step_size': 4, 'gamma': 0.7908241355671879}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:02:06,832][0m Trial 26 finished with value: 0.18745519345301717 and parameters: {'observation_period_num': 85, 'train_rates': 0.854044678861997, 'learning_rate': 4.2126168470155784e-05, 'batch_size': 80, 'step_size': 7, 'gamma': 0.7693222081055541}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:04:05,021][0m Trial 27 finished with value: 0.15758152126907585 and parameters: {'observation_period_num': 49, 'train_rates': 0.6565709580235393, 'learning_rate': 0.00019875262076636242, 'batch_size': 35, 'step_size': 10, 'gamma': 0.8076941736643096}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:04:50,795][0m Trial 28 finished with value: 0.0798269659280777 and parameters: {'observation_period_num': 24, 'train_rates': 0.9655946155908063, 'learning_rate': 0.0005957785138984014, 'batch_size': 125, 'step_size': 2, 'gamma': 0.8312360079433595}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:06:23,145][0m Trial 29 finished with value: 0.06758089768781997 and parameters: {'observation_period_num': 67, 'train_rates': 0.9013614639038391, 'learning_rate': 0.00012126216095453376, 'batch_size': 59, 'step_size': 9, 'gamma': 0.8842286510715283}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:07:28,757][0m Trial 30 finished with value: 0.04725269026103172 and parameters: {'observation_period_num': 24, 'train_rates': 0.9360918742781748, 'learning_rate': 0.0009449694044632343, 'batch_size': 85, 'step_size': 4, 'gamma': 0.7731751214179097}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:08:31,425][0m Trial 31 finished with value: 0.05096405887355407 and parameters: {'observation_period_num': 18, 'train_rates': 0.9387974270056951, 'learning_rate': 0.0009432472700116605, 'batch_size': 87, 'step_size': 4, 'gamma': 0.7749313520138947}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:11:37,805][0m Trial 32 finished with value: 0.0536854217918414 and parameters: {'observation_period_num': 41, 'train_rates': 0.9636795486139186, 'learning_rate': 0.0006193352454566393, 'batch_size': 30, 'step_size': 6, 'gamma': 0.759102931523193}. Best is trial 10 with value: 0.04198554373131348.[0m
Early stopping at epoch 59
[32m[I 2025-01-12 18:12:52,289][0m Trial 33 finished with value: 0.08544060587882996 and parameters: {'observation_period_num': 16, 'train_rates': 0.9887008713374341, 'learning_rate': 0.00033134955133996965, 'batch_size': 48, 'step_size': 1, 'gamma': 0.7885350100309669}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:13:25,984][0m Trial 34 finished with value: 0.250637358590651 and parameters: {'observation_period_num': 73, 'train_rates': 0.7662472233833431, 'learning_rate': 0.00044286558447001424, 'batch_size': 151, 'step_size': 3, 'gamma': 0.8187131538371132}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:15:04,771][0m Trial 35 finished with value: 0.21234199184502286 and parameters: {'observation_period_num': 34, 'train_rates': 0.9307588955325103, 'learning_rate': 1.6729678393263333e-05, 'batch_size': 57, 'step_size': 6, 'gamma': 0.8008162473196503}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:19:47,838][0m Trial 36 finished with value: 0.390968157913222 and parameters: {'observation_period_num': 155, 'train_rates': 0.8050041786646681, 'learning_rate': 5.121112903461401e-06, 'batch_size': 16, 'step_size': 4, 'gamma': 0.7795279479447196}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:20:34,983][0m Trial 37 finished with value: 0.13287922407138797 and parameters: {'observation_period_num': 55, 'train_rates': 0.8999155864639977, 'learning_rate': 0.00024917394729019875, 'batch_size': 104, 'step_size': 2, 'gamma': 0.7651483085812975}. Best is trial 10 with value: 0.04198554373131348.[0m
Early stopping at epoch 48
[32m[I 2025-01-12 18:20:58,614][0m Trial 38 finished with value: 0.2386886477470398 and parameters: {'observation_period_num': 85, 'train_rates': 0.9648801954350652, 'learning_rate': 0.0007016644184030902, 'batch_size': 116, 'step_size': 1, 'gamma': 0.7542758671790907}. Best is trial 10 with value: 0.04198554373131348.[0m
[32m[I 2025-01-12 18:24:02,661][0m Trial 39 finished with value: 0.03539063959645155 and parameters: {'observation_period_num': 5, 'train_rates': 0.8679849073717476, 'learning_rate': 0.00031832998036440213, 'batch_size': 29, 'step_size': 5, 'gamma': 0.9367039557278387}. Best is trial 39 with value: 0.03539063959645155.[0m
[32m[I 2025-01-12 18:25:03,534][0m Trial 40 finished with value: 0.0733333514148339 and parameters: {'observation_period_num': 138, 'train_rates': 0.9136511362178596, 'learning_rate': 0.0002605691047137649, 'batch_size': 89, 'step_size': 3, 'gamma': 0.9516889072920732}. Best is trial 39 with value: 0.03539063959645155.[0m
[32m[I 2025-01-12 18:27:44,231][0m Trial 41 finished with value: 0.03558353847756828 and parameters: {'observation_period_num': 15, 'train_rates': 0.8350340246876589, 'learning_rate': 0.0003783884335580887, 'batch_size': 32, 'step_size': 7, 'gamma': 0.929447254859276}. Best is trial 39 with value: 0.03539063959645155.[0m
[32m[I 2025-01-12 18:30:04,827][0m Trial 42 finished with value: 0.03478913153322554 and parameters: {'observation_period_num': 17, 'train_rates': 0.8329177114137881, 'learning_rate': 0.00033768914031608824, 'batch_size': 37, 'step_size': 8, 'gamma': 0.9278511796376999}. Best is trial 42 with value: 0.03478913153322554.[0m
[32m[I 2025-01-12 18:30:29,831][0m Trial 43 finished with value: 0.07577783293648116 and parameters: {'observation_period_num': 5, 'train_rates': 0.8244267121574865, 'learning_rate': 7.752429133119495e-05, 'batch_size': 223, 'step_size': 9, 'gamma': 0.9284939698387957}. Best is trial 42 with value: 0.03478913153322554.[0m
[32m[I 2025-01-12 18:32:59,834][0m Trial 44 finished with value: 0.17170943888225432 and parameters: {'observation_period_num': 16, 'train_rates': 0.7829304596554159, 'learning_rate': 0.0003308467479474794, 'batch_size': 33, 'step_size': 7, 'gamma': 0.9400971126702157}. Best is trial 42 with value: 0.03478913153322554.[0m
[32m[I 2025-01-12 18:35:04,437][0m Trial 45 finished with value: 0.05452375682917508 and parameters: {'observation_period_num': 49, 'train_rates': 0.8108850017750656, 'learning_rate': 0.00019592917530212876, 'batch_size': 40, 'step_size': 8, 'gamma': 0.8945486203813644}. Best is trial 42 with value: 0.03478913153322554.[0m
[32m[I 2025-01-12 18:36:20,870][0m Trial 46 finished with value: 0.20091487506232858 and parameters: {'observation_period_num': 14, 'train_rates': 0.7527618539074439, 'learning_rate': 6.111135407287271e-05, 'batch_size': 64, 'step_size': 9, 'gamma': 0.9266033248342584}. Best is trial 42 with value: 0.03478913153322554.[0m
[32m[I 2025-01-12 18:39:08,759][0m Trial 47 finished with value: 0.19575589823490772 and parameters: {'observation_period_num': 5, 'train_rates': 0.8345484830479348, 'learning_rate': 2.234169338922596e-06, 'batch_size': 31, 'step_size': 5, 'gamma': 0.9668031616619419}. Best is trial 42 with value: 0.03478913153322554.[0m
[32m[I 2025-01-12 18:42:30,328][0m Trial 48 finished with value: 0.0425788155315738 and parameters: {'observation_period_num': 39, 'train_rates': 0.8698807824430159, 'learning_rate': 0.00012499563568138332, 'batch_size': 26, 'step_size': 13, 'gamma': 0.9205060745725526}. Best is trial 42 with value: 0.03478913153322554.[0m
[32m[I 2025-01-12 18:44:02,717][0m Trial 49 finished with value: 0.12344600675921691 and parameters: {'observation_period_num': 199, 'train_rates': 0.7933580823781082, 'learning_rate': 0.0007161619596864514, 'batch_size': 51, 'step_size': 8, 'gamma': 0.9526835707513147}. Best is trial 42 with value: 0.03478913153322554.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-12 18:44:02,727][0m A new study created in memory with name: no-name-f1924de1-7df6-4ad6-9aaf-6c6d1a92c9fb[0m
[32m[I 2025-01-12 18:44:35,845][0m Trial 0 finished with value: 1.6878604991369837 and parameters: {'observation_period_num': 24, 'train_rates': 0.6189045985072084, 'learning_rate': 3.493918309376045e-06, 'batch_size': 131, 'step_size': 7, 'gamma': 0.947003459902976}. Best is trial 0 with value: 1.6878604991369837.[0m
[32m[I 2025-01-12 18:46:00,912][0m Trial 1 finished with value: 0.16413352459438088 and parameters: {'observation_period_num': 192, 'train_rates': 0.9067732138442199, 'learning_rate': 0.00013189394947494724, 'batch_size': 62, 'step_size': 11, 'gamma': 0.9704775797856765}. Best is trial 1 with value: 0.16413352459438088.[0m
[32m[I 2025-01-12 18:46:23,976][0m Trial 2 finished with value: 0.13651100879858347 and parameters: {'observation_period_num': 219, 'train_rates': 0.7787326162864132, 'learning_rate': 0.0008701326376590899, 'batch_size': 224, 'step_size': 6, 'gamma': 0.9772411460636599}. Best is trial 2 with value: 0.13651100879858347.[0m
[32m[I 2025-01-12 18:46:43,405][0m Trial 3 finished with value: 0.6838030420816862 and parameters: {'observation_period_num': 9, 'train_rates': 0.6248173651782449, 'learning_rate': 9.28478744192654e-06, 'batch_size': 238, 'step_size': 10, 'gamma': 0.9069480124265756}. Best is trial 2 with value: 0.13651100879858347.[0m
[32m[I 2025-01-12 18:47:24,220][0m Trial 4 finished with value: 0.9699583649635315 and parameters: {'observation_period_num': 211, 'train_rates': 0.9860002767257947, 'learning_rate': 1.722185368120402e-06, 'batch_size': 139, 'step_size': 7, 'gamma': 0.7650969563013528}. Best is trial 2 with value: 0.13651100879858347.[0m
[32m[I 2025-01-12 18:47:59,249][0m Trial 5 finished with value: 1.3358854654514043 and parameters: {'observation_period_num': 98, 'train_rates': 0.6594679069833439, 'learning_rate': 1.8669359199930392e-06, 'batch_size': 131, 'step_size': 15, 'gamma': 0.8525250015383258}. Best is trial 2 with value: 0.13651100879858347.[0m
[32m[I 2025-01-12 18:48:18,490][0m Trial 6 finished with value: 0.2589251210405068 and parameters: {'observation_period_num': 194, 'train_rates': 0.6813409663994411, 'learning_rate': 0.0006020711000133522, 'batch_size': 244, 'step_size': 9, 'gamma': 0.7984118987856985}. Best is trial 2 with value: 0.13651100879858347.[0m
[32m[I 2025-01-12 18:48:44,648][0m Trial 7 finished with value: 0.27190655248474654 and parameters: {'observation_period_num': 231, 'train_rates': 0.6477255249167029, 'learning_rate': 0.00023325008826934678, 'batch_size': 173, 'step_size': 9, 'gamma': 0.9438740415985067}. Best is trial 2 with value: 0.13651100879858347.[0m
[32m[I 2025-01-12 18:49:23,982][0m Trial 8 finished with value: 1.2192336320877075 and parameters: {'observation_period_num': 172, 'train_rates': 0.9501607017270144, 'learning_rate': 2.313375564863957e-06, 'batch_size': 142, 'step_size': 4, 'gamma': 0.7803023830095278}. Best is trial 2 with value: 0.13651100879858347.[0m
[32m[I 2025-01-12 18:49:41,834][0m Trial 9 finished with value: 0.6023978864462182 and parameters: {'observation_period_num': 220, 'train_rates': 0.6176553798889255, 'learning_rate': 5.851329346743572e-05, 'batch_size': 251, 'step_size': 13, 'gamma': 0.7873444190505237}. Best is trial 2 with value: 0.13651100879858347.[0m
Early stopping at epoch 99
[32m[I 2025-01-12 18:50:09,195][0m Trial 10 finished with value: 0.19317781528815847 and parameters: {'observation_period_num': 123, 'train_rates': 0.7849246531082469, 'learning_rate': 0.0008973747880778748, 'batch_size': 194, 'step_size': 1, 'gamma': 0.8754761822163359}. Best is trial 2 with value: 0.13651100879858347.[0m
[32m[I 2025-01-12 18:51:53,296][0m Trial 11 finished with value: 0.0926521218812929 and parameters: {'observation_period_num': 160, 'train_rates': 0.8989236840031302, 'learning_rate': 0.00010880341304449227, 'batch_size': 51, 'step_size': 12, 'gamma': 0.985217194055081}. Best is trial 11 with value: 0.0926521218812929.[0m
[32m[I 2025-01-12 18:55:37,291][0m Trial 12 finished with value: 0.08446778790164666 and parameters: {'observation_period_num': 152, 'train_rates': 0.8355201916574119, 'learning_rate': 2.2638492106853916e-05, 'batch_size': 22, 'step_size': 4, 'gamma': 0.9862671153490133}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 18:59:45,872][0m Trial 13 finished with value: 0.08926636443475937 and parameters: {'observation_period_num': 150, 'train_rates': 0.8694255424299921, 'learning_rate': 1.9080696989583833e-05, 'batch_size': 20, 'step_size': 3, 'gamma': 0.983206366558423}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:00:51,955][0m Trial 14 finished with value: 0.33484805600905637 and parameters: {'observation_period_num': 79, 'train_rates': 0.8486131599594076, 'learning_rate': 1.549323151781356e-05, 'batch_size': 78, 'step_size': 2, 'gamma': 0.9189391595298705}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:03:12,217][0m Trial 15 finished with value: 0.24008165357790598 and parameters: {'observation_period_num': 135, 'train_rates': 0.8264967389387963, 'learning_rate': 2.066082033591152e-05, 'batch_size': 35, 'step_size': 4, 'gamma': 0.8445181666745754}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:07:16,401][0m Trial 16 finished with value: 0.16562683810142503 and parameters: {'observation_period_num': 67, 'train_rates': 0.8567467254070595, 'learning_rate': 8.516180297784663e-06, 'batch_size': 21, 'step_size': 4, 'gamma': 0.9433679943179489}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:08:09,032][0m Trial 17 finished with value: 0.5822878776603193 and parameters: {'observation_period_num': 141, 'train_rates': 0.7693270789583948, 'learning_rate': 3.898055063874412e-05, 'batch_size': 92, 'step_size': 2, 'gamma': 0.8968383819930876}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:08:55,983][0m Trial 18 finished with value: 0.7231282347237876 and parameters: {'observation_period_num': 110, 'train_rates': 0.7313167202123666, 'learning_rate': 5.066967923733876e-06, 'batch_size': 101, 'step_size': 5, 'gamma': 0.9894224187899727}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:12:42,974][0m Trial 19 finished with value: 0.17864997759461404 and parameters: {'observation_period_num': 161, 'train_rates': 0.9070036181422327, 'learning_rate': 3.221432590258544e-05, 'batch_size': 23, 'step_size': 3, 'gamma': 0.8197237691893712}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:14:06,266][0m Trial 20 finished with value: 0.37721413752159694 and parameters: {'observation_period_num': 249, 'train_rates': 0.7336311976137044, 'learning_rate': 6.511626720796058e-05, 'batch_size': 53, 'step_size': 1, 'gamma': 0.9586794810312084}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:16:01,784][0m Trial 21 finished with value: 0.1049879485160693 and parameters: {'observation_period_num': 157, 'train_rates': 0.8901665778364598, 'learning_rate': 0.00016170619562532156, 'batch_size': 45, 'step_size': 13, 'gamma': 0.9876729132322484}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:17:14,751][0m Trial 22 finished with value: 0.10481661478377352 and parameters: {'observation_period_num': 183, 'train_rates': 0.8216337180667018, 'learning_rate': 7.027936130174866e-05, 'batch_size': 67, 'step_size': 12, 'gamma': 0.9246327551395767}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:21:40,612][0m Trial 23 finished with value: 0.09267694093932995 and parameters: {'observation_period_num': 146, 'train_rates': 0.877050627946338, 'learning_rate': 1.5605616922491486e-05, 'batch_size': 19, 'step_size': 6, 'gamma': 0.9660601306863213}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:22:34,780][0m Trial 24 finished with value: 0.09174836009361181 and parameters: {'observation_period_num': 121, 'train_rates': 0.9446330709943728, 'learning_rate': 0.00032668596538915133, 'batch_size': 105, 'step_size': 15, 'gamma': 0.9356593681731078}. Best is trial 12 with value: 0.08446778790164666.[0m
[32m[I 2025-01-12 19:23:28,941][0m Trial 25 finished with value: 0.0830158030023118 and parameters: {'observation_period_num': 93, 'train_rates': 0.9416381250164458, 'learning_rate': 0.00022509203871917208, 'batch_size': 104, 'step_size': 15, 'gamma': 0.9310987088341376}. Best is trial 25 with value: 0.0830158030023118.[0m
[32m[I 2025-01-12 19:24:37,110][0m Trial 26 finished with value: 0.5673122107982635 and parameters: {'observation_period_num': 83, 'train_rates': 0.942187604051937, 'learning_rate': 8.141398588519994e-06, 'batch_size': 83, 'step_size': 3, 'gamma': 0.8847445721449337}. Best is trial 25 with value: 0.0830158030023118.[0m
[32m[I 2025-01-12 19:27:21,592][0m Trial 27 finished with value: 0.08041957881249173 and parameters: {'observation_period_num': 39, 'train_rates': 0.9759346697986716, 'learning_rate': 2.258410429629959e-05, 'batch_size': 35, 'step_size': 5, 'gamma': 0.9569103963647735}. Best is trial 27 with value: 0.08041957881249173.[0m
[32m[I 2025-01-12 19:28:13,133][0m Trial 28 finished with value: 0.05589282512664795 and parameters: {'observation_period_num': 54, 'train_rates': 0.9841458488319605, 'learning_rate': 0.0003777027821369679, 'batch_size': 115, 'step_size': 8, 'gamma': 0.9555845918061664}. Best is trial 28 with value: 0.05589282512664795.[0m
[32m[I 2025-01-12 19:29:04,194][0m Trial 29 finished with value: 0.09955672174692154 and parameters: {'observation_period_num': 50, 'train_rates': 0.9815348182993374, 'learning_rate': 0.0004069496422938568, 'batch_size': 118, 'step_size': 7, 'gamma': 0.9564784282964458}. Best is trial 28 with value: 0.05589282512664795.[0m
[32m[I 2025-01-12 19:29:41,384][0m Trial 30 finished with value: 0.06236732378602028 and parameters: {'observation_period_num': 36, 'train_rates': 0.9549590955373183, 'learning_rate': 0.00022730074989534116, 'batch_size': 161, 'step_size': 8, 'gamma': 0.9248541277412097}. Best is trial 28 with value: 0.05589282512664795.[0m
[32m[I 2025-01-12 19:30:18,760][0m Trial 31 finished with value: 0.07133061438798904 and parameters: {'observation_period_num': 33, 'train_rates': 0.9617395159122722, 'learning_rate': 0.0002365098193772289, 'batch_size': 160, 'step_size': 8, 'gamma': 0.9259631989993278}. Best is trial 28 with value: 0.05589282512664795.[0m
[32m[I 2025-01-12 19:30:56,511][0m Trial 32 finished with value: 0.05324114114046097 and parameters: {'observation_period_num': 37, 'train_rates': 0.9659946003406799, 'learning_rate': 0.0005018895127624887, 'batch_size': 157, 'step_size': 8, 'gamma': 0.9111275738103786}. Best is trial 32 with value: 0.05324114114046097.[0m
[32m[I 2025-01-12 19:31:34,546][0m Trial 33 finished with value: 0.054165177047252655 and parameters: {'observation_period_num': 25, 'train_rates': 0.9616951571027385, 'learning_rate': 0.00047558869172559227, 'batch_size': 163, 'step_size': 8, 'gamma': 0.9091142737525996}. Best is trial 32 with value: 0.05324114114046097.[0m
[32m[I 2025-01-12 19:32:04,575][0m Trial 34 finished with value: 0.04287449270486832 and parameters: {'observation_period_num': 5, 'train_rates': 0.9360412898561238, 'learning_rate': 0.0004689101010997695, 'batch_size': 198, 'step_size': 9, 'gamma': 0.9010361648629482}. Best is trial 34 with value: 0.04287449270486832.[0m
[32m[I 2025-01-12 19:32:34,869][0m Trial 35 finished with value: 0.03806309051901461 and parameters: {'observation_period_num': 6, 'train_rates': 0.9156081191828903, 'learning_rate': 0.0005826857444802797, 'batch_size': 207, 'step_size': 10, 'gamma': 0.9025998115600623}. Best is trial 35 with value: 0.03806309051901461.[0m
[32m[I 2025-01-12 19:33:04,978][0m Trial 36 finished with value: 0.041108859791642145 and parameters: {'observation_period_num': 7, 'train_rates': 0.9217208794207105, 'learning_rate': 0.0006124096578103108, 'batch_size': 202, 'step_size': 10, 'gamma': 0.9044838009905516}. Best is trial 35 with value: 0.03806309051901461.[0m
[32m[I 2025-01-12 19:33:33,307][0m Trial 37 finished with value: 0.03951299878021167 and parameters: {'observation_period_num': 10, 'train_rates': 0.9209586732341886, 'learning_rate': 0.000656274331862487, 'batch_size': 216, 'step_size': 10, 'gamma': 0.8533893655877148}. Best is trial 35 with value: 0.03806309051901461.[0m
[32m[I 2025-01-12 19:34:01,397][0m Trial 38 finished with value: 0.036504344142313246 and parameters: {'observation_period_num': 13, 'train_rates': 0.9195437625822369, 'learning_rate': 0.0007180433857283203, 'batch_size': 216, 'step_size': 10, 'gamma': 0.8531576485180181}. Best is trial 38 with value: 0.036504344142313246.[0m
[32m[I 2025-01-12 19:34:29,657][0m Trial 39 finished with value: 0.04867133093201768 and parameters: {'observation_period_num': 18, 'train_rates': 0.9181070178173241, 'learning_rate': 0.0007479213073300937, 'batch_size': 221, 'step_size': 10, 'gamma': 0.8551333407194371}. Best is trial 38 with value: 0.036504344142313246.[0m
[32m[I 2025-01-12 19:34:57,744][0m Trial 40 finished with value: 0.041017993638912835 and parameters: {'observation_period_num': 16, 'train_rates': 0.9235043578927317, 'learning_rate': 0.0006835011918110143, 'batch_size': 223, 'step_size': 10, 'gamma': 0.8251623604411017}. Best is trial 38 with value: 0.036504344142313246.[0m
[32m[I 2025-01-12 19:35:23,916][0m Trial 41 finished with value: 0.051682478855264946 and parameters: {'observation_period_num': 16, 'train_rates': 0.9200344827430047, 'learning_rate': 0.0009254183551242482, 'batch_size': 226, 'step_size': 10, 'gamma': 0.8304055052075975}. Best is trial 38 with value: 0.036504344142313246.[0m
[32m[I 2025-01-12 19:35:51,786][0m Trial 42 finished with value: 0.03592806159082903 and parameters: {'observation_period_num': 8, 'train_rates': 0.8890503642068391, 'learning_rate': 0.0006596441910718066, 'batch_size': 204, 'step_size': 11, 'gamma': 0.8633711677735286}. Best is trial 42 with value: 0.03592806159082903.[0m
[32m[I 2025-01-12 19:36:20,127][0m Trial 43 finished with value: 0.059611805975437165 and parameters: {'observation_period_num': 22, 'train_rates': 0.8894103955124623, 'learning_rate': 0.0006510160635583016, 'batch_size': 214, 'step_size': 11, 'gamma': 0.8604078437091948}. Best is trial 42 with value: 0.03592806159082903.[0m
[32m[I 2025-01-12 19:36:50,746][0m Trial 44 finished with value: 0.0539195178584619 and parameters: {'observation_period_num': 53, 'train_rates': 0.8751408750782709, 'learning_rate': 0.0009786266766874951, 'batch_size': 184, 'step_size': 11, 'gamma': 0.8148660850160987}. Best is trial 42 with value: 0.03592806159082903.[0m
[32m[I 2025-01-12 19:37:17,128][0m Trial 45 finished with value: 0.045456120576289366 and parameters: {'observation_period_num': 5, 'train_rates': 0.9100585257714615, 'learning_rate': 0.0003080235165000556, 'batch_size': 235, 'step_size': 11, 'gamma': 0.8398691575221715}. Best is trial 42 with value: 0.03592806159082903.[0m
[32m[I 2025-01-12 19:37:39,610][0m Trial 46 finished with value: 0.07739491572846537 and parameters: {'observation_period_num': 27, 'train_rates': 0.8035097791223149, 'learning_rate': 0.00013965138376438788, 'batch_size': 256, 'step_size': 12, 'gamma': 0.8761068424456057}. Best is trial 42 with value: 0.03592806159082903.[0m
[32m[I 2025-01-12 19:38:07,641][0m Trial 47 finished with value: 0.04966465011239052 and parameters: {'observation_period_num': 45, 'train_rates': 0.9310935055507558, 'learning_rate': 0.0006365828660317331, 'batch_size': 210, 'step_size': 9, 'gamma': 0.86513181199306}. Best is trial 42 with value: 0.03592806159082903.[0m
[32m[I 2025-01-12 19:38:37,991][0m Trial 48 finished with value: 0.06909124299883843 and parameters: {'observation_period_num': 66, 'train_rates': 0.8614695720515342, 'learning_rate': 0.0002880391718095506, 'batch_size': 181, 'step_size': 13, 'gamma': 0.8864653140104698}. Best is trial 42 with value: 0.03592806159082903.[0m
[32m[I 2025-01-12 19:39:04,465][0m Trial 49 finished with value: 0.07643133749405796 and parameters: {'observation_period_num': 16, 'train_rates': 0.8935207173034112, 'learning_rate': 9.3987469092709e-05, 'batch_size': 237, 'step_size': 9, 'gamma': 0.8040660435884232}. Best is trial 42 with value: 0.03592806159082903.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-12 19:39:04,476][0m A new study created in memory with name: no-name-aa730dbc-c5f3-44ca-8a39-04d5082ccd63[0m
[32m[I 2025-01-12 19:39:28,579][0m Trial 0 finished with value: 1.5483156442642212 and parameters: {'observation_period_num': 225, 'train_rates': 0.9248562504378175, 'learning_rate': 7.735364014369523e-06, 'batch_size': 248, 'step_size': 3, 'gamma': 0.776908829836504}. Best is trial 0 with value: 1.5483156442642212.[0m
[32m[I 2025-01-12 19:39:51,979][0m Trial 1 finished with value: 0.3892175238418939 and parameters: {'observation_period_num': 191, 'train_rates': 0.6640518342210082, 'learning_rate': 8.562717642869534e-05, 'batch_size': 189, 'step_size': 7, 'gamma': 0.8668287116523374}. Best is trial 1 with value: 0.3892175238418939.[0m
[32m[I 2025-01-12 19:40:55,236][0m Trial 2 finished with value: 0.29026782712661475 and parameters: {'observation_period_num': 242, 'train_rates': 0.6097690788622696, 'learning_rate': 0.00026296545165539216, 'batch_size': 62, 'step_size': 5, 'gamma': 0.9073928995365667}. Best is trial 2 with value: 0.29026782712661475.[0m
[32m[I 2025-01-12 19:43:31,115][0m Trial 3 finished with value: 0.14007390869428982 and parameters: {'observation_period_num': 21, 'train_rates': 0.8991280573354763, 'learning_rate': 5.317198042670843e-06, 'batch_size': 35, 'step_size': 10, 'gamma': 0.9036972776695558}. Best is trial 3 with value: 0.14007390869428982.[0m
[32m[I 2025-01-12 19:44:06,857][0m Trial 4 finished with value: 0.3720507323741913 and parameters: {'observation_period_num': 167, 'train_rates': 0.9850171956896647, 'learning_rate': 3.0061713182114804e-05, 'batch_size': 171, 'step_size': 7, 'gamma': 0.8143653879644972}. Best is trial 3 with value: 0.14007390869428982.[0m
[32m[I 2025-01-12 19:44:30,417][0m Trial 5 finished with value: 0.3144194296236788 and parameters: {'observation_period_num': 95, 'train_rates': 0.68866058847123, 'learning_rate': 0.00018058352219693573, 'batch_size': 206, 'step_size': 4, 'gamma': 0.865065308637206}. Best is trial 3 with value: 0.14007390869428982.[0m
[32m[I 2025-01-12 19:45:00,832][0m Trial 6 finished with value: 0.6129219209561583 and parameters: {'observation_period_num': 237, 'train_rates': 0.9103975518068408, 'learning_rate': 6.377850255084036e-06, 'batch_size': 188, 'step_size': 10, 'gamma': 0.7619356183918513}. Best is trial 3 with value: 0.14007390869428982.[0m
[32m[I 2025-01-12 19:45:42,771][0m Trial 7 finished with value: 0.34374604705798517 and parameters: {'observation_period_num': 63, 'train_rates': 0.7669724251394225, 'learning_rate': 3.1842149737560395e-05, 'batch_size': 119, 'step_size': 9, 'gamma': 0.944070227164995}. Best is trial 3 with value: 0.14007390869428982.[0m
[32m[I 2025-01-12 19:46:29,207][0m Trial 8 finished with value: 0.2073012404840627 and parameters: {'observation_period_num': 96, 'train_rates': 0.7568586499854015, 'learning_rate': 0.0007970663810535553, 'batch_size': 107, 'step_size': 9, 'gamma': 0.7906468255104581}. Best is trial 3 with value: 0.14007390869428982.[0m
[32m[I 2025-01-12 19:47:07,291][0m Trial 9 finished with value: 0.6642683896630704 and parameters: {'observation_period_num': 63, 'train_rates': 0.7575175894424564, 'learning_rate': 5.2954647649323605e-06, 'batch_size': 132, 'step_size': 14, 'gamma': 0.7751222020876993}. Best is trial 3 with value: 0.14007390869428982.[0m
[32m[I 2025-01-12 19:50:27,898][0m Trial 10 finished with value: 0.2077223920396396 and parameters: {'observation_period_num': 16, 'train_rates': 0.8571156712213669, 'learning_rate': 1.4212390203113174e-06, 'batch_size': 26, 'step_size': 13, 'gamma': 0.9773159443770572}. Best is trial 3 with value: 0.14007390869428982.[0m
[32m[I 2025-01-12 19:51:31,813][0m Trial 11 finished with value: 0.03145589469145117 and parameters: {'observation_period_num': 11, 'train_rates': 0.8291263908496994, 'learning_rate': 0.0006189417913660519, 'batch_size': 82, 'step_size': 11, 'gamma': 0.8273071236522137}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 19:53:06,568][0m Trial 12 finished with value: 0.8537155244577235 and parameters: {'observation_period_num': 7, 'train_rates': 0.8537206793796883, 'learning_rate': 1.1507547370146656e-06, 'batch_size': 57, 'step_size': 12, 'gamma': 0.8301167307889679}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 19:54:24,954][0m Trial 13 finished with value: 0.0728296009791002 and parameters: {'observation_period_num': 39, 'train_rates': 0.8437629698487187, 'learning_rate': 0.0009597447613224507, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9035268128636889}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 19:55:28,572][0m Trial 14 finished with value: 0.050209830853917725 and parameters: {'observation_period_num': 51, 'train_rates': 0.8268981061835329, 'learning_rate': 0.000724040550069922, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8356833241433517}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 19:56:22,893][0m Trial 15 finished with value: 0.06952920533143557 and parameters: {'observation_period_num': 124, 'train_rates': 0.8164845151466118, 'learning_rate': 0.00041928197342371384, 'batch_size': 96, 'step_size': 15, 'gamma': 0.8329522720333442}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 19:57:14,935][0m Trial 16 finished with value: 0.21916596808059272 and parameters: {'observation_period_num': 52, 'train_rates': 0.7036907588081232, 'learning_rate': 0.00010186188979704139, 'batch_size': 90, 'step_size': 12, 'gamma': 0.8116972779339422}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 19:57:50,161][0m Trial 17 finished with value: 0.07085002730362606 and parameters: {'observation_period_num': 84, 'train_rates': 0.8092644356862547, 'learning_rate': 0.00045419434837060796, 'batch_size': 152, 'step_size': 15, 'gamma': 0.849024771989703}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 19:58:57,748][0m Trial 18 finished with value: 0.30647554112152314 and parameters: {'observation_period_num': 142, 'train_rates': 0.9497029993823793, 'learning_rate': 8.622184197702376e-05, 'batch_size': 83, 'step_size': 1, 'gamma': 0.8874986311409859}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:03:46,963][0m Trial 19 finished with value: 0.05204579092852481 and parameters: {'observation_period_num': 39, 'train_rates': 0.868062682954159, 'learning_rate': 4.12193335752675e-05, 'batch_size': 18, 'step_size': 13, 'gamma': 0.8013260978514876}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:04:21,875][0m Trial 20 finished with value: 0.11125856911184832 and parameters: {'observation_period_num': 117, 'train_rates': 0.7926307325353709, 'learning_rate': 0.00046677339688048244, 'batch_size': 149, 'step_size': 7, 'gamma': 0.839855723020591}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:06:35,197][0m Trial 21 finished with value: 0.06844085089799028 and parameters: {'observation_period_num': 34, 'train_rates': 0.8792552075389054, 'learning_rate': 2.894605154778589e-05, 'batch_size': 40, 'step_size': 13, 'gamma': 0.8009532830411805}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:08:17,027][0m Trial 22 finished with value: 0.04149550677966753 and parameters: {'observation_period_num': 5, 'train_rates': 0.8222680709919783, 'learning_rate': 0.0001438770474815223, 'batch_size': 51, 'step_size': 12, 'gamma': 0.7550276097968355}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:09:25,269][0m Trial 23 finished with value: 0.04252078949528582 and parameters: {'observation_period_num': 14, 'train_rates': 0.8264976737409323, 'learning_rate': 0.0002004652852813781, 'batch_size': 77, 'step_size': 11, 'gamma': 0.7500993219434414}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:11:05,722][0m Trial 24 finished with value: 0.15823513521597934 and parameters: {'observation_period_num': 11, 'train_rates': 0.7350977130448035, 'learning_rate': 0.0002080994258176467, 'batch_size': 48, 'step_size': 11, 'gamma': 0.7524499699173074}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:11:50,054][0m Trial 25 finished with value: 0.11420128274233546 and parameters: {'observation_period_num': 76, 'train_rates': 0.7851189530051602, 'learning_rate': 0.00014664762401688598, 'batch_size': 116, 'step_size': 9, 'gamma': 0.7743803809000922}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:13:02,219][0m Trial 26 finished with value: 0.09533797240400889 and parameters: {'observation_period_num': 28, 'train_rates': 0.8299364362814281, 'learning_rate': 5.400302297730816e-05, 'batch_size': 72, 'step_size': 11, 'gamma': 0.7514418782604667}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:13:51,427][0m Trial 27 finished with value: 0.167661649018454 and parameters: {'observation_period_num': 6, 'train_rates': 0.7194165793518567, 'learning_rate': 0.0002890151353119455, 'batch_size': 99, 'step_size': 8, 'gamma': 0.7867360645520304}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:15:31,842][0m Trial 28 finished with value: 0.055160638125192736 and parameters: {'observation_period_num': 29, 'train_rates': 0.7880141227271714, 'learning_rate': 0.0001348987651705743, 'batch_size': 50, 'step_size': 14, 'gamma': 0.7665995085648112}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:15:55,699][0m Trial 29 finished with value: 0.5977425575256348 and parameters: {'observation_period_num': 208, 'train_rates': 0.935363070208893, 'learning_rate': 1.5937046232767818e-05, 'batch_size': 241, 'step_size': 10, 'gamma': 0.7845139124461797}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:17:08,286][0m Trial 30 finished with value: 0.06196391720723056 and parameters: {'observation_period_num': 57, 'train_rates': 0.890287049975903, 'learning_rate': 0.00035707170553601864, 'batch_size': 74, 'step_size': 8, 'gamma': 0.7505454152045438}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:18:12,755][0m Trial 31 finished with value: 0.06783248757033847 and parameters: {'observation_period_num': 46, 'train_rates': 0.822626674017493, 'learning_rate': 0.0006234536755352721, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8515526767911179}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:18:57,983][0m Trial 32 finished with value: 0.037328089986528666 and parameters: {'observation_period_num': 5, 'train_rates': 0.841165075196434, 'learning_rate': 0.0006463536762497887, 'batch_size': 123, 'step_size': 12, 'gamma': 0.8195441120744531}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:19:40,971][0m Trial 33 finished with value: 0.04812460619825543 and parameters: {'observation_period_num': 22, 'train_rates': 0.8444857428660583, 'learning_rate': 0.00021929451014673587, 'batch_size': 130, 'step_size': 14, 'gamma': 0.8200316463038959}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:20:31,961][0m Trial 34 finished with value: 0.03315625948976256 and parameters: {'observation_period_num': 9, 'train_rates': 0.8735478429091282, 'learning_rate': 0.0006065750729184162, 'batch_size': 108, 'step_size': 11, 'gamma': 0.8766742121306799}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:21:12,503][0m Trial 35 finished with value: 0.04049301357008517 and parameters: {'observation_period_num': 5, 'train_rates': 0.9187277766461939, 'learning_rate': 0.0006377007046946328, 'batch_size': 147, 'step_size': 10, 'gamma': 0.8808980428632515}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:21:52,616][0m Trial 36 finished with value: 0.08613349497318268 and parameters: {'observation_period_num': 163, 'train_rates': 0.9684389983268386, 'learning_rate': 0.0006268400989423669, 'batch_size': 146, 'step_size': 10, 'gamma': 0.8801986937846475}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:22:27,001][0m Trial 37 finished with value: 0.04560440894292325 and parameters: {'observation_period_num': 24, 'train_rates': 0.9164087804286798, 'learning_rate': 0.0003075988411777254, 'batch_size': 178, 'step_size': 9, 'gamma': 0.9237867986355766}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:22:53,466][0m Trial 38 finished with value: 0.21281482227335002 and parameters: {'observation_period_num': 72, 'train_rates': 0.6021098966242218, 'learning_rate': 0.000988609621241134, 'batch_size': 167, 'step_size': 5, 'gamma': 0.8683285012260517}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:23:15,076][0m Trial 39 finished with value: 0.1865097632251897 and parameters: {'observation_period_num': 36, 'train_rates': 0.6470648905262589, 'learning_rate': 0.0004701906627621307, 'batch_size': 210, 'step_size': 6, 'gamma': 0.8561010951534593}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:24:03,456][0m Trial 40 finished with value: 0.08921257202403948 and parameters: {'observation_period_num': 104, 'train_rates': 0.9045282628102114, 'learning_rate': 0.000556886960486502, 'batch_size': 113, 'step_size': 10, 'gamma': 0.8851005936901448}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:24:47,952][0m Trial 41 finished with value: 0.03927760352232958 and parameters: {'observation_period_num': 6, 'train_rates': 0.8824405879519072, 'learning_rate': 0.00031638388865272893, 'batch_size': 124, 'step_size': 11, 'gamma': 0.8927278361215469}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:25:30,462][0m Trial 42 finished with value: 0.06558750318998302 and parameters: {'observation_period_num': 21, 'train_rates': 0.8802225957853141, 'learning_rate': 0.00035298294575476937, 'batch_size': 131, 'step_size': 11, 'gamma': 0.9209956882426473}. Best is trial 11 with value: 0.03145589469145117.[0m
[32m[I 2025-01-12 20:26:26,332][0m Trial 43 finished with value: 0.02810585050292276 and parameters: {'observation_period_num': 5, 'train_rates': 0.932002288255847, 'learning_rate': 0.0009627007353084302, 'batch_size': 104, 'step_size': 10, 'gamma': 0.8964767543554167}. Best is trial 43 with value: 0.02810585050292276.[0m
[32m[I 2025-01-12 20:27:21,190][0m Trial 44 finished with value: 0.04574111618355594 and parameters: {'observation_period_num': 19, 'train_rates': 0.9401045242735782, 'learning_rate': 0.0009473216757066764, 'batch_size': 103, 'step_size': 13, 'gamma': 0.9043601395120167}. Best is trial 43 with value: 0.02810585050292276.[0m
[32m[I 2025-01-12 20:28:09,135][0m Trial 45 finished with value: 0.05654696002602577 and parameters: {'observation_period_num': 45, 'train_rates': 0.9733860027743061, 'learning_rate': 0.0007739009696566533, 'batch_size': 124, 'step_size': 9, 'gamma': 0.9184324161591032}. Best is trial 43 with value: 0.02810585050292276.[0m
[32m[I 2025-01-12 20:29:00,049][0m Trial 46 finished with value: 0.03996238077706006 and parameters: {'observation_period_num': 18, 'train_rates': 0.8685560053688299, 'learning_rate': 0.0002787738001720902, 'batch_size': 109, 'step_size': 11, 'gamma': 0.9400511767782292}. Best is trial 43 with value: 0.02810585050292276.[0m
[32m[I 2025-01-12 20:29:40,963][0m Trial 47 finished with value: 0.06370781222804667 and parameters: {'observation_period_num': 64, 'train_rates': 0.8919063657844807, 'learning_rate': 0.0005450205897913893, 'batch_size': 139, 'step_size': 8, 'gamma': 0.8689035593002736}. Best is trial 43 with value: 0.02810585050292276.[0m
[32m[I 2025-01-12 20:30:40,010][0m Trial 48 finished with value: 0.05698516890050122 and parameters: {'observation_period_num': 32, 'train_rates': 0.856996950938517, 'learning_rate': 0.0007589193894274134, 'batch_size': 92, 'step_size': 10, 'gamma': 0.8934526363358345}. Best is trial 43 with value: 0.02810585050292276.[0m
[32m[I 2025-01-12 20:31:29,067][0m Trial 49 finished with value: 0.34637259818889476 and parameters: {'observation_period_num': 15, 'train_rates': 0.9543740846469433, 'learning_rate': 3.999910522141566e-06, 'batch_size': 121, 'step_size': 12, 'gamma': 0.89556935238313}. Best is trial 43 with value: 0.02810585050292276.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-12 20:31:29,077][0m A new study created in memory with name: no-name-04b0fb82-114f-4227-9736-2d535f4aa8bd[0m
[32m[I 2025-01-12 20:31:59,050][0m Trial 0 finished with value: 0.26637086272239685 and parameters: {'observation_period_num': 122, 'train_rates': 0.9390414255228838, 'learning_rate': 3.748562064783347e-05, 'batch_size': 203, 'step_size': 8, 'gamma': 0.8502049982883502}. Best is trial 0 with value: 0.26637086272239685.[0m
[32m[I 2025-01-12 20:33:47,178][0m Trial 1 finished with value: 0.3588189350234138 and parameters: {'observation_period_num': 59, 'train_rates': 0.96924343441429, 'learning_rate': 2.0254367965304656e-05, 'batch_size': 52, 'step_size': 2, 'gamma': 0.8574414136519266}. Best is trial 0 with value: 0.26637086272239685.[0m
[32m[I 2025-01-12 20:34:38,657][0m Trial 2 finished with value: 0.4511209142594914 and parameters: {'observation_period_num': 133, 'train_rates': 0.6394099710046649, 'learning_rate': 4.519422767752419e-05, 'batch_size': 84, 'step_size': 5, 'gamma': 0.8767203347982873}. Best is trial 0 with value: 0.26637086272239685.[0m
[32m[I 2025-01-12 20:35:28,916][0m Trial 3 finished with value: 0.39537092973770743 and parameters: {'observation_period_num': 214, 'train_rates': 0.7968917608634678, 'learning_rate': 9.481405985183176e-06, 'batch_size': 97, 'step_size': 4, 'gamma': 0.9891421429883304}. Best is trial 0 with value: 0.26637086272239685.[0m
[32m[I 2025-01-12 20:39:17,474][0m Trial 4 finished with value: 0.620946846341574 and parameters: {'observation_period_num': 212, 'train_rates': 0.8985748646091127, 'learning_rate': 1.1294247410726187e-06, 'batch_size': 22, 'step_size': 1, 'gamma': 0.9629899015316838}. Best is trial 0 with value: 0.26637086272239685.[0m
[32m[I 2025-01-12 20:41:24,497][0m Trial 5 finished with value: 0.5567101061029887 and parameters: {'observation_period_num': 252, 'train_rates': 0.7894183957872425, 'learning_rate': 7.3779968564329236e-06, 'batch_size': 36, 'step_size': 2, 'gamma': 0.8712924872123724}. Best is trial 0 with value: 0.26637086272239685.[0m
[32m[I 2025-01-12 20:46:16,070][0m Trial 6 finished with value: 0.07310912439883765 and parameters: {'observation_period_num': 204, 'train_rates': 0.8928909483928309, 'learning_rate': 0.00020350346732863923, 'batch_size': 17, 'step_size': 1, 'gamma': 0.910619324755283}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:46:46,024][0m Trial 7 finished with value: 0.14925579135720643 and parameters: {'observation_period_num': 8, 'train_rates': 0.6758322767495284, 'learning_rate': 0.00048669003801844225, 'batch_size': 160, 'step_size': 2, 'gamma': 0.94525538074983}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:47:10,324][0m Trial 8 finished with value: 0.117629902680283 and parameters: {'observation_period_num': 119, 'train_rates': 0.8540656621628542, 'learning_rate': 0.00010255580817069344, 'batch_size': 223, 'step_size': 14, 'gamma': 0.8977568930582509}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:47:42,593][0m Trial 9 finished with value: 0.2845772697708823 and parameters: {'observation_period_num': 236, 'train_rates': 0.8988014057940884, 'learning_rate': 1.4434356437673747e-05, 'batch_size': 169, 'step_size': 9, 'gamma': 0.9294565549962306}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:48:03,568][0m Trial 10 finished with value: 0.26941966204866163 and parameters: {'observation_period_num': 166, 'train_rates': 0.735216209013758, 'learning_rate': 0.0007982828238299305, 'batch_size': 255, 'step_size': 15, 'gamma': 0.7746725972643749}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:48:25,923][0m Trial 11 finished with value: 0.12811581715225026 and parameters: {'observation_period_num': 99, 'train_rates': 0.8542748212530431, 'learning_rate': 0.0002000455904639076, 'batch_size': 251, 'step_size': 15, 'gamma': 0.8883633904406267}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:49:09,417][0m Trial 12 finished with value: 0.08761163115501404 and parameters: {'observation_period_num': 167, 'train_rates': 0.8297770616316574, 'learning_rate': 0.00013587492490717347, 'batch_size': 117, 'step_size': 12, 'gamma': 0.9110693252469758}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:49:52,947][0m Trial 13 finished with value: 0.11180978500120894 and parameters: {'observation_period_num': 168, 'train_rates': 0.8016419555606203, 'learning_rate': 0.00015362084053777557, 'batch_size': 114, 'step_size': 11, 'gamma': 0.8188079131575086}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:50:59,055][0m Trial 14 finished with value: 0.2605047719667776 and parameters: {'observation_period_num': 177, 'train_rates': 0.7369646094311334, 'learning_rate': 0.0003199217269867292, 'batch_size': 70, 'step_size': 12, 'gamma': 0.917101607414474}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:51:38,020][0m Trial 15 finished with value: 0.17929028701312436 and parameters: {'observation_period_num': 194, 'train_rates': 0.8529286940616324, 'learning_rate': 8.367923013211479e-05, 'batch_size': 134, 'step_size': 7, 'gamma': 0.8267763240841213}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:52:14,200][0m Trial 16 finished with value: 0.08308156579732895 and parameters: {'observation_period_num': 152, 'train_rates': 0.9860726927754626, 'learning_rate': 0.0009029823672377969, 'batch_size': 168, 'step_size': 12, 'gamma': 0.9182588713101054}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:52:47,959][0m Trial 17 finished with value: 0.08742198348045349 and parameters: {'observation_period_num': 66, 'train_rates': 0.9776377833852878, 'learning_rate': 0.0009498842159824718, 'batch_size': 180, 'step_size': 10, 'gamma': 0.9647519956465996}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:53:28,132][0m Trial 18 finished with value: 0.08282172382630483 and parameters: {'observation_period_num': 208, 'train_rates': 0.9281909026414065, 'learning_rate': 0.0003749477691629382, 'batch_size': 139, 'step_size': 6, 'gamma': 0.9408831956057266}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:54:06,121][0m Trial 19 finished with value: 0.11747920329188123 and parameters: {'observation_period_num': 214, 'train_rates': 0.9114718009798137, 'learning_rate': 0.0003458836933087382, 'batch_size': 139, 'step_size': 4, 'gamma': 0.9875703350296821}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:55:35,982][0m Trial 20 finished with value: 0.4561844962594755 and parameters: {'observation_period_num': 194, 'train_rates': 0.9191881009377527, 'learning_rate': 3.4399217879210097e-06, 'batch_size': 58, 'step_size': 7, 'gamma': 0.9499728416515688}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:56:14,076][0m Trial 21 finished with value: 0.08271671831607819 and parameters: {'observation_period_num': 155, 'train_rates': 0.9518181216307694, 'learning_rate': 0.0005016475795421507, 'batch_size': 150, 'step_size': 6, 'gamma': 0.9293245973159088}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:56:51,583][0m Trial 22 finished with value: 0.10476788878440857 and parameters: {'observation_period_num': 195, 'train_rates': 0.9473491354564387, 'learning_rate': 0.0002775938266850005, 'batch_size': 148, 'step_size': 5, 'gamma': 0.9353100718588262}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:57:20,120][0m Trial 23 finished with value: 0.22988950156565732 and parameters: {'observation_period_num': 234, 'train_rates': 0.8832383490784566, 'learning_rate': 6.86601258072583e-05, 'batch_size': 193, 'step_size': 6, 'gamma': 0.9027249728213531}. Best is trial 6 with value: 0.07310912439883765.[0m
[32m[I 2025-01-12 20:58:08,080][0m Trial 24 finished with value: 0.07063837597767512 and parameters: {'observation_period_num': 144, 'train_rates': 0.9383744885187932, 'learning_rate': 0.0005044977976113891, 'batch_size': 116, 'step_size': 4, 'gamma': 0.9643410792965724}. Best is trial 24 with value: 0.07063837597767512.[0m
[32m[I 2025-01-12 20:59:04,212][0m Trial 25 finished with value: 0.07905551816447307 and parameters: {'observation_period_num': 145, 'train_rates': 0.9583012217365062, 'learning_rate': 0.0005114079433827547, 'batch_size': 101, 'step_size': 3, 'gamma': 0.9680359961787984}. Best is trial 24 with value: 0.07063837597767512.[0m
[32m[I 2025-01-12 20:59:59,259][0m Trial 26 finished with value: 0.08332350892318206 and parameters: {'observation_period_num': 100, 'train_rates': 0.8763450721431679, 'learning_rate': 0.00019961382443248868, 'batch_size': 97, 'step_size': 3, 'gamma': 0.9711248667891995}. Best is trial 24 with value: 0.07063837597767512.[0m
[32m[I 2025-01-12 21:05:14,847][0m Trial 27 finished with value: 0.07502671552159018 and parameters: {'observation_period_num': 145, 'train_rates': 0.9582307686707937, 'learning_rate': 0.0005637874693814712, 'batch_size': 17, 'step_size': 1, 'gamma': 0.972272961892162}. Best is trial 24 with value: 0.07063837597767512.[0m
[32m[I 2025-01-12 21:08:49,885][0m Trial 28 finished with value: 0.09573672050420119 and parameters: {'observation_period_num': 83, 'train_rates': 0.9317974921148701, 'learning_rate': 5.688867112739178e-05, 'batch_size': 25, 'step_size': 1, 'gamma': 0.953389281284354}. Best is trial 24 with value: 0.07063837597767512.[0m
Early stopping at epoch 41
[32m[I 2025-01-12 21:09:42,265][0m Trial 29 finished with value: 0.5428989177400415 and parameters: {'observation_period_num': 121, 'train_rates': 0.9420659915652881, 'learning_rate': 2.9106907932123617e-05, 'batch_size': 45, 'step_size': 1, 'gamma': 0.7523700716455302}. Best is trial 24 with value: 0.07063837597767512.[0m
[32m[I 2025-01-12 21:10:54,912][0m Trial 30 finished with value: 0.0765746815928391 and parameters: {'observation_period_num': 135, 'train_rates': 0.8262160609402223, 'learning_rate': 0.0006048965858392291, 'batch_size': 70, 'step_size': 3, 'gamma': 0.9784847290869241}. Best is trial 24 with value: 0.07063837597767512.[0m
[32m[I 2025-01-12 21:12:05,280][0m Trial 31 finished with value: 0.13322530551390213 and parameters: {'observation_period_num': 136, 'train_rates': 0.8283406329025088, 'learning_rate': 0.0006295297978704447, 'batch_size': 72, 'step_size': 3, 'gamma': 0.9810030740454303}. Best is trial 24 with value: 0.07063837597767512.[0m
[32m[I 2025-01-12 21:14:17,606][0m Trial 32 finished with value: 0.08288461212878642 and parameters: {'observation_period_num': 112, 'train_rates': 0.8786560881897969, 'learning_rate': 0.00019962436466392587, 'batch_size': 39, 'step_size': 2, 'gamma': 0.9715912692829408}. Best is trial 24 with value: 0.07063837597767512.[0m
[32m[I 2025-01-12 21:18:16,672][0m Trial 33 finished with value: 0.07660953485103975 and parameters: {'observation_period_num': 134, 'train_rates': 0.9706908949050734, 'learning_rate': 0.0006444747806336682, 'batch_size': 23, 'step_size': 4, 'gamma': 0.8524260621051664}. Best is trial 24 with value: 0.07063837597767512.[0m
[32m[I 2025-01-12 21:19:37,359][0m Trial 34 finished with value: 0.2768426913677192 and parameters: {'observation_period_num': 182, 'train_rates': 0.7634444122743888, 'learning_rate': 0.00024794609888602516, 'batch_size': 58, 'step_size': 1, 'gamma': 0.9535140235897708}. Best is trial 24 with value: 0.07063837597767512.[0m
[32m[I 2025-01-12 21:20:43,777][0m Trial 35 finished with value: 0.0545411845422568 and parameters: {'observation_period_num': 39, 'train_rates': 0.8315510790338012, 'learning_rate': 0.00012725159713100268, 'batch_size': 79, 'step_size': 3, 'gamma': 0.9896631577781698}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:21:22,197][0m Trial 36 finished with value: 0.1584200447380746 and parameters: {'observation_period_num': 7, 'train_rates': 0.6021774140553622, 'learning_rate': 0.00012261153652017218, 'batch_size': 116, 'step_size': 2, 'gamma': 0.9857697176039655}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:26:35,077][0m Trial 37 finished with value: 0.05709635021408539 and parameters: {'observation_period_num': 37, 'train_rates': 0.9031463781280031, 'learning_rate': 5.175325447499416e-05, 'batch_size': 17, 'step_size': 4, 'gamma': 0.885286426486732}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:27:41,781][0m Trial 38 finished with value: 0.1657256172461943 and parameters: {'observation_period_num': 33, 'train_rates': 0.8948253678360316, 'learning_rate': 3.398179333343786e-05, 'batch_size': 82, 'step_size': 5, 'gamma': 0.8356899303320091}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:30:01,219][0m Trial 39 finished with value: 0.06915506008928851 and parameters: {'observation_period_num': 36, 'train_rates': 0.8627564210403339, 'learning_rate': 5.596732629762679e-05, 'batch_size': 38, 'step_size': 4, 'gamma': 0.863510400255817}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:31:45,937][0m Trial 40 finished with value: 0.19002347809297068 and parameters: {'observation_period_num': 34, 'train_rates': 0.8614186286913651, 'learning_rate': 2.0331778271474748e-05, 'batch_size': 51, 'step_size': 4, 'gamma': 0.8646000379274399}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:34:51,445][0m Trial 41 finished with value: 0.07096245991042435 and parameters: {'observation_period_num': 39, 'train_rates': 0.8112254742225549, 'learning_rate': 5.0211979588720874e-05, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8858764235515562}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:37:11,163][0m Trial 42 finished with value: 0.08197093256095798 and parameters: {'observation_period_num': 38, 'train_rates': 0.8071817964504239, 'learning_rate': 4.614983189518459e-05, 'batch_size': 36, 'step_size': 4, 'gamma': 0.8849556525832183}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:39:57,292][0m Trial 43 finished with value: 0.3251360564172723 and parameters: {'observation_period_num': 54, 'train_rates': 0.7741291837154916, 'learning_rate': 2.391050289104669e-05, 'batch_size': 29, 'step_size': 5, 'gamma': 0.8403999627824067}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:41:01,726][0m Trial 44 finished with value: 0.26063383815141744 and parameters: {'observation_period_num': 22, 'train_rates': 0.8393353751147025, 'learning_rate': 1.2869867445229553e-05, 'batch_size': 83, 'step_size': 8, 'gamma': 0.7967871617874964}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:43:06,753][0m Trial 45 finished with value: 0.07767495955340564 and parameters: {'observation_period_num': 56, 'train_rates': 0.9118356615035357, 'learning_rate': 4.6082272073412496e-05, 'batch_size': 44, 'step_size': 7, 'gamma': 0.8708060869818163}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:44:29,038][0m Trial 46 finished with value: 0.27905909689055713 and parameters: {'observation_period_num': 76, 'train_rates': 0.7784731088686339, 'learning_rate': 8.863883535195665e-05, 'batch_size': 59, 'step_size': 3, 'gamma': 0.8844498211155107}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:47:03,632][0m Trial 47 finished with value: 0.06800209287856077 and parameters: {'observation_period_num': 21, 'train_rates': 0.8109271759855146, 'learning_rate': 6.309830850811634e-05, 'batch_size': 33, 'step_size': 5, 'gamma': 0.8000166443784696}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:47:58,448][0m Trial 48 finished with value: 0.25886507999961356 and parameters: {'observation_period_num': 19, 'train_rates': 0.7477725844353451, 'learning_rate': 6.945768214519953e-05, 'batch_size': 89, 'step_size': 5, 'gamma': 0.8088235227357752}. Best is trial 35 with value: 0.0545411845422568.[0m
[32m[I 2025-01-12 21:50:42,173][0m Trial 49 finished with value: 0.06236137330378057 and parameters: {'observation_period_num': 48, 'train_rates': 0.8670023521139358, 'learning_rate': 0.00010742370628081559, 'batch_size': 32, 'step_size': 6, 'gamma': 0.7785537475268532}. Best is trial 35 with value: 0.0545411845422568.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 11, 'train_rates': 0.8591185742794396, 'learning_rate': 0.00030058802861210185, 'batch_size': 86, 'step_size': 12, 'gamma': 0.8393571266042812}
Epoch 1/300, trend Loss: 0.5784 | 0.4012
Epoch 2/300, trend Loss: 0.2806 | 0.2061
Epoch 3/300, trend Loss: 0.1975 | 0.1543
Epoch 4/300, trend Loss: 0.1672 | 0.1387
Epoch 5/300, trend Loss: 0.1529 | 0.1230
Epoch 6/300, trend Loss: 0.1435 | 0.1124
Epoch 7/300, trend Loss: 0.1377 | 0.1100
Epoch 8/300, trend Loss: 0.1372 | 0.0977
Epoch 9/300, trend Loss: 0.1461 | 0.0886
Epoch 10/300, trend Loss: 0.1561 | 0.0943
Epoch 11/300, trend Loss: 0.1515 | 0.0791
Epoch 12/300, trend Loss: 0.1404 | 0.0772
Epoch 13/300, trend Loss: 0.1328 | 0.0791
Epoch 14/300, trend Loss: 0.1235 | 0.0768
Epoch 15/300, trend Loss: 0.1177 | 0.0699
Epoch 16/300, trend Loss: 0.1160 | 0.0662
Epoch 17/300, trend Loss: 0.1161 | 0.0648
Epoch 18/300, trend Loss: 0.1172 | 0.0646
Epoch 19/300, trend Loss: 0.1183 | 0.0664
Epoch 20/300, trend Loss: 0.1184 | 0.0639
Epoch 21/300, trend Loss: 0.1171 | 0.0619
Epoch 22/300, trend Loss: 0.1144 | 0.0605
Epoch 23/300, trend Loss: 0.1110 | 0.0594
Epoch 24/300, trend Loss: 0.1086 | 0.0585
Epoch 25/300, trend Loss: 0.1077 | 0.0594
Epoch 26/300, trend Loss: 0.1073 | 0.0588
Epoch 27/300, trend Loss: 0.1066 | 0.0578
Epoch 28/300, trend Loss: 0.1057 | 0.0569
Epoch 29/300, trend Loss: 0.1052 | 0.0564
Epoch 30/300, trend Loss: 0.1057 | 0.0566
Epoch 31/300, trend Loss: 0.1071 | 0.0616
Epoch 32/300, trend Loss: 0.1075 | 0.0616
Epoch 33/300, trend Loss: 0.1052 | 0.0577
Epoch 34/300, trend Loss: 0.1033 | 0.0553
Epoch 35/300, trend Loss: 0.1030 | 0.0548
Epoch 36/300, trend Loss: 0.1044 | 0.0561
Epoch 37/300, trend Loss: 0.1056 | 0.0603
Epoch 38/300, trend Loss: 0.1043 | 0.0548
Epoch 39/300, trend Loss: 0.1016 | 0.0533
Epoch 40/300, trend Loss: 0.1004 | 0.0537
Epoch 41/300, trend Loss: 0.1008 | 0.0539
Epoch 42/300, trend Loss: 0.1014 | 0.0533
Epoch 43/300, trend Loss: 0.1016 | 0.0521
Epoch 44/300, trend Loss: 0.1005 | 0.0510
Epoch 45/300, trend Loss: 0.0986 | 0.0506
Epoch 46/300, trend Loss: 0.0981 | 0.0506
Epoch 47/300, trend Loss: 0.0982 | 0.0506
Epoch 48/300, trend Loss: 0.0978 | 0.0504
Epoch 49/300, trend Loss: 0.0972 | 0.0503
Epoch 50/300, trend Loss: 0.0966 | 0.0500
Epoch 51/300, trend Loss: 0.0962 | 0.0496
Epoch 52/300, trend Loss: 0.0960 | 0.0494
Epoch 53/300, trend Loss: 0.0958 | 0.0492
Epoch 54/300, trend Loss: 0.0956 | 0.0490
Epoch 55/300, trend Loss: 0.0954 | 0.0487
Epoch 56/300, trend Loss: 0.0951 | 0.0486
Epoch 57/300, trend Loss: 0.0949 | 0.0484
Epoch 58/300, trend Loss: 0.0946 | 0.0483
Epoch 59/300, trend Loss: 0.0944 | 0.0481
Epoch 60/300, trend Loss: 0.0942 | 0.0480
Epoch 61/300, trend Loss: 0.0940 | 0.0478
Epoch 62/300, trend Loss: 0.0938 | 0.0477
Epoch 63/300, trend Loss: 0.0936 | 0.0476
Epoch 64/300, trend Loss: 0.0935 | 0.0474
Epoch 65/300, trend Loss: 0.0933 | 0.0473
Epoch 66/300, trend Loss: 0.0932 | 0.0472
Epoch 67/300, trend Loss: 0.0930 | 0.0470
Epoch 68/300, trend Loss: 0.0928 | 0.0469
Epoch 69/300, trend Loss: 0.0927 | 0.0468
Epoch 70/300, trend Loss: 0.0925 | 0.0467
Epoch 71/300, trend Loss: 0.0924 | 0.0466
Epoch 72/300, trend Loss: 0.0922 | 0.0464
Epoch 73/300, trend Loss: 0.0921 | 0.0463
Epoch 74/300, trend Loss: 0.0919 | 0.0462
Epoch 75/300, trend Loss: 0.0918 | 0.0461
Epoch 76/300, trend Loss: 0.0917 | 0.0460
Epoch 77/300, trend Loss: 0.0915 | 0.0459
Epoch 78/300, trend Loss: 0.0914 | 0.0458
Epoch 79/300, trend Loss: 0.0913 | 0.0457
Epoch 80/300, trend Loss: 0.0912 | 0.0456
Epoch 81/300, trend Loss: 0.0910 | 0.0455
Epoch 82/300, trend Loss: 0.0909 | 0.0454
Epoch 83/300, trend Loss: 0.0908 | 0.0453
Epoch 84/300, trend Loss: 0.0907 | 0.0452
Epoch 85/300, trend Loss: 0.0905 | 0.0451
Epoch 86/300, trend Loss: 0.0904 | 0.0450
Epoch 87/300, trend Loss: 0.0903 | 0.0449
Epoch 88/300, trend Loss: 0.0902 | 0.0448
Epoch 89/300, trend Loss: 0.0901 | 0.0447
Epoch 90/300, trend Loss: 0.0900 | 0.0446
Epoch 91/300, trend Loss: 0.0899 | 0.0445
Epoch 92/300, trend Loss: 0.0898 | 0.0445
Epoch 93/300, trend Loss: 0.0897 | 0.0444
Epoch 94/300, trend Loss: 0.0896 | 0.0443
Epoch 95/300, trend Loss: 0.0895 | 0.0442
Epoch 96/300, trend Loss: 0.0894 | 0.0441
Epoch 97/300, trend Loss: 0.0893 | 0.0441
Epoch 98/300, trend Loss: 0.0892 | 0.0440
Epoch 99/300, trend Loss: 0.0892 | 0.0439
Epoch 100/300, trend Loss: 0.0891 | 0.0438
Epoch 101/300, trend Loss: 0.0890 | 0.0438
Epoch 102/300, trend Loss: 0.0889 | 0.0437
Epoch 103/300, trend Loss: 0.0888 | 0.0436
Epoch 104/300, trend Loss: 0.0887 | 0.0435
Epoch 105/300, trend Loss: 0.0887 | 0.0435
Epoch 106/300, trend Loss: 0.0886 | 0.0434
Epoch 107/300, trend Loss: 0.0885 | 0.0433
Epoch 108/300, trend Loss: 0.0884 | 0.0433
Epoch 109/300, trend Loss: 0.0884 | 0.0432
Epoch 110/300, trend Loss: 0.0883 | 0.0432
Epoch 111/300, trend Loss: 0.0882 | 0.0431
Epoch 112/300, trend Loss: 0.0882 | 0.0430
Epoch 113/300, trend Loss: 0.0881 | 0.0430
Epoch 114/300, trend Loss: 0.0880 | 0.0429
Epoch 115/300, trend Loss: 0.0880 | 0.0429
Epoch 116/300, trend Loss: 0.0879 | 0.0428
Epoch 117/300, trend Loss: 0.0878 | 0.0428
Epoch 118/300, trend Loss: 0.0878 | 0.0427
Epoch 119/300, trend Loss: 0.0877 | 0.0427
Epoch 120/300, trend Loss: 0.0877 | 0.0426
Epoch 121/300, trend Loss: 0.0876 | 0.0425
Epoch 122/300, trend Loss: 0.0876 | 0.0425
Epoch 123/300, trend Loss: 0.0875 | 0.0425
Epoch 124/300, trend Loss: 0.0875 | 0.0424
Epoch 125/300, trend Loss: 0.0874 | 0.0424
Epoch 126/300, trend Loss: 0.0874 | 0.0423
Epoch 127/300, trend Loss: 0.0873 | 0.0423
Epoch 128/300, trend Loss: 0.0873 | 0.0422
Epoch 129/300, trend Loss: 0.0872 | 0.0422
Epoch 130/300, trend Loss: 0.0872 | 0.0422
Epoch 131/300, trend Loss: 0.0871 | 0.0421
Epoch 132/300, trend Loss: 0.0871 | 0.0421
Epoch 133/300, trend Loss: 0.0871 | 0.0420
Epoch 134/300, trend Loss: 0.0870 | 0.0420
Epoch 135/300, trend Loss: 0.0870 | 0.0420
Epoch 136/300, trend Loss: 0.0869 | 0.0419
Epoch 137/300, trend Loss: 0.0869 | 0.0419
Epoch 138/300, trend Loss: 0.0869 | 0.0419
Epoch 139/300, trend Loss: 0.0868 | 0.0418
Epoch 140/300, trend Loss: 0.0868 | 0.0418
Epoch 141/300, trend Loss: 0.0868 | 0.0418
Epoch 142/300, trend Loss: 0.0867 | 0.0417
Epoch 143/300, trend Loss: 0.0867 | 0.0417
Epoch 144/300, trend Loss: 0.0867 | 0.0417
Epoch 145/300, trend Loss: 0.0866 | 0.0416
Epoch 146/300, trend Loss: 0.0866 | 0.0416
Epoch 147/300, trend Loss: 0.0866 | 0.0416
Epoch 148/300, trend Loss: 0.0866 | 0.0416
Epoch 149/300, trend Loss: 0.0865 | 0.0415
Epoch 150/300, trend Loss: 0.0865 | 0.0415
Epoch 151/300, trend Loss: 0.0865 | 0.0415
Epoch 152/300, trend Loss: 0.0864 | 0.0415
Epoch 153/300, trend Loss: 0.0864 | 0.0414
Epoch 154/300, trend Loss: 0.0864 | 0.0414
Epoch 155/300, trend Loss: 0.0864 | 0.0414
Epoch 156/300, trend Loss: 0.0864 | 0.0414
Epoch 157/300, trend Loss: 0.0863 | 0.0414
Epoch 158/300, trend Loss: 0.0863 | 0.0413
Epoch 159/300, trend Loss: 0.0863 | 0.0413
Epoch 160/300, trend Loss: 0.0863 | 0.0413
Epoch 161/300, trend Loss: 0.0862 | 0.0413
Epoch 162/300, trend Loss: 0.0862 | 0.0413
Epoch 163/300, trend Loss: 0.0862 | 0.0412
Epoch 164/300, trend Loss: 0.0862 | 0.0412
Epoch 165/300, trend Loss: 0.0862 | 0.0412
Epoch 166/300, trend Loss: 0.0862 | 0.0412
Epoch 167/300, trend Loss: 0.0861 | 0.0412
Epoch 168/300, trend Loss: 0.0861 | 0.0412
Epoch 169/300, trend Loss: 0.0861 | 0.0412
Epoch 170/300, trend Loss: 0.0861 | 0.0411
Epoch 171/300, trend Loss: 0.0861 | 0.0411
Epoch 172/300, trend Loss: 0.0861 | 0.0411
Epoch 173/300, trend Loss: 0.0860 | 0.0411
Epoch 174/300, trend Loss: 0.0860 | 0.0411
Epoch 175/300, trend Loss: 0.0860 | 0.0411
Epoch 176/300, trend Loss: 0.0860 | 0.0411
Epoch 177/300, trend Loss: 0.0860 | 0.0410
Epoch 178/300, trend Loss: 0.0860 | 0.0410
Epoch 179/300, trend Loss: 0.0860 | 0.0410
Epoch 180/300, trend Loss: 0.0860 | 0.0410
Epoch 181/300, trend Loss: 0.0859 | 0.0410
Epoch 182/300, trend Loss: 0.0859 | 0.0410
Epoch 183/300, trend Loss: 0.0859 | 0.0410
Epoch 184/300, trend Loss: 0.0859 | 0.0410
Epoch 185/300, trend Loss: 0.0859 | 0.0410
Epoch 186/300, trend Loss: 0.0859 | 0.0409
Epoch 187/300, trend Loss: 0.0859 | 0.0409
Epoch 188/300, trend Loss: 0.0859 | 0.0409
Epoch 189/300, trend Loss: 0.0859 | 0.0409
Epoch 190/300, trend Loss: 0.0858 | 0.0409
Epoch 191/300, trend Loss: 0.0858 | 0.0409
Epoch 192/300, trend Loss: 0.0858 | 0.0409
Epoch 193/300, trend Loss: 0.0858 | 0.0409
Epoch 194/300, trend Loss: 0.0858 | 0.0409
Epoch 195/300, trend Loss: 0.0858 | 0.0409
Epoch 196/300, trend Loss: 0.0858 | 0.0409
Epoch 197/300, trend Loss: 0.0858 | 0.0409
Epoch 198/300, trend Loss: 0.0858 | 0.0409
Epoch 199/300, trend Loss: 0.0858 | 0.0408
Epoch 200/300, trend Loss: 0.0858 | 0.0408
Epoch 201/300, trend Loss: 0.0858 | 0.0408
Epoch 202/300, trend Loss: 0.0858 | 0.0408
Epoch 203/300, trend Loss: 0.0857 | 0.0408
Epoch 204/300, trend Loss: 0.0857 | 0.0408
Epoch 205/300, trend Loss: 0.0857 | 0.0408
Epoch 206/300, trend Loss: 0.0857 | 0.0408
Epoch 207/300, trend Loss: 0.0857 | 0.0408
Epoch 208/300, trend Loss: 0.0857 | 0.0408
Epoch 209/300, trend Loss: 0.0857 | 0.0408
Epoch 210/300, trend Loss: 0.0857 | 0.0408
Epoch 211/300, trend Loss: 0.0857 | 0.0408
Epoch 212/300, trend Loss: 0.0857 | 0.0408
Epoch 213/300, trend Loss: 0.0857 | 0.0408
Epoch 214/300, trend Loss: 0.0857 | 0.0408
Epoch 215/300, trend Loss: 0.0857 | 0.0408
Epoch 216/300, trend Loss: 0.0857 | 0.0408
Epoch 217/300, trend Loss: 0.0857 | 0.0408
Epoch 218/300, trend Loss: 0.0857 | 0.0408
Epoch 219/300, trend Loss: 0.0857 | 0.0407
Epoch 220/300, trend Loss: 0.0857 | 0.0407
Epoch 221/300, trend Loss: 0.0857 | 0.0407
Epoch 222/300, trend Loss: 0.0857 | 0.0407
Epoch 223/300, trend Loss: 0.0856 | 0.0407
Epoch 224/300, trend Loss: 0.0856 | 0.0407
Epoch 225/300, trend Loss: 0.0856 | 0.0407
Epoch 226/300, trend Loss: 0.0856 | 0.0407
Epoch 227/300, trend Loss: 0.0856 | 0.0407
Epoch 228/300, trend Loss: 0.0856 | 0.0407
Epoch 229/300, trend Loss: 0.0856 | 0.0407
Epoch 230/300, trend Loss: 0.0856 | 0.0407
Epoch 231/300, trend Loss: 0.0856 | 0.0407
Epoch 232/300, trend Loss: 0.0856 | 0.0407
Epoch 233/300, trend Loss: 0.0856 | 0.0407
Epoch 234/300, trend Loss: 0.0856 | 0.0407
Epoch 235/300, trend Loss: 0.0856 | 0.0407
Epoch 236/300, trend Loss: 0.0856 | 0.0407
Epoch 237/300, trend Loss: 0.0856 | 0.0407
Epoch 238/300, trend Loss: 0.0856 | 0.0407
Epoch 239/300, trend Loss: 0.0856 | 0.0407
Epoch 240/300, trend Loss: 0.0856 | 0.0407
Epoch 241/300, trend Loss: 0.0856 | 0.0407
Epoch 242/300, trend Loss: 0.0856 | 0.0407
Epoch 243/300, trend Loss: 0.0856 | 0.0407
Epoch 244/300, trend Loss: 0.0856 | 0.0407
Epoch 245/300, trend Loss: 0.0856 | 0.0407
Epoch 246/300, trend Loss: 0.0856 | 0.0407
Epoch 247/300, trend Loss: 0.0856 | 0.0407
Epoch 248/300, trend Loss: 0.0856 | 0.0407
Epoch 249/300, trend Loss: 0.0856 | 0.0407
Epoch 250/300, trend Loss: 0.0856 | 0.0407
Epoch 251/300, trend Loss: 0.0856 | 0.0407
Epoch 252/300, trend Loss: 0.0856 | 0.0407
Epoch 253/300, trend Loss: 0.0856 | 0.0407
Epoch 254/300, trend Loss: 0.0856 | 0.0407
Epoch 255/300, trend Loss: 0.0856 | 0.0407
Epoch 256/300, trend Loss: 0.0856 | 0.0407
Epoch 257/300, trend Loss: 0.0856 | 0.0407
Epoch 258/300, trend Loss: 0.0856 | 0.0407
Epoch 259/300, trend Loss: 0.0856 | 0.0407
Epoch 260/300, trend Loss: 0.0856 | 0.0407
Epoch 261/300, trend Loss: 0.0856 | 0.0407
Epoch 262/300, trend Loss: 0.0856 | 0.0407
Epoch 263/300, trend Loss: 0.0856 | 0.0407
Epoch 264/300, trend Loss: 0.0856 | 0.0407
Epoch 265/300, trend Loss: 0.0856 | 0.0407
Epoch 266/300, trend Loss: 0.0856 | 0.0407
Epoch 267/300, trend Loss: 0.0856 | 0.0407
Epoch 268/300, trend Loss: 0.0856 | 0.0407
Epoch 269/300, trend Loss: 0.0856 | 0.0406
Epoch 270/300, trend Loss: 0.0856 | 0.0406
Epoch 271/300, trend Loss: 0.0856 | 0.0406
Epoch 272/300, trend Loss: 0.0856 | 0.0406
Epoch 273/300, trend Loss: 0.0856 | 0.0406
Epoch 274/300, trend Loss: 0.0856 | 0.0406
Epoch 275/300, trend Loss: 0.0856 | 0.0406
Epoch 276/300, trend Loss: 0.0856 | 0.0406
Epoch 277/300, trend Loss: 0.0856 | 0.0406
Epoch 278/300, trend Loss: 0.0855 | 0.0406
Epoch 279/300, trend Loss: 0.0855 | 0.0406
Epoch 280/300, trend Loss: 0.0855 | 0.0406
Epoch 281/300, trend Loss: 0.0855 | 0.0406
Epoch 282/300, trend Loss: 0.0855 | 0.0406
Epoch 283/300, trend Loss: 0.0855 | 0.0406
Epoch 284/300, trend Loss: 0.0855 | 0.0406
Epoch 285/300, trend Loss: 0.0855 | 0.0406
Epoch 286/300, trend Loss: 0.0855 | 0.0406
Epoch 287/300, trend Loss: 0.0855 | 0.0406
Epoch 288/300, trend Loss: 0.0855 | 0.0406
Epoch 289/300, trend Loss: 0.0855 | 0.0406
Epoch 290/300, trend Loss: 0.0855 | 0.0406
Epoch 291/300, trend Loss: 0.0855 | 0.0406
Epoch 292/300, trend Loss: 0.0855 | 0.0406
Epoch 293/300, trend Loss: 0.0855 | 0.0406
Epoch 294/300, trend Loss: 0.0855 | 0.0406
Epoch 295/300, trend Loss: 0.0855 | 0.0406
Epoch 296/300, trend Loss: 0.0855 | 0.0406
Epoch 297/300, trend Loss: 0.0855 | 0.0406
Epoch 298/300, trend Loss: 0.0855 | 0.0406
Epoch 299/300, trend Loss: 0.0855 | 0.0406
Epoch 300/300, trend Loss: 0.0855 | 0.0406
Training seasonal_0 component with params: {'observation_period_num': 6, 'train_rates': 0.9856239085242493, 'learning_rate': 0.0006192760326421162, 'batch_size': 26, 'step_size': 14, 'gamma': 0.7926101349551267}
Epoch 1/300, seasonal_0 Loss: 0.8602 | 0.0957
Epoch 2/300, seasonal_0 Loss: 0.1807 | 0.0983
Epoch 3/300, seasonal_0 Loss: 0.1866 | 0.1126
Epoch 4/300, seasonal_0 Loss: 0.1794 | 0.0948
Epoch 5/300, seasonal_0 Loss: 0.1835 | 0.1060
Epoch 6/300, seasonal_0 Loss: 0.1580 | 0.0722
Epoch 7/300, seasonal_0 Loss: 0.1282 | 0.0935
Epoch 8/300, seasonal_0 Loss: 0.1258 | 0.0582
Epoch 9/300, seasonal_0 Loss: 0.1081 | 0.0707
Epoch 10/300, seasonal_0 Loss: 0.1086 | 0.0597
Epoch 11/300, seasonal_0 Loss: 0.1037 | 0.0615
Epoch 12/300, seasonal_0 Loss: 0.1031 | 0.0797
Epoch 13/300, seasonal_0 Loss: 0.1288 | 0.0709
Epoch 14/300, seasonal_0 Loss: 0.1199 | 0.0708
Epoch 15/300, seasonal_0 Loss: 0.1057 | 0.0598
Epoch 16/300, seasonal_0 Loss: 0.1005 | 0.0668
Epoch 17/300, seasonal_0 Loss: 0.0914 | 0.0611
Epoch 18/300, seasonal_0 Loss: 0.0865 | 0.0571
Epoch 19/300, seasonal_0 Loss: 0.0780 | 0.0505
Epoch 20/300, seasonal_0 Loss: 0.0815 | 0.0535
Epoch 21/300, seasonal_0 Loss: 0.0794 | 0.0523
Epoch 22/300, seasonal_0 Loss: 0.0802 | 0.0549
Epoch 23/300, seasonal_0 Loss: 0.0798 | 0.0567
Epoch 24/300, seasonal_0 Loss: 0.0841 | 0.0600
Epoch 25/300, seasonal_0 Loss: 0.0778 | 0.0576
Epoch 26/300, seasonal_0 Loss: 0.0742 | 0.0636
Epoch 27/300, seasonal_0 Loss: 0.0717 | 0.0554
Epoch 28/300, seasonal_0 Loss: 0.0704 | 0.0558
Epoch 29/300, seasonal_0 Loss: 0.0775 | 0.0535
Epoch 30/300, seasonal_0 Loss: 0.0677 | 0.0505
Epoch 31/300, seasonal_0 Loss: 0.0700 | 0.0548
Epoch 32/300, seasonal_0 Loss: 0.0659 | 0.0552
Epoch 33/300, seasonal_0 Loss: 0.0682 | 0.0653
Epoch 34/300, seasonal_0 Loss: 0.0650 | 0.0445
Epoch 35/300, seasonal_0 Loss: 0.0616 | 0.0465
Epoch 36/300, seasonal_0 Loss: 0.0622 | 0.0463
Epoch 37/300, seasonal_0 Loss: 0.0612 | 0.0476
Epoch 38/300, seasonal_0 Loss: 0.0576 | 0.0413
Epoch 39/300, seasonal_0 Loss: 0.0593 | 0.0433
Epoch 40/300, seasonal_0 Loss: 0.0570 | 0.0426
Epoch 41/300, seasonal_0 Loss: 0.0596 | 0.0503
Epoch 42/300, seasonal_0 Loss: 0.0593 | 0.0442
Epoch 43/300, seasonal_0 Loss: 0.0649 | 0.0607
Epoch 44/300, seasonal_0 Loss: 0.0644 | 0.0492
Epoch 45/300, seasonal_0 Loss: 0.0673 | 0.0549
Epoch 46/300, seasonal_0 Loss: 0.0662 | 0.0612
Epoch 47/300, seasonal_0 Loss: 0.0622 | 0.0474
Epoch 48/300, seasonal_0 Loss: 0.0588 | 0.0583
Epoch 49/300, seasonal_0 Loss: 0.0578 | 0.0509
Epoch 50/300, seasonal_0 Loss: 0.0615 | 0.0457
Epoch 51/300, seasonal_0 Loss: 0.0543 | 0.0471
Epoch 52/300, seasonal_0 Loss: 0.0490 | 0.0494
Epoch 53/300, seasonal_0 Loss: 0.0472 | 0.0522
Epoch 54/300, seasonal_0 Loss: 0.0491 | 0.0454
Epoch 55/300, seasonal_0 Loss: 0.0467 | 0.0456
Epoch 56/300, seasonal_0 Loss: 0.0494 | 0.0443
Epoch 57/300, seasonal_0 Loss: 0.0449 | 0.0429
Epoch 58/300, seasonal_0 Loss: 0.0437 | 0.0432
Epoch 59/300, seasonal_0 Loss: 0.0468 | 0.0457
Epoch 60/300, seasonal_0 Loss: 0.0445 | 0.0472
Epoch 61/300, seasonal_0 Loss: 0.0441 | 0.0447
Epoch 62/300, seasonal_0 Loss: 0.0513 | 0.0450
Epoch 63/300, seasonal_0 Loss: 0.0458 | 0.0484
Epoch 64/300, seasonal_0 Loss: 0.0415 | 0.0432
Epoch 65/300, seasonal_0 Loss: 0.0396 | 0.0442
Epoch 66/300, seasonal_0 Loss: 0.0388 | 0.0440
Epoch 67/300, seasonal_0 Loss: 0.0372 | 0.0456
Epoch 68/300, seasonal_0 Loss: 0.0364 | 0.0432
Epoch 69/300, seasonal_0 Loss: 0.0356 | 0.0458
Epoch 70/300, seasonal_0 Loss: 0.0356 | 0.0475
Epoch 71/300, seasonal_0 Loss: 0.0372 | 0.0536
Epoch 72/300, seasonal_0 Loss: 0.0378 | 0.0458
Epoch 73/300, seasonal_0 Loss: 0.0367 | 0.0484
Epoch 74/300, seasonal_0 Loss: 0.0353 | 0.0456
Epoch 75/300, seasonal_0 Loss: 0.0330 | 0.0444
Epoch 76/300, seasonal_0 Loss: 0.0348 | 0.0469
Epoch 77/300, seasonal_0 Loss: 0.0350 | 0.0441
Epoch 78/300, seasonal_0 Loss: 0.0361 | 0.0433
Epoch 79/300, seasonal_0 Loss: 0.0335 | 0.0422
Epoch 80/300, seasonal_0 Loss: 0.0315 | 0.0436
Epoch 81/300, seasonal_0 Loss: 0.0305 | 0.0440
Epoch 82/300, seasonal_0 Loss: 0.0292 | 0.0430
Epoch 83/300, seasonal_0 Loss: 0.0284 | 0.0434
Epoch 84/300, seasonal_0 Loss: 0.0278 | 0.0426
Epoch 85/300, seasonal_0 Loss: 0.0278 | 0.0435
Epoch 86/300, seasonal_0 Loss: 0.0274 | 0.0455
Epoch 87/300, seasonal_0 Loss: 0.0270 | 0.0482
Epoch 88/300, seasonal_0 Loss: 0.0277 | 0.0532
Epoch 89/300, seasonal_0 Loss: 0.0281 | 0.0554
Epoch 90/300, seasonal_0 Loss: 0.0274 | 0.0552
Epoch 91/300, seasonal_0 Loss: 0.0277 | 0.0523
Epoch 92/300, seasonal_0 Loss: 0.0291 | 0.0446
Epoch 93/300, seasonal_0 Loss: 0.0296 | 0.0446
Epoch 94/300, seasonal_0 Loss: 0.0304 | 0.0510
Epoch 95/300, seasonal_0 Loss: 0.0277 | 0.0446
Epoch 96/300, seasonal_0 Loss: 0.0268 | 0.0469
Epoch 97/300, seasonal_0 Loss: 0.0262 | 0.0469
Epoch 98/300, seasonal_0 Loss: 0.0257 | 0.0487
Epoch 99/300, seasonal_0 Loss: 0.0257 | 0.0458
Epoch 100/300, seasonal_0 Loss: 0.0251 | 0.0445
Epoch 101/300, seasonal_0 Loss: 0.0244 | 0.0453
Epoch 102/300, seasonal_0 Loss: 0.0242 | 0.0479
Epoch 103/300, seasonal_0 Loss: 0.0242 | 0.0478
Epoch 104/300, seasonal_0 Loss: 0.0239 | 0.0500
Epoch 105/300, seasonal_0 Loss: 0.0239 | 0.0499
Epoch 106/300, seasonal_0 Loss: 0.0240 | 0.0569
Epoch 107/300, seasonal_0 Loss: 0.0241 | 0.0569
Epoch 108/300, seasonal_0 Loss: 0.0241 | 0.0551
Epoch 109/300, seasonal_0 Loss: 0.0240 | 0.0524
Epoch 110/300, seasonal_0 Loss: 0.0238 | 0.0494
Epoch 111/300, seasonal_0 Loss: 0.0236 | 0.0457
Epoch 112/300, seasonal_0 Loss: 0.0234 | 0.0420
Epoch 113/300, seasonal_0 Loss: 0.0235 | 0.0401
Epoch 114/300, seasonal_0 Loss: 0.0233 | 0.0402
Epoch 115/300, seasonal_0 Loss: 0.0232 | 0.0411
Epoch 116/300, seasonal_0 Loss: 0.0231 | 0.0413
Epoch 117/300, seasonal_0 Loss: 0.0230 | 0.0416
Epoch 118/300, seasonal_0 Loss: 0.0229 | 0.0417
Epoch 119/300, seasonal_0 Loss: 0.0228 | 0.0418
Epoch 120/300, seasonal_0 Loss: 0.0229 | 0.0417
Epoch 121/300, seasonal_0 Loss: 0.0228 | 0.0422
Epoch 122/300, seasonal_0 Loss: 0.0227 | 0.0418
Epoch 123/300, seasonal_0 Loss: 0.0225 | 0.0417
Epoch 124/300, seasonal_0 Loss: 0.0224 | 0.0415
Epoch 125/300, seasonal_0 Loss: 0.0222 | 0.0418
Epoch 126/300, seasonal_0 Loss: 0.0222 | 0.0419
Epoch 127/300, seasonal_0 Loss: 0.0223 | 0.0428
Epoch 128/300, seasonal_0 Loss: 0.0222 | 0.0429
Epoch 129/300, seasonal_0 Loss: 0.0221 | 0.0434
Epoch 130/300, seasonal_0 Loss: 0.0220 | 0.0435
Epoch 131/300, seasonal_0 Loss: 0.0218 | 0.0438
Epoch 132/300, seasonal_0 Loss: 0.0218 | 0.0437
Epoch 133/300, seasonal_0 Loss: 0.0217 | 0.0438
Epoch 134/300, seasonal_0 Loss: 0.0217 | 0.0434
Epoch 135/300, seasonal_0 Loss: 0.0217 | 0.0433
Epoch 136/300, seasonal_0 Loss: 0.0216 | 0.0433
Epoch 137/300, seasonal_0 Loss: 0.0215 | 0.0434
Epoch 138/300, seasonal_0 Loss: 0.0214 | 0.0435
Epoch 139/300, seasonal_0 Loss: 0.0214 | 0.0436
Epoch 140/300, seasonal_0 Loss: 0.0213 | 0.0437
Epoch 141/300, seasonal_0 Loss: 0.0214 | 0.0435
Epoch 142/300, seasonal_0 Loss: 0.0213 | 0.0429
Epoch 143/300, seasonal_0 Loss: 0.0213 | 0.0424
Epoch 144/300, seasonal_0 Loss: 0.0212 | 0.0422
Epoch 145/300, seasonal_0 Loss: 0.0212 | 0.0418
Epoch 146/300, seasonal_0 Loss: 0.0211 | 0.0418
Epoch 147/300, seasonal_0 Loss: 0.0211 | 0.0416
Epoch 148/300, seasonal_0 Loss: 0.0212 | 0.0414
Epoch 149/300, seasonal_0 Loss: 0.0211 | 0.0413
Epoch 150/300, seasonal_0 Loss: 0.0210 | 0.0413
Epoch 151/300, seasonal_0 Loss: 0.0210 | 0.0414
Epoch 152/300, seasonal_0 Loss: 0.0209 | 0.0414
Epoch 153/300, seasonal_0 Loss: 0.0209 | 0.0414
Epoch 154/300, seasonal_0 Loss: 0.0209 | 0.0414
Epoch 155/300, seasonal_0 Loss: 0.0209 | 0.0413
Epoch 156/300, seasonal_0 Loss: 0.0208 | 0.0415
Epoch 157/300, seasonal_0 Loss: 0.0208 | 0.0415
Epoch 158/300, seasonal_0 Loss: 0.0208 | 0.0418
Epoch 159/300, seasonal_0 Loss: 0.0208 | 0.0417
Epoch 160/300, seasonal_0 Loss: 0.0208 | 0.0420
Epoch 161/300, seasonal_0 Loss: 0.0207 | 0.0420
Epoch 162/300, seasonal_0 Loss: 0.0207 | 0.0422
Epoch 163/300, seasonal_0 Loss: 0.0207 | 0.0422
Epoch 164/300, seasonal_0 Loss: 0.0207 | 0.0424
Epoch 165/300, seasonal_0 Loss: 0.0208 | 0.0423
Epoch 166/300, seasonal_0 Loss: 0.0208 | 0.0424
Epoch 167/300, seasonal_0 Loss: 0.0207 | 0.0425
Epoch 168/300, seasonal_0 Loss: 0.0207 | 0.0425
Epoch 169/300, seasonal_0 Loss: 0.0207 | 0.0426
Epoch 170/300, seasonal_0 Loss: 0.0207 | 0.0427
Epoch 171/300, seasonal_0 Loss: 0.0206 | 0.0428
Epoch 172/300, seasonal_0 Loss: 0.0207 | 0.0429
Epoch 173/300, seasonal_0 Loss: 0.0207 | 0.0429
Epoch 174/300, seasonal_0 Loss: 0.0207 | 0.0430
Epoch 175/300, seasonal_0 Loss: 0.0206 | 0.0430
Epoch 176/300, seasonal_0 Loss: 0.0207 | 0.0433
Epoch 177/300, seasonal_0 Loss: 0.0207 | 0.0431
Epoch 178/300, seasonal_0 Loss: 0.0207 | 0.0430
Epoch 179/300, seasonal_0 Loss: 0.0206 | 0.0428
Epoch 180/300, seasonal_0 Loss: 0.0206 | 0.0427
Epoch 181/300, seasonal_0 Loss: 0.0206 | 0.0427
Epoch 182/300, seasonal_0 Loss: 0.0205 | 0.0426
Epoch 183/300, seasonal_0 Loss: 0.0205 | 0.0423
Epoch 184/300, seasonal_0 Loss: 0.0205 | 0.0424
Epoch 185/300, seasonal_0 Loss: 0.0204 | 0.0424
Epoch 186/300, seasonal_0 Loss: 0.0204 | 0.0424
Epoch 187/300, seasonal_0 Loss: 0.0204 | 0.0424
Epoch 188/300, seasonal_0 Loss: 0.0203 | 0.0425
Epoch 189/300, seasonal_0 Loss: 0.0203 | 0.0425
Epoch 190/300, seasonal_0 Loss: 0.0203 | 0.0425
Epoch 191/300, seasonal_0 Loss: 0.0203 | 0.0425
Epoch 192/300, seasonal_0 Loss: 0.0203 | 0.0425
Epoch 193/300, seasonal_0 Loss: 0.0203 | 0.0426
Epoch 194/300, seasonal_0 Loss: 0.0203 | 0.0426
Epoch 195/300, seasonal_0 Loss: 0.0203 | 0.0426
Epoch 196/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 197/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 198/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 199/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 200/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 201/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 202/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 203/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 204/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 205/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 206/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 207/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 208/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 209/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 210/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 211/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 212/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 213/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 214/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 215/300, seasonal_0 Loss: 0.0202 | 0.0426
Epoch 216/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 217/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 218/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 219/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 220/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 221/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 222/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 223/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 224/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 225/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 226/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 227/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 228/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 229/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 230/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 231/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 232/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 233/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 234/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 235/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 236/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 237/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 238/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 239/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 240/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 241/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 242/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 243/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 244/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 245/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 246/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 247/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 248/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 249/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 250/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 251/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 252/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 253/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 254/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 255/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 256/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 257/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 258/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 259/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 260/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 261/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 262/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 263/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 264/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 265/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 266/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 267/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 268/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 269/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 270/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 271/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 272/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 273/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 274/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 275/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 276/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 277/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 278/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 279/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 280/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 281/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 282/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 283/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 284/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 285/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 286/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 287/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 288/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 289/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 290/300, seasonal_0 Loss: 0.0201 | 0.0426
Epoch 291/300, seasonal_0 Loss: 0.0200 | 0.0426
Epoch 292/300, seasonal_0 Loss: 0.0200 | 0.0426
Epoch 293/300, seasonal_0 Loss: 0.0200 | 0.0426
Epoch 294/300, seasonal_0 Loss: 0.0200 | 0.0426
Epoch 295/300, seasonal_0 Loss: 0.0200 | 0.0426
Epoch 296/300, seasonal_0 Loss: 0.0200 | 0.0426
Epoch 297/300, seasonal_0 Loss: 0.0200 | 0.0426
Epoch 298/300, seasonal_0 Loss: 0.0200 | 0.0426
Epoch 299/300, seasonal_0 Loss: 0.0200 | 0.0426
Epoch 300/300, seasonal_0 Loss: 0.0200 | 0.0426
Training seasonal_1 component with params: {'observation_period_num': 17, 'train_rates': 0.8329177114137881, 'learning_rate': 0.00033768914031608824, 'batch_size': 37, 'step_size': 8, 'gamma': 0.9278511796376999}
Epoch 1/300, seasonal_1 Loss: 0.5525 | 0.1347
Epoch 2/300, seasonal_1 Loss: 0.1848 | 0.1216
Epoch 3/300, seasonal_1 Loss: 0.1675 | 0.1621
Epoch 4/300, seasonal_1 Loss: 0.1872 | 0.1314
Epoch 5/300, seasonal_1 Loss: 0.1825 | 0.1209
Epoch 6/300, seasonal_1 Loss: 0.1637 | 0.2280
Epoch 7/300, seasonal_1 Loss: 0.1601 | 0.1097
Epoch 8/300, seasonal_1 Loss: 0.1551 | 0.1296
Epoch 9/300, seasonal_1 Loss: 0.1416 | 0.1085
Epoch 10/300, seasonal_1 Loss: 0.1175 | 0.1066
Epoch 11/300, seasonal_1 Loss: 0.1031 | 0.0653
Epoch 12/300, seasonal_1 Loss: 0.1052 | 0.0724
Epoch 13/300, seasonal_1 Loss: 0.1050 | 0.1266
Epoch 14/300, seasonal_1 Loss: 0.1157 | 0.0876
Epoch 15/300, seasonal_1 Loss: 0.1154 | 0.0912
Epoch 16/300, seasonal_1 Loss: 0.0966 | 0.1135
Epoch 17/300, seasonal_1 Loss: 0.1091 | 0.1341
Epoch 18/300, seasonal_1 Loss: 0.1087 | 0.1020
Epoch 19/300, seasonal_1 Loss: 0.1109 | 0.0989
Epoch 20/300, seasonal_1 Loss: 0.1131 | 0.1031
Epoch 21/300, seasonal_1 Loss: 0.1057 | 0.0819
Epoch 22/300, seasonal_1 Loss: 0.1111 | 0.0921
Epoch 23/300, seasonal_1 Loss: 0.1092 | 0.0780
Epoch 24/300, seasonal_1 Loss: 0.1055 | 0.0830
Epoch 25/300, seasonal_1 Loss: 0.1018 | 0.1242
Epoch 26/300, seasonal_1 Loss: 0.0963 | 0.0891
Epoch 27/300, seasonal_1 Loss: 0.0815 | 0.0915
Epoch 28/300, seasonal_1 Loss: 0.0744 | 0.0854
Epoch 29/300, seasonal_1 Loss: 0.0718 | 0.0858
Epoch 30/300, seasonal_1 Loss: 0.0695 | 0.0756
Epoch 31/300, seasonal_1 Loss: 0.0692 | 0.0723
Epoch 32/300, seasonal_1 Loss: 0.0721 | 0.0929
Epoch 33/300, seasonal_1 Loss: 0.0687 | 0.0633
Epoch 34/300, seasonal_1 Loss: 0.0633 | 0.0948
Epoch 35/300, seasonal_1 Loss: 0.0657 | 0.0452
Epoch 36/300, seasonal_1 Loss: 0.0646 | 0.0643
Epoch 37/300, seasonal_1 Loss: 0.0701 | 0.0593
Epoch 38/300, seasonal_1 Loss: 0.0714 | 0.0632
Epoch 39/300, seasonal_1 Loss: 0.0625 | 0.0709
Epoch 40/300, seasonal_1 Loss: 0.0599 | 0.0611
Epoch 41/300, seasonal_1 Loss: 0.0547 | 0.0720
Epoch 42/300, seasonal_1 Loss: 0.0640 | 0.0569
Epoch 43/300, seasonal_1 Loss: 0.0620 | 0.0667
Epoch 44/300, seasonal_1 Loss: 0.0614 | 0.0852
Epoch 45/300, seasonal_1 Loss: 0.0617 | 0.0882
Epoch 46/300, seasonal_1 Loss: 0.0651 | 0.0623
Epoch 47/300, seasonal_1 Loss: 0.0586 | 0.0538
Epoch 48/300, seasonal_1 Loss: 0.0531 | 0.0711
Epoch 49/300, seasonal_1 Loss: 0.0582 | 0.0620
Epoch 50/300, seasonal_1 Loss: 0.0632 | 0.1398
Epoch 51/300, seasonal_1 Loss: 0.0770 | 0.1359
Epoch 52/300, seasonal_1 Loss: 0.0862 | 0.0662
Epoch 53/300, seasonal_1 Loss: 0.0673 | 0.0661
Epoch 54/300, seasonal_1 Loss: 0.0585 | 0.0438
Epoch 55/300, seasonal_1 Loss: 0.0564 | 0.0473
Epoch 56/300, seasonal_1 Loss: 0.0619 | 0.0681
Epoch 57/300, seasonal_1 Loss: 0.0573 | 0.0582
Epoch 58/300, seasonal_1 Loss: 0.0532 | 0.0424
Epoch 59/300, seasonal_1 Loss: 0.0486 | 0.0432
Epoch 60/300, seasonal_1 Loss: 0.0506 | 0.0391
Epoch 61/300, seasonal_1 Loss: 0.0449 | 0.0441
Epoch 62/300, seasonal_1 Loss: 0.0521 | 0.0589
Epoch 63/300, seasonal_1 Loss: 0.0541 | 0.0437
Epoch 64/300, seasonal_1 Loss: 0.0447 | 0.0412
Epoch 65/300, seasonal_1 Loss: 0.0507 | 0.0476
Epoch 66/300, seasonal_1 Loss: 0.0554 | 0.0420
Epoch 67/300, seasonal_1 Loss: 0.0462 | 0.0525
Epoch 68/300, seasonal_1 Loss: 0.0473 | 0.0708
Epoch 69/300, seasonal_1 Loss: 0.0510 | 0.0570
Epoch 70/300, seasonal_1 Loss: 0.0435 | 0.0645
Epoch 71/300, seasonal_1 Loss: 0.0470 | 0.0512
Epoch 72/300, seasonal_1 Loss: 0.0408 | 0.0521
Epoch 73/300, seasonal_1 Loss: 0.0418 | 0.0488
Epoch 74/300, seasonal_1 Loss: 0.0367 | 0.0531
Epoch 75/300, seasonal_1 Loss: 0.0358 | 0.0567
Epoch 76/300, seasonal_1 Loss: 0.0354 | 0.0615
Epoch 77/300, seasonal_1 Loss: 0.0357 | 0.0619
Epoch 78/300, seasonal_1 Loss: 0.0349 | 0.0601
Epoch 79/300, seasonal_1 Loss: 0.0345 | 0.0568
Epoch 80/300, seasonal_1 Loss: 0.0337 | 0.0515
Epoch 81/300, seasonal_1 Loss: 0.0335 | 0.0661
Epoch 82/300, seasonal_1 Loss: 0.0337 | 0.0658
Epoch 83/300, seasonal_1 Loss: 0.0329 | 0.0569
Epoch 84/300, seasonal_1 Loss: 0.0335 | 0.0451
Epoch 85/300, seasonal_1 Loss: 0.0345 | 0.0477
Epoch 86/300, seasonal_1 Loss: 0.0355 | 0.0525
Epoch 87/300, seasonal_1 Loss: 0.0371 | 0.0602
Epoch 88/300, seasonal_1 Loss: 0.0340 | 0.0568
Epoch 89/300, seasonal_1 Loss: 0.0340 | 0.0512
Epoch 90/300, seasonal_1 Loss: 0.0313 | 0.0492
Epoch 91/300, seasonal_1 Loss: 0.0303 | 0.0505
Epoch 92/300, seasonal_1 Loss: 0.0304 | 0.0544
Epoch 93/300, seasonal_1 Loss: 0.0300 | 0.0573
Epoch 94/300, seasonal_1 Loss: 0.0296 | 0.0543
Epoch 95/300, seasonal_1 Loss: 0.0294 | 0.0493
Epoch 96/300, seasonal_1 Loss: 0.0293 | 0.0496
Epoch 97/300, seasonal_1 Loss: 0.0290 | 0.0552
Epoch 98/300, seasonal_1 Loss: 0.0288 | 0.0584
Epoch 99/300, seasonal_1 Loss: 0.0290 | 0.0473
Epoch 100/300, seasonal_1 Loss: 0.0297 | 0.0462
Epoch 101/300, seasonal_1 Loss: 0.0305 | 0.0531
Epoch 102/300, seasonal_1 Loss: 0.0306 | 0.0535
Epoch 103/300, seasonal_1 Loss: 0.0300 | 0.0498
Epoch 104/300, seasonal_1 Loss: 0.0291 | 0.0509
Epoch 105/300, seasonal_1 Loss: 0.0284 | 0.0582
Epoch 106/300, seasonal_1 Loss: 0.0291 | 0.0646
Epoch 107/300, seasonal_1 Loss: 0.0284 | 0.0620
Epoch 108/300, seasonal_1 Loss: 0.0283 | 0.0620
Epoch 109/300, seasonal_1 Loss: 0.0286 | 0.0633
Epoch 110/300, seasonal_1 Loss: 0.0287 | 0.0637
Epoch 111/300, seasonal_1 Loss: 0.0286 | 0.0616
Epoch 112/300, seasonal_1 Loss: 0.0285 | 0.0621
Epoch 113/300, seasonal_1 Loss: 0.0292 | 0.0500
Epoch 114/300, seasonal_1 Loss: 0.0289 | 0.0516
Epoch 115/300, seasonal_1 Loss: 0.0290 | 0.0543
Epoch 116/300, seasonal_1 Loss: 0.0283 | 0.0515
Epoch 117/300, seasonal_1 Loss: 0.0285 | 0.0532
Epoch 118/300, seasonal_1 Loss: 0.0280 | 0.0561
Epoch 119/300, seasonal_1 Loss: 0.0275 | 0.0567
Epoch 120/300, seasonal_1 Loss: 0.0277 | 0.0517
Epoch 121/300, seasonal_1 Loss: 0.0274 | 0.0496
Epoch 122/300, seasonal_1 Loss: 0.0273 | 0.0537
Epoch 123/300, seasonal_1 Loss: 0.0268 | 0.0495
Epoch 124/300, seasonal_1 Loss: 0.0264 | 0.0439
Epoch 125/300, seasonal_1 Loss: 0.0264 | 0.0449
Epoch 126/300, seasonal_1 Loss: 0.0264 | 0.0420
Epoch 127/300, seasonal_1 Loss: 0.0270 | 0.0444
Epoch 128/300, seasonal_1 Loss: 0.0277 | 0.0514
Epoch 129/300, seasonal_1 Loss: 0.0273 | 0.0726
Epoch 130/300, seasonal_1 Loss: 0.0277 | 0.0629
Epoch 131/300, seasonal_1 Loss: 0.0267 | 0.0482
Epoch 132/300, seasonal_1 Loss: 0.0267 | 0.0478
Epoch 133/300, seasonal_1 Loss: 0.0269 | 0.0501
Epoch 134/300, seasonal_1 Loss: 0.0267 | 0.0560
Epoch 135/300, seasonal_1 Loss: 0.0262 | 0.0526
Epoch 136/300, seasonal_1 Loss: 0.0254 | 0.0490
Epoch 137/300, seasonal_1 Loss: 0.0251 | 0.0514
Epoch 138/300, seasonal_1 Loss: 0.0250 | 0.0571
Epoch 139/300, seasonal_1 Loss: 0.0249 | 0.0548
Epoch 140/300, seasonal_1 Loss: 0.0249 | 0.0536
Epoch 141/300, seasonal_1 Loss: 0.0249 | 0.0534
Epoch 142/300, seasonal_1 Loss: 0.0250 | 0.0577
Epoch 143/300, seasonal_1 Loss: 0.0251 | 0.0558
Epoch 144/300, seasonal_1 Loss: 0.0249 | 0.0550
Epoch 145/300, seasonal_1 Loss: 0.0249 | 0.0510
Epoch 146/300, seasonal_1 Loss: 0.0252 | 0.0505
Epoch 147/300, seasonal_1 Loss: 0.0252 | 0.0494
Epoch 148/300, seasonal_1 Loss: 0.0251 | 0.0551
Epoch 149/300, seasonal_1 Loss: 0.0255 | 0.0535
Epoch 150/300, seasonal_1 Loss: 0.0250 | 0.0525
Epoch 151/300, seasonal_1 Loss: 0.0257 | 0.0510
Epoch 152/300, seasonal_1 Loss: 0.0249 | 0.0561
Epoch 153/300, seasonal_1 Loss: 0.0250 | 0.0522
Epoch 154/300, seasonal_1 Loss: 0.0250 | 0.0535
Epoch 155/300, seasonal_1 Loss: 0.0252 | 0.0534
Epoch 156/300, seasonal_1 Loss: 0.0251 | 0.0500
Epoch 157/300, seasonal_1 Loss: 0.0249 | 0.0455
Epoch 158/300, seasonal_1 Loss: 0.0251 | 0.0514
Epoch 159/300, seasonal_1 Loss: 0.0252 | 0.0479
Epoch 160/300, seasonal_1 Loss: 0.0251 | 0.0473
Epoch 161/300, seasonal_1 Loss: 0.0255 | 0.0498
Epoch 162/300, seasonal_1 Loss: 0.0252 | 0.0555
Epoch 163/300, seasonal_1 Loss: 0.0247 | 0.0536
Epoch 164/300, seasonal_1 Loss: 0.0247 | 0.0563
Epoch 165/300, seasonal_1 Loss: 0.0247 | 0.0557
Epoch 166/300, seasonal_1 Loss: 0.0246 | 0.0533
Epoch 167/300, seasonal_1 Loss: 0.0247 | 0.0513
Epoch 168/300, seasonal_1 Loss: 0.0248 | 0.0505
Epoch 169/300, seasonal_1 Loss: 0.0253 | 0.0480
Epoch 170/300, seasonal_1 Loss: 0.0254 | 0.0475
Epoch 171/300, seasonal_1 Loss: 0.0248 | 0.0465
Epoch 172/300, seasonal_1 Loss: 0.0244 | 0.0476
Epoch 173/300, seasonal_1 Loss: 0.0243 | 0.0472
Epoch 174/300, seasonal_1 Loss: 0.0242 | 0.0464
Epoch 175/300, seasonal_1 Loss: 0.0240 | 0.0457
Epoch 176/300, seasonal_1 Loss: 0.0239 | 0.0469
Epoch 177/300, seasonal_1 Loss: 0.0240 | 0.0475
Epoch 178/300, seasonal_1 Loss: 0.0240 | 0.0497
Epoch 179/300, seasonal_1 Loss: 0.0244 | 0.0510
Epoch 180/300, seasonal_1 Loss: 0.0248 | 0.0555
Epoch 181/300, seasonal_1 Loss: 0.0244 | 0.0617
Epoch 182/300, seasonal_1 Loss: 0.0238 | 0.0633
Epoch 183/300, seasonal_1 Loss: 0.0240 | 0.0640
Epoch 184/300, seasonal_1 Loss: 0.0250 | 0.0650
Epoch 185/300, seasonal_1 Loss: 0.0247 | 0.0601
Epoch 186/300, seasonal_1 Loss: 0.0249 | 0.0548
Epoch 187/300, seasonal_1 Loss: 0.0257 | 0.0512
Epoch 188/300, seasonal_1 Loss: 0.0249 | 0.0495
Epoch 189/300, seasonal_1 Loss: 0.0241 | 0.0493
Epoch 190/300, seasonal_1 Loss: 0.0250 | 0.0500
Epoch 191/300, seasonal_1 Loss: 0.0238 | 0.0503
Epoch 192/300, seasonal_1 Loss: 0.0234 | 0.0498
Epoch 193/300, seasonal_1 Loss: 0.0233 | 0.0512
Epoch 194/300, seasonal_1 Loss: 0.0232 | 0.0507
Epoch 195/300, seasonal_1 Loss: 0.0232 | 0.0503
Epoch 196/300, seasonal_1 Loss: 0.0231 | 0.0489
Epoch 197/300, seasonal_1 Loss: 0.0232 | 0.0487
Epoch 198/300, seasonal_1 Loss: 0.0231 | 0.0475
Epoch 199/300, seasonal_1 Loss: 0.0230 | 0.0489
Epoch 200/300, seasonal_1 Loss: 0.0229 | 0.0493
Epoch 201/300, seasonal_1 Loss: 0.0229 | 0.0494
Epoch 202/300, seasonal_1 Loss: 0.0228 | 0.0486
Epoch 203/300, seasonal_1 Loss: 0.0229 | 0.0501
Epoch 204/300, seasonal_1 Loss: 0.0230 | 0.0503
Epoch 205/300, seasonal_1 Loss: 0.0229 | 0.0506
Epoch 206/300, seasonal_1 Loss: 0.0227 | 0.0495
Epoch 207/300, seasonal_1 Loss: 0.0229 | 0.0495
Epoch 208/300, seasonal_1 Loss: 0.0230 | 0.0488
Epoch 209/300, seasonal_1 Loss: 0.0229 | 0.0488
Epoch 210/300, seasonal_1 Loss: 0.0227 | 0.0482
Epoch 211/300, seasonal_1 Loss: 0.0227 | 0.0481
Epoch 212/300, seasonal_1 Loss: 0.0227 | 0.0476
Epoch 213/300, seasonal_1 Loss: 0.0227 | 0.0477
Epoch 214/300, seasonal_1 Loss: 0.0226 | 0.0475
Epoch 215/300, seasonal_1 Loss: 0.0226 | 0.0478
Epoch 216/300, seasonal_1 Loss: 0.0226 | 0.0480
Epoch 217/300, seasonal_1 Loss: 0.0226 | 0.0494
Epoch 218/300, seasonal_1 Loss: 0.0226 | 0.0498
Epoch 219/300, seasonal_1 Loss: 0.0226 | 0.0502
Epoch 220/300, seasonal_1 Loss: 0.0225 | 0.0498
Epoch 221/300, seasonal_1 Loss: 0.0226 | 0.0495
Epoch 222/300, seasonal_1 Loss: 0.0227 | 0.0492
Epoch 223/300, seasonal_1 Loss: 0.0227 | 0.0485
Epoch 224/300, seasonal_1 Loss: 0.0227 | 0.0470
Epoch 225/300, seasonal_1 Loss: 0.0227 | 0.0472
Epoch 226/300, seasonal_1 Loss: 0.0228 | 0.0473
Epoch 227/300, seasonal_1 Loss: 0.0227 | 0.0473
Epoch 228/300, seasonal_1 Loss: 0.0226 | 0.0473
Epoch 229/300, seasonal_1 Loss: 0.0227 | 0.0476
Epoch 230/300, seasonal_1 Loss: 0.0228 | 0.0481
Epoch 231/300, seasonal_1 Loss: 0.0227 | 0.0484
Epoch 232/300, seasonal_1 Loss: 0.0226 | 0.0483
Epoch 233/300, seasonal_1 Loss: 0.0226 | 0.0485
Epoch 234/300, seasonal_1 Loss: 0.0229 | 0.0485
Epoch 235/300, seasonal_1 Loss: 0.0229 | 0.0485
Epoch 236/300, seasonal_1 Loss: 0.0227 | 0.0488
Epoch 237/300, seasonal_1 Loss: 0.0227 | 0.0492
Epoch 238/300, seasonal_1 Loss: 0.0228 | 0.0488
Epoch 239/300, seasonal_1 Loss: 0.0228 | 0.0484
Epoch 240/300, seasonal_1 Loss: 0.0228 | 0.0479
Epoch 241/300, seasonal_1 Loss: 0.0228 | 0.0476
Epoch 242/300, seasonal_1 Loss: 0.0227 | 0.0477
Epoch 243/300, seasonal_1 Loss: 0.0228 | 0.0483
Epoch 244/300, seasonal_1 Loss: 0.0229 | 0.0500
Epoch 245/300, seasonal_1 Loss: 0.0229 | 0.0506
Epoch 246/300, seasonal_1 Loss: 0.0227 | 0.0493
Epoch 247/300, seasonal_1 Loss: 0.0227 | 0.0487
Epoch 248/300, seasonal_1 Loss: 0.0227 | 0.0488
Epoch 249/300, seasonal_1 Loss: 0.0227 | 0.0487
Epoch 250/300, seasonal_1 Loss: 0.0226 | 0.0483
Epoch 251/300, seasonal_1 Loss: 0.0226 | 0.0476
Epoch 252/300, seasonal_1 Loss: 0.0225 | 0.0473
Epoch 253/300, seasonal_1 Loss: 0.0224 | 0.0475
Epoch 254/300, seasonal_1 Loss: 0.0223 | 0.0479
Epoch 255/300, seasonal_1 Loss: 0.0223 | 0.0482
Epoch 256/300, seasonal_1 Loss: 0.0223 | 0.0485
Epoch 257/300, seasonal_1 Loss: 0.0223 | 0.0485
Epoch 258/300, seasonal_1 Loss: 0.0222 | 0.0484
Epoch 259/300, seasonal_1 Loss: 0.0222 | 0.0484
Epoch 260/300, seasonal_1 Loss: 0.0222 | 0.0483
Epoch 261/300, seasonal_1 Loss: 0.0222 | 0.0480
Epoch 262/300, seasonal_1 Loss: 0.0221 | 0.0478
Epoch 263/300, seasonal_1 Loss: 0.0221 | 0.0476
Epoch 264/300, seasonal_1 Loss: 0.0221 | 0.0476
Epoch 265/300, seasonal_1 Loss: 0.0221 | 0.0478
Epoch 266/300, seasonal_1 Loss: 0.0220 | 0.0479
Epoch 267/300, seasonal_1 Loss: 0.0220 | 0.0481
Epoch 268/300, seasonal_1 Loss: 0.0220 | 0.0481
Epoch 269/300, seasonal_1 Loss: 0.0220 | 0.0481
Epoch 270/300, seasonal_1 Loss: 0.0220 | 0.0480
Epoch 271/300, seasonal_1 Loss: 0.0220 | 0.0479
Epoch 272/300, seasonal_1 Loss: 0.0220 | 0.0478
Epoch 273/300, seasonal_1 Loss: 0.0220 | 0.0476
Epoch 274/300, seasonal_1 Loss: 0.0219 | 0.0475
Epoch 275/300, seasonal_1 Loss: 0.0219 | 0.0474
Epoch 276/300, seasonal_1 Loss: 0.0219 | 0.0474
Epoch 277/300, seasonal_1 Loss: 0.0219 | 0.0476
Epoch 278/300, seasonal_1 Loss: 0.0219 | 0.0477
Epoch 279/300, seasonal_1 Loss: 0.0219 | 0.0478
Epoch 280/300, seasonal_1 Loss: 0.0219 | 0.0478
Epoch 281/300, seasonal_1 Loss: 0.0219 | 0.0478
Epoch 282/300, seasonal_1 Loss: 0.0219 | 0.0477
Epoch 283/300, seasonal_1 Loss: 0.0219 | 0.0476
Epoch 284/300, seasonal_1 Loss: 0.0219 | 0.0475
Epoch 285/300, seasonal_1 Loss: 0.0219 | 0.0473
Epoch 286/300, seasonal_1 Loss: 0.0219 | 0.0472
Epoch 287/300, seasonal_1 Loss: 0.0219 | 0.0472
Epoch 288/300, seasonal_1 Loss: 0.0219 | 0.0472
Epoch 289/300, seasonal_1 Loss: 0.0219 | 0.0474
Epoch 290/300, seasonal_1 Loss: 0.0219 | 0.0474
Epoch 291/300, seasonal_1 Loss: 0.0219 | 0.0475
Epoch 292/300, seasonal_1 Loss: 0.0218 | 0.0476
Epoch 293/300, seasonal_1 Loss: 0.0218 | 0.0476
Epoch 294/300, seasonal_1 Loss: 0.0218 | 0.0475
Epoch 295/300, seasonal_1 Loss: 0.0218 | 0.0475
Epoch 296/300, seasonal_1 Loss: 0.0218 | 0.0473
Epoch 297/300, seasonal_1 Loss: 0.0218 | 0.0472
Epoch 298/300, seasonal_1 Loss: 0.0218 | 0.0471
Epoch 299/300, seasonal_1 Loss: 0.0218 | 0.0470
Epoch 300/300, seasonal_1 Loss: 0.0218 | 0.0470
Training seasonal_2 component with params: {'observation_period_num': 8, 'train_rates': 0.8890503642068391, 'learning_rate': 0.0006596441910718066, 'batch_size': 204, 'step_size': 11, 'gamma': 0.8633711677735286}
Epoch 1/300, seasonal_2 Loss: 2.0734 | 1.2854
Epoch 2/300, seasonal_2 Loss: 1.1135 | 1.3618
Epoch 3/300, seasonal_2 Loss: 1.0611 | 1.3065
Epoch 4/300, seasonal_2 Loss: 1.5195 | 1.4592
Epoch 5/300, seasonal_2 Loss: 1.2184 | 1.9255
Epoch 6/300, seasonal_2 Loss: 0.9329 | 1.6608
Epoch 7/300, seasonal_2 Loss: 1.0019 | 1.8106
Epoch 8/300, seasonal_2 Loss: 0.9523 | 1.7413
Epoch 9/300, seasonal_2 Loss: 0.9718 | 1.7679
Epoch 10/300, seasonal_2 Loss: 0.9644 | 1.7594
Epoch 11/300, seasonal_2 Loss: 0.9662 | 1.7614
Epoch 12/300, seasonal_2 Loss: 0.9581 | 1.7927
Epoch 13/300, seasonal_2 Loss: 0.9488 | 1.7758
Epoch 14/300, seasonal_2 Loss: 0.9539 | 1.7843
Epoch 15/300, seasonal_2 Loss: 0.9514 | 1.7800
Epoch 16/300, seasonal_2 Loss: 0.9526 | 1.7828
Epoch 17/300, seasonal_2 Loss: 0.9516 | 1.7823
Epoch 18/300, seasonal_2 Loss: 0.9452 | 1.8047
Epoch 19/300, seasonal_2 Loss: 0.9396 | 1.7967
Epoch 20/300, seasonal_2 Loss: 0.9414 | 1.7990
Epoch 21/300, seasonal_2 Loss: 0.9409 | 1.7992
Epoch 22/300, seasonal_2 Loss: 0.9406 | 1.7991
Epoch 23/300, seasonal_2 Loss: 0.9353 | 1.8147
Epoch 24/300, seasonal_2 Loss: 0.9322 | 1.8121
Epoch 25/300, seasonal_2 Loss: 0.9324 | 1.8105
Epoch 26/300, seasonal_2 Loss: 0.9329 | 1.8128
Epoch 27/300, seasonal_2 Loss: 0.9322 | 1.8115
Epoch 28/300, seasonal_2 Loss: 0.9325 | 1.8126
Epoch 29/300, seasonal_2 Loss: 0.9278 | 1.8227
Epoch 30/300, seasonal_2 Loss: 0.9263 | 1.8240
Epoch 31/300, seasonal_2 Loss: 0.9256 | 1.8204
Epoch 32/300, seasonal_2 Loss: 0.9265 | 1.8230
Epoch 33/300, seasonal_2 Loss: 0.9259 | 1.8220
Epoch 34/300, seasonal_2 Loss: 0.9223 | 1.8296
Epoch 35/300, seasonal_2 Loss: 0.9215 | 1.8325
Epoch 36/300, seasonal_2 Loss: 0.9206 | 1.8286
Epoch 37/300, seasonal_2 Loss: 0.9214 | 1.8304
Epoch 38/300, seasonal_2 Loss: 0.9211 | 1.8303
Epoch 39/300, seasonal_2 Loss: 0.9211 | 1.8300
Epoch 40/300, seasonal_2 Loss: 0.9180 | 1.8355
Epoch 41/300, seasonal_2 Loss: 0.9176 | 1.8388
Epoch 42/300, seasonal_2 Loss: 0.9167 | 1.8357
Epoch 43/300, seasonal_2 Loss: 0.9173 | 1.8364
Epoch 44/300, seasonal_2 Loss: 0.9173 | 1.8368
Epoch 45/300, seasonal_2 Loss: 0.9145 | 1.8402
Epoch 46/300, seasonal_2 Loss: 0.9145 | 1.8436
Epoch 47/300, seasonal_2 Loss: 0.9137 | 1.8415
Epoch 48/300, seasonal_2 Loss: 0.9141 | 1.8413
Epoch 49/300, seasonal_2 Loss: 0.9142 | 1.8419
Epoch 50/300, seasonal_2 Loss: 0.9141 | 1.8417
Epoch 51/300, seasonal_2 Loss: 0.9118 | 1.8443
Epoch 52/300, seasonal_2 Loss: 0.9119 | 1.8476
Epoch 53/300, seasonal_2 Loss: 0.9113 | 1.8461
Epoch 54/300, seasonal_2 Loss: 0.9114 | 1.8456
Epoch 55/300, seasonal_2 Loss: 0.9116 | 1.8460
Epoch 56/300, seasonal_2 Loss: 0.9096 | 1.8480
Epoch 57/300, seasonal_2 Loss: 0.9097 | 1.8507
Epoch 58/300, seasonal_2 Loss: 0.9093 | 1.8499
Epoch 59/300, seasonal_2 Loss: 0.9093 | 1.8492
Epoch 60/300, seasonal_2 Loss: 0.9094 | 1.8494
Epoch 61/300, seasonal_2 Loss: 0.9095 | 1.8496
Epoch 62/300, seasonal_2 Loss: 0.9078 | 1.8510
Epoch 63/300, seasonal_2 Loss: 0.9079 | 1.8533
Epoch 64/300, seasonal_2 Loss: 0.9076 | 1.8530
Epoch 65/300, seasonal_2 Loss: 0.9076 | 1.8524
Epoch 66/300, seasonal_2 Loss: 0.9077 | 1.8524
Epoch 67/300, seasonal_2 Loss: 0.9063 | 1.8536
Epoch 68/300, seasonal_2 Loss: 0.9064 | 1.8556
Epoch 69/300, seasonal_2 Loss: 0.9061 | 1.8556
Epoch 70/300, seasonal_2 Loss: 0.9061 | 1.8550
Epoch 71/300, seasonal_2 Loss: 0.9062 | 1.8549
Epoch 72/300, seasonal_2 Loss: 0.9062 | 1.8550
Epoch 73/300, seasonal_2 Loss: 0.9050 | 1.8558
Epoch 74/300, seasonal_2 Loss: 0.9051 | 1.8575
Epoch 75/300, seasonal_2 Loss: 0.9049 | 1.8577
Epoch 76/300, seasonal_2 Loss: 0.9049 | 1.8573
Epoch 77/300, seasonal_2 Loss: 0.9049 | 1.8571
Epoch 78/300, seasonal_2 Loss: 0.9039 | 1.8577
Epoch 79/300, seasonal_2 Loss: 0.9040 | 1.8591
Epoch 80/300, seasonal_2 Loss: 0.9038 | 1.8595
Epoch 81/300, seasonal_2 Loss: 0.9038 | 1.8592
Epoch 82/300, seasonal_2 Loss: 0.9038 | 1.8590
Epoch 83/300, seasonal_2 Loss: 0.9039 | 1.8590
Epoch 84/300, seasonal_2 Loss: 0.9029 | 1.8595
Epoch 85/300, seasonal_2 Loss: 0.9030 | 1.8606
Epoch 86/300, seasonal_2 Loss: 0.9029 | 1.8609
Epoch 87/300, seasonal_2 Loss: 0.9029 | 1.8608
Epoch 88/300, seasonal_2 Loss: 0.9029 | 1.8606
Epoch 89/300, seasonal_2 Loss: 0.9021 | 1.8609
Epoch 90/300, seasonal_2 Loss: 0.9022 | 1.8618
Epoch 91/300, seasonal_2 Loss: 0.9022 | 1.8622
Epoch 92/300, seasonal_2 Loss: 0.9021 | 1.8622
Epoch 93/300, seasonal_2 Loss: 0.9021 | 1.8621
Epoch 94/300, seasonal_2 Loss: 0.9021 | 1.8620
Epoch 95/300, seasonal_2 Loss: 0.9014 | 1.8622
Epoch 96/300, seasonal_2 Loss: 0.9015 | 1.8629
Epoch 97/300, seasonal_2 Loss: 0.9015 | 1.8633
Epoch 98/300, seasonal_2 Loss: 0.9014 | 1.8633
Epoch 99/300, seasonal_2 Loss: 0.9014 | 1.8633
Epoch 100/300, seasonal_2 Loss: 0.9008 | 1.8634
Epoch 101/300, seasonal_2 Loss: 0.9009 | 1.8639
Epoch 102/300, seasonal_2 Loss: 0.9009 | 1.8642
Epoch 103/300, seasonal_2 Loss: 0.9009 | 1.8643
Epoch 104/300, seasonal_2 Loss: 0.9009 | 1.8643
Epoch 105/300, seasonal_2 Loss: 0.9009 | 1.8643
Epoch 106/300, seasonal_2 Loss: 0.9003 | 1.8644
Epoch 107/300, seasonal_2 Loss: 0.9004 | 1.8648
Epoch 108/300, seasonal_2 Loss: 0.9004 | 1.8651
Epoch 109/300, seasonal_2 Loss: 0.9004 | 1.8652
Epoch 110/300, seasonal_2 Loss: 0.9004 | 1.8652
Epoch 111/300, seasonal_2 Loss: 0.8999 | 1.8653
Epoch 112/300, seasonal_2 Loss: 0.8999 | 1.8656
Epoch 113/300, seasonal_2 Loss: 0.8999 | 1.8658
Epoch 114/300, seasonal_2 Loss: 0.8999 | 1.8659
Epoch 115/300, seasonal_2 Loss: 0.8999 | 1.8660
Epoch 116/300, seasonal_2 Loss: 0.8999 | 1.8659
Epoch 117/300, seasonal_2 Loss: 0.8995 | 1.8660
Epoch 118/300, seasonal_2 Loss: 0.8995 | 1.8663
Epoch 119/300, seasonal_2 Loss: 0.8995 | 1.8665
Epoch 120/300, seasonal_2 Loss: 0.8995 | 1.8666
Epoch 121/300, seasonal_2 Loss: 0.8995 | 1.8666
Epoch 122/300, seasonal_2 Loss: 0.8992 | 1.8667
Epoch 123/300, seasonal_2 Loss: 0.8992 | 1.8669
Epoch 124/300, seasonal_2 Loss: 0.8992 | 1.8670
Epoch 125/300, seasonal_2 Loss: 0.8992 | 1.8671
Epoch 126/300, seasonal_2 Loss: 0.8992 | 1.8672
Epoch 127/300, seasonal_2 Loss: 0.8992 | 1.8672
Epoch 128/300, seasonal_2 Loss: 0.8989 | 1.8672
Epoch 129/300, seasonal_2 Loss: 0.8989 | 1.8674
Epoch 130/300, seasonal_2 Loss: 0.8989 | 1.8675
Epoch 131/300, seasonal_2 Loss: 0.8989 | 1.8676
Epoch 132/300, seasonal_2 Loss: 0.8989 | 1.8677
Epoch 133/300, seasonal_2 Loss: 0.8986 | 1.8677
Epoch 134/300, seasonal_2 Loss: 0.8986 | 1.8679
Epoch 135/300, seasonal_2 Loss: 0.8986 | 1.8680
Epoch 136/300, seasonal_2 Loss: 0.8986 | 1.8681
Epoch 137/300, seasonal_2 Loss: 0.8986 | 1.8681
Epoch 138/300, seasonal_2 Loss: 0.8986 | 1.8681
Epoch 139/300, seasonal_2 Loss: 0.8984 | 1.8682
Epoch 140/300, seasonal_2 Loss: 0.8984 | 1.8683
Epoch 141/300, seasonal_2 Loss: 0.8984 | 1.8684
Epoch 142/300, seasonal_2 Loss: 0.8984 | 1.8684
Epoch 143/300, seasonal_2 Loss: 0.8984 | 1.8685
Epoch 144/300, seasonal_2 Loss: 0.8982 | 1.8685
Epoch 145/300, seasonal_2 Loss: 0.8982 | 1.8686
Epoch 146/300, seasonal_2 Loss: 0.8982 | 1.8687
Epoch 147/300, seasonal_2 Loss: 0.8982 | 1.8688
Epoch 148/300, seasonal_2 Loss: 0.8982 | 1.8688
Epoch 149/300, seasonal_2 Loss: 0.8982 | 1.8688
Epoch 150/300, seasonal_2 Loss: 0.8980 | 1.8689
Epoch 151/300, seasonal_2 Loss: 0.8980 | 1.8689
Epoch 152/300, seasonal_2 Loss: 0.8980 | 1.8690
Epoch 153/300, seasonal_2 Loss: 0.8980 | 1.8691
Epoch 154/300, seasonal_2 Loss: 0.8980 | 1.8691
Epoch 155/300, seasonal_2 Loss: 0.8979 | 1.8691
Epoch 156/300, seasonal_2 Loss: 0.8979 | 1.8692
Epoch 157/300, seasonal_2 Loss: 0.8979 | 1.8693
Epoch 158/300, seasonal_2 Loss: 0.8979 | 1.8693
Epoch 159/300, seasonal_2 Loss: 0.8979 | 1.8693
Epoch 160/300, seasonal_2 Loss: 0.8979 | 1.8694
Epoch 161/300, seasonal_2 Loss: 0.8978 | 1.8694
Epoch 162/300, seasonal_2 Loss: 0.8978 | 1.8695
Epoch 163/300, seasonal_2 Loss: 0.8978 | 1.8695
Epoch 164/300, seasonal_2 Loss: 0.8978 | 1.8695
Epoch 165/300, seasonal_2 Loss: 0.8978 | 1.8696
Epoch 166/300, seasonal_2 Loss: 0.8977 | 1.8696
Epoch 167/300, seasonal_2 Loss: 0.8977 | 1.8696
Epoch 168/300, seasonal_2 Loss: 0.8977 | 1.8697
Epoch 169/300, seasonal_2 Loss: 0.8977 | 1.8697
Epoch 170/300, seasonal_2 Loss: 0.8977 | 1.8698
Epoch 171/300, seasonal_2 Loss: 0.8977 | 1.8698
Epoch 172/300, seasonal_2 Loss: 0.8976 | 1.8698
Epoch 173/300, seasonal_2 Loss: 0.8976 | 1.8698
Epoch 174/300, seasonal_2 Loss: 0.8976 | 1.8699
Epoch 175/300, seasonal_2 Loss: 0.8976 | 1.8699
Epoch 176/300, seasonal_2 Loss: 0.8976 | 1.8699
Epoch 177/300, seasonal_2 Loss: 0.8975 | 1.8700
Epoch 178/300, seasonal_2 Loss: 0.8975 | 1.8700
Epoch 179/300, seasonal_2 Loss: 0.8975 | 1.8700
Epoch 180/300, seasonal_2 Loss: 0.8975 | 1.8700
Epoch 181/300, seasonal_2 Loss: 0.8975 | 1.8701
Epoch 182/300, seasonal_2 Loss: 0.8975 | 1.8701
Epoch 183/300, seasonal_2 Loss: 0.8974 | 1.8701
Epoch 184/300, seasonal_2 Loss: 0.8974 | 1.8701
Epoch 185/300, seasonal_2 Loss: 0.8974 | 1.8702
Epoch 186/300, seasonal_2 Loss: 0.8974 | 1.8702
Epoch 187/300, seasonal_2 Loss: 0.8974 | 1.8702
Epoch 188/300, seasonal_2 Loss: 0.8973 | 1.8702
Epoch 189/300, seasonal_2 Loss: 0.8973 | 1.8702
Epoch 190/300, seasonal_2 Loss: 0.8973 | 1.8703
Epoch 191/300, seasonal_2 Loss: 0.8973 | 1.8703
Epoch 192/300, seasonal_2 Loss: 0.8973 | 1.8703
Epoch 193/300, seasonal_2 Loss: 0.8973 | 1.8703
Epoch 194/300, seasonal_2 Loss: 0.8973 | 1.8703
Epoch 195/300, seasonal_2 Loss: 0.8973 | 1.8704
Epoch 196/300, seasonal_2 Loss: 0.8973 | 1.8704
Epoch 197/300, seasonal_2 Loss: 0.8973 | 1.8704
Epoch 198/300, seasonal_2 Loss: 0.8973 | 1.8704
Epoch 199/300, seasonal_2 Loss: 0.8972 | 1.8704
Epoch 200/300, seasonal_2 Loss: 0.8972 | 1.8704
Epoch 201/300, seasonal_2 Loss: 0.8972 | 1.8705
Epoch 202/300, seasonal_2 Loss: 0.8972 | 1.8705
Epoch 203/300, seasonal_2 Loss: 0.8972 | 1.8705
Epoch 204/300, seasonal_2 Loss: 0.8972 | 1.8705
Epoch 205/300, seasonal_2 Loss: 0.8972 | 1.8705
Epoch 206/300, seasonal_2 Loss: 0.8972 | 1.8705
Epoch 207/300, seasonal_2 Loss: 0.8972 | 1.8705
Epoch 208/300, seasonal_2 Loss: 0.8972 | 1.8706
Epoch 209/300, seasonal_2 Loss: 0.8972 | 1.8706
Epoch 210/300, seasonal_2 Loss: 0.8971 | 1.8706
Epoch 211/300, seasonal_2 Loss: 0.8971 | 1.8706
Epoch 212/300, seasonal_2 Loss: 0.8971 | 1.8706
Epoch 213/300, seasonal_2 Loss: 0.8971 | 1.8706
Epoch 214/300, seasonal_2 Loss: 0.8971 | 1.8706
Epoch 215/300, seasonal_2 Loss: 0.8971 | 1.8706
Epoch 216/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 217/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 218/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 219/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 220/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 221/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 222/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 223/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 224/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 225/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 226/300, seasonal_2 Loss: 0.8971 | 1.8707
Epoch 227/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 228/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 229/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 230/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 231/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 232/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 233/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 234/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 235/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 236/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 237/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 238/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 239/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 240/300, seasonal_2 Loss: 0.8970 | 1.8708
Epoch 241/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 242/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 243/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 244/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 245/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 246/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 247/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 248/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 249/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 250/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 251/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 252/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 253/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 254/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 255/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 256/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 257/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 258/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 259/300, seasonal_2 Loss: 0.8970 | 1.8709
Epoch 260/300, seasonal_2 Loss: 0.8969 | 1.8709
Epoch 261/300, seasonal_2 Loss: 0.8969 | 1.8709
Epoch 262/300, seasonal_2 Loss: 0.8969 | 1.8709
Epoch 263/300, seasonal_2 Loss: 0.8969 | 1.8709
Epoch 264/300, seasonal_2 Loss: 0.8969 | 1.8709
Epoch 265/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 266/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 267/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 268/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 269/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 270/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 271/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 272/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 273/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 274/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 275/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 276/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 277/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 278/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 279/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 280/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 281/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 282/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 283/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 284/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 285/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 286/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 287/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 288/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 289/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 290/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 291/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 292/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 293/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 294/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 295/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 296/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 297/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 298/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 299/300, seasonal_2 Loss: 0.8969 | 1.8710
Epoch 300/300, seasonal_2 Loss: 0.8969 | 1.8710
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.932002288255847, 'learning_rate': 0.0009627007353084302, 'batch_size': 104, 'step_size': 10, 'gamma': 0.8964767543554167}
Epoch 1/300, seasonal_3 Loss: 2.5212 | 1.4884
Epoch 2/300, seasonal_3 Loss: 0.9943 | 1.5222
Epoch 3/300, seasonal_3 Loss: 0.9777 | 1.3935
Epoch 4/300, seasonal_3 Loss: 1.0724 | 1.3948
Epoch 5/300, seasonal_3 Loss: 1.0211 | 1.4235
Epoch 6/300, seasonal_3 Loss: 1.0225 | 1.4012
Epoch 7/300, seasonal_3 Loss: 1.0557 | 1.3971
Epoch 8/300, seasonal_3 Loss: 1.1349 | 1.4109
Epoch 9/300, seasonal_3 Loss: 1.2387 | 1.3955
Epoch 10/300, seasonal_3 Loss: 1.2762 | 1.4857
Epoch 11/300, seasonal_3 Loss: 1.1992 | 1.7406
Epoch 12/300, seasonal_3 Loss: 1.0551 | 1.7290
Epoch 13/300, seasonal_3 Loss: 1.0451 | 1.7284
Epoch 14/300, seasonal_3 Loss: 1.0453 | 1.7421
Epoch 15/300, seasonal_3 Loss: 1.0388 | 1.7496
Epoch 16/300, seasonal_3 Loss: 1.0297 | 1.7958
Epoch 17/300, seasonal_3 Loss: 1.0125 | 1.7867
Epoch 18/300, seasonal_3 Loss: 1.0145 | 1.7930
Epoch 19/300, seasonal_3 Loss: 1.0120 | 1.7961
Epoch 20/300, seasonal_3 Loss: 1.0102 | 1.7999
Epoch 21/300, seasonal_3 Loss: 1.0037 | 1.8299
Epoch 22/300, seasonal_3 Loss: 0.9940 | 1.8240
Epoch 23/300, seasonal_3 Loss: 0.9954 | 1.8275
Epoch 24/300, seasonal_3 Loss: 0.9940 | 1.8288
Epoch 25/300, seasonal_3 Loss: 0.9932 | 1.8308
Epoch 26/300, seasonal_3 Loss: 0.9880 | 1.8516
Epoch 27/300, seasonal_3 Loss: 0.9822 | 1.8486
Epoch 28/300, seasonal_3 Loss: 0.9829 | 1.8500
Epoch 29/300, seasonal_3 Loss: 0.9822 | 1.8509
Epoch 30/300, seasonal_3 Loss: 0.9818 | 1.8519
Epoch 31/300, seasonal_3 Loss: 0.9775 | 1.8668
Epoch 32/300, seasonal_3 Loss: 0.9738 | 1.8659
Epoch 33/300, seasonal_3 Loss: 0.9740 | 1.8661
Epoch 34/300, seasonal_3 Loss: 0.9738 | 1.8668
Epoch 35/300, seasonal_3 Loss: 0.9735 | 1.8673
Epoch 36/300, seasonal_3 Loss: 0.9699 | 1.8783
Epoch 37/300, seasonal_3 Loss: 0.9675 | 1.8788
Epoch 38/300, seasonal_3 Loss: 0.9674 | 1.8783
Epoch 39/300, seasonal_3 Loss: 0.9674 | 1.8788
Epoch 40/300, seasonal_3 Loss: 0.9672 | 1.8790
Epoch 41/300, seasonal_3 Loss: 0.9641 | 1.8874
Epoch 42/300, seasonal_3 Loss: 0.9625 | 1.8886
Epoch 43/300, seasonal_3 Loss: 0.9622 | 1.8878
Epoch 44/300, seasonal_3 Loss: 0.9624 | 1.8881
Epoch 45/300, seasonal_3 Loss: 0.9622 | 1.8883
Epoch 46/300, seasonal_3 Loss: 0.9595 | 1.8946
Epoch 47/300, seasonal_3 Loss: 0.9585 | 1.8962
Epoch 48/300, seasonal_3 Loss: 0.9582 | 1.8955
Epoch 49/300, seasonal_3 Loss: 0.9583 | 1.8956
Epoch 50/300, seasonal_3 Loss: 0.9583 | 1.8957
Epoch 51/300, seasonal_3 Loss: 0.9559 | 1.9006
Epoch 52/300, seasonal_3 Loss: 0.9552 | 1.9024
Epoch 53/300, seasonal_3 Loss: 0.9549 | 1.9018
Epoch 54/300, seasonal_3 Loss: 0.9550 | 1.9018
Epoch 55/300, seasonal_3 Loss: 0.9550 | 1.9018
Epoch 56/300, seasonal_3 Loss: 0.9529 | 1.9057
Epoch 57/300, seasonal_3 Loss: 0.9524 | 1.9075
Epoch 58/300, seasonal_3 Loss: 0.9521 | 1.9071
Epoch 59/300, seasonal_3 Loss: 0.9522 | 1.9069
Epoch 60/300, seasonal_3 Loss: 0.9522 | 1.9069
Epoch 61/300, seasonal_3 Loss: 0.9503 | 1.9100
Epoch 62/300, seasonal_3 Loss: 0.9500 | 1.9118
Epoch 63/300, seasonal_3 Loss: 0.9498 | 1.9115
Epoch 64/300, seasonal_3 Loss: 0.9499 | 1.9113
Epoch 65/300, seasonal_3 Loss: 0.9499 | 1.9113
Epoch 66/300, seasonal_3 Loss: 0.9482 | 1.9137
Epoch 67/300, seasonal_3 Loss: 0.9480 | 1.9154
Epoch 68/300, seasonal_3 Loss: 0.9478 | 1.9152
Epoch 69/300, seasonal_3 Loss: 0.9478 | 1.9151
Epoch 70/300, seasonal_3 Loss: 0.9479 | 1.9150
Epoch 71/300, seasonal_3 Loss: 0.9464 | 1.9169
Epoch 72/300, seasonal_3 Loss: 0.9463 | 1.9185
Epoch 73/300, seasonal_3 Loss: 0.9461 | 1.9185
Epoch 74/300, seasonal_3 Loss: 0.9461 | 1.9183
Epoch 75/300, seasonal_3 Loss: 0.9462 | 1.9182
Epoch 76/300, seasonal_3 Loss: 0.9448 | 1.9198
Epoch 77/300, seasonal_3 Loss: 0.9447 | 1.9212
Epoch 78/300, seasonal_3 Loss: 0.9446 | 1.9213
Epoch 79/300, seasonal_3 Loss: 0.9446 | 1.9211
Epoch 80/300, seasonal_3 Loss: 0.9446 | 1.9210
Epoch 81/300, seasonal_3 Loss: 0.9434 | 1.9223
Epoch 82/300, seasonal_3 Loss: 0.9434 | 1.9235
Epoch 83/300, seasonal_3 Loss: 0.9432 | 1.9237
Epoch 84/300, seasonal_3 Loss: 0.9433 | 1.9236
Epoch 85/300, seasonal_3 Loss: 0.9433 | 1.9235
Epoch 86/300, seasonal_3 Loss: 0.9422 | 1.9246
Epoch 87/300, seasonal_3 Loss: 0.9422 | 1.9256
Epoch 88/300, seasonal_3 Loss: 0.9421 | 1.9259
Epoch 89/300, seasonal_3 Loss: 0.9421 | 1.9258
Epoch 90/300, seasonal_3 Loss: 0.9421 | 1.9257
Epoch 91/300, seasonal_3 Loss: 0.9411 | 1.9266
Epoch 92/300, seasonal_3 Loss: 0.9411 | 1.9275
Epoch 93/300, seasonal_3 Loss: 0.9410 | 1.9278
Epoch 94/300, seasonal_3 Loss: 0.9410 | 1.9277
Epoch 95/300, seasonal_3 Loss: 0.9410 | 1.9277
Epoch 96/300, seasonal_3 Loss: 0.9401 | 1.9284
Epoch 97/300, seasonal_3 Loss: 0.9401 | 1.9292
Epoch 98/300, seasonal_3 Loss: 0.9401 | 1.9295
Epoch 99/300, seasonal_3 Loss: 0.9401 | 1.9295
Epoch 100/300, seasonal_3 Loss: 0.9401 | 1.9294
Epoch 101/300, seasonal_3 Loss: 0.9392 | 1.9300
Epoch 102/300, seasonal_3 Loss: 0.9393 | 1.9307
Epoch 103/300, seasonal_3 Loss: 0.9392 | 1.9310
Epoch 104/300, seasonal_3 Loss: 0.9392 | 1.9310
Epoch 105/300, seasonal_3 Loss: 0.9392 | 1.9310
Epoch 106/300, seasonal_3 Loss: 0.9385 | 1.9315
Epoch 107/300, seasonal_3 Loss: 0.9385 | 1.9321
Epoch 108/300, seasonal_3 Loss: 0.9385 | 1.9323
Epoch 109/300, seasonal_3 Loss: 0.9385 | 1.9324
Epoch 110/300, seasonal_3 Loss: 0.9385 | 1.9324
Epoch 111/300, seasonal_3 Loss: 0.9378 | 1.9328
Epoch 112/300, seasonal_3 Loss: 0.9378 | 1.9333
Epoch 113/300, seasonal_3 Loss: 0.9378 | 1.9336
Epoch 114/300, seasonal_3 Loss: 0.9378 | 1.9337
Epoch 115/300, seasonal_3 Loss: 0.9378 | 1.9337
Epoch 116/300, seasonal_3 Loss: 0.9371 | 1.9340
Epoch 117/300, seasonal_3 Loss: 0.9372 | 1.9345
Epoch 118/300, seasonal_3 Loss: 0.9371 | 1.9347
Epoch 119/300, seasonal_3 Loss: 0.9371 | 1.9348
Epoch 120/300, seasonal_3 Loss: 0.9372 | 1.9348
Epoch 121/300, seasonal_3 Loss: 0.9366 | 1.9351
Epoch 122/300, seasonal_3 Loss: 0.9366 | 1.9355
Epoch 123/300, seasonal_3 Loss: 0.9366 | 1.9357
Epoch 124/300, seasonal_3 Loss: 0.9366 | 1.9358
Epoch 125/300, seasonal_3 Loss: 0.9366 | 1.9358
Epoch 126/300, seasonal_3 Loss: 0.9361 | 1.9361
Epoch 127/300, seasonal_3 Loss: 0.9361 | 1.9364
Epoch 128/300, seasonal_3 Loss: 0.9361 | 1.9366
Epoch 129/300, seasonal_3 Loss: 0.9361 | 1.9367
Epoch 130/300, seasonal_3 Loss: 0.9361 | 1.9368
Epoch 131/300, seasonal_3 Loss: 0.9356 | 1.9370
Epoch 132/300, seasonal_3 Loss: 0.9356 | 1.9373
Epoch 133/300, seasonal_3 Loss: 0.9356 | 1.9375
Epoch 134/300, seasonal_3 Loss: 0.9356 | 1.9376
Epoch 135/300, seasonal_3 Loss: 0.9356 | 1.9376
Epoch 136/300, seasonal_3 Loss: 0.9352 | 1.9378
Epoch 137/300, seasonal_3 Loss: 0.9352 | 1.9380
Epoch 138/300, seasonal_3 Loss: 0.9352 | 1.9382
Epoch 139/300, seasonal_3 Loss: 0.9352 | 1.9383
Epoch 140/300, seasonal_3 Loss: 0.9352 | 1.9384
Epoch 141/300, seasonal_3 Loss: 0.9348 | 1.9385
Epoch 142/300, seasonal_3 Loss: 0.9348 | 1.9387
Epoch 143/300, seasonal_3 Loss: 0.9348 | 1.9389
Epoch 144/300, seasonal_3 Loss: 0.9348 | 1.9390
Epoch 145/300, seasonal_3 Loss: 0.9348 | 1.9391
Epoch 146/300, seasonal_3 Loss: 0.9344 | 1.9392
Epoch 147/300, seasonal_3 Loss: 0.9344 | 1.9394
Epoch 148/300, seasonal_3 Loss: 0.9344 | 1.9395
Epoch 149/300, seasonal_3 Loss: 0.9344 | 1.9396
Epoch 150/300, seasonal_3 Loss: 0.9344 | 1.9397
Epoch 151/300, seasonal_3 Loss: 0.9341 | 1.9398
Epoch 152/300, seasonal_3 Loss: 0.9341 | 1.9400
Epoch 153/300, seasonal_3 Loss: 0.9341 | 1.9401
Epoch 154/300, seasonal_3 Loss: 0.9341 | 1.9402
Epoch 155/300, seasonal_3 Loss: 0.9341 | 1.9402
Epoch 156/300, seasonal_3 Loss: 0.9338 | 1.9404
Epoch 157/300, seasonal_3 Loss: 0.9338 | 1.9405
Epoch 158/300, seasonal_3 Loss: 0.9338 | 1.9406
Epoch 159/300, seasonal_3 Loss: 0.9338 | 1.9407
Epoch 160/300, seasonal_3 Loss: 0.9338 | 1.9408
Epoch 161/300, seasonal_3 Loss: 0.9335 | 1.9409
Epoch 162/300, seasonal_3 Loss: 0.9336 | 1.9410
Epoch 163/300, seasonal_3 Loss: 0.9336 | 1.9411
Epoch 164/300, seasonal_3 Loss: 0.9336 | 1.9412
Epoch 165/300, seasonal_3 Loss: 0.9336 | 1.9412
Epoch 166/300, seasonal_3 Loss: 0.9333 | 1.9413
Epoch 167/300, seasonal_3 Loss: 0.9333 | 1.9414
Epoch 168/300, seasonal_3 Loss: 0.9333 | 1.9415
Epoch 169/300, seasonal_3 Loss: 0.9333 | 1.9416
Epoch 170/300, seasonal_3 Loss: 0.9333 | 1.9417
Epoch 171/300, seasonal_3 Loss: 0.9331 | 1.9417
Epoch 172/300, seasonal_3 Loss: 0.9331 | 1.9418
Epoch 173/300, seasonal_3 Loss: 0.9331 | 1.9419
Epoch 174/300, seasonal_3 Loss: 0.9331 | 1.9420
Epoch 175/300, seasonal_3 Loss: 0.9331 | 1.9420
Epoch 176/300, seasonal_3 Loss: 0.9329 | 1.9421
Epoch 177/300, seasonal_3 Loss: 0.9329 | 1.9422
Epoch 178/300, seasonal_3 Loss: 0.9329 | 1.9423
Epoch 179/300, seasonal_3 Loss: 0.9329 | 1.9423
Epoch 180/300, seasonal_3 Loss: 0.9329 | 1.9424
Epoch 181/300, seasonal_3 Loss: 0.9327 | 1.9424
Epoch 182/300, seasonal_3 Loss: 0.9327 | 1.9425
Epoch 183/300, seasonal_3 Loss: 0.9327 | 1.9426
Epoch 184/300, seasonal_3 Loss: 0.9327 | 1.9427
Epoch 185/300, seasonal_3 Loss: 0.9327 | 1.9427
Epoch 186/300, seasonal_3 Loss: 0.9325 | 1.9428
Epoch 187/300, seasonal_3 Loss: 0.9325 | 1.9428
Epoch 188/300, seasonal_3 Loss: 0.9325 | 1.9429
Epoch 189/300, seasonal_3 Loss: 0.9325 | 1.9429
Epoch 190/300, seasonal_3 Loss: 0.9325 | 1.9430
Epoch 191/300, seasonal_3 Loss: 0.9324 | 1.9430
Epoch 192/300, seasonal_3 Loss: 0.9324 | 1.9431
Epoch 193/300, seasonal_3 Loss: 0.9324 | 1.9432
Epoch 194/300, seasonal_3 Loss: 0.9324 | 1.9432
Epoch 195/300, seasonal_3 Loss: 0.9324 | 1.9433
Epoch 196/300, seasonal_3 Loss: 0.9323 | 1.9433
Epoch 197/300, seasonal_3 Loss: 0.9323 | 1.9434
Epoch 198/300, seasonal_3 Loss: 0.9323 | 1.9434
Epoch 199/300, seasonal_3 Loss: 0.9323 | 1.9434
Epoch 200/300, seasonal_3 Loss: 0.9323 | 1.9435
Epoch 201/300, seasonal_3 Loss: 0.9321 | 1.9435
Epoch 202/300, seasonal_3 Loss: 0.9321 | 1.9436
Epoch 203/300, seasonal_3 Loss: 0.9321 | 1.9436
Epoch 204/300, seasonal_3 Loss: 0.9321 | 1.9437
Epoch 205/300, seasonal_3 Loss: 0.9321 | 1.9437
Epoch 206/300, seasonal_3 Loss: 0.9320 | 1.9437
Epoch 207/300, seasonal_3 Loss: 0.9320 | 1.9438
Epoch 208/300, seasonal_3 Loss: 0.9320 | 1.9438
Epoch 209/300, seasonal_3 Loss: 0.9320 | 1.9439
Epoch 210/300, seasonal_3 Loss: 0.9320 | 1.9439
Epoch 211/300, seasonal_3 Loss: 0.9319 | 1.9439
Epoch 212/300, seasonal_3 Loss: 0.9319 | 1.9440
Epoch 213/300, seasonal_3 Loss: 0.9319 | 1.9440
Epoch 214/300, seasonal_3 Loss: 0.9319 | 1.9440
Epoch 215/300, seasonal_3 Loss: 0.9319 | 1.9441
Epoch 216/300, seasonal_3 Loss: 0.9318 | 1.9441
Epoch 217/300, seasonal_3 Loss: 0.9318 | 1.9441
Epoch 218/300, seasonal_3 Loss: 0.9318 | 1.9442
Epoch 219/300, seasonal_3 Loss: 0.9318 | 1.9442
Epoch 220/300, seasonal_3 Loss: 0.9318 | 1.9442
Epoch 221/300, seasonal_3 Loss: 0.9318 | 1.9443
Epoch 222/300, seasonal_3 Loss: 0.9318 | 1.9443
Epoch 223/300, seasonal_3 Loss: 0.9318 | 1.9443
Epoch 224/300, seasonal_3 Loss: 0.9318 | 1.9443
Epoch 225/300, seasonal_3 Loss: 0.9318 | 1.9444
Epoch 226/300, seasonal_3 Loss: 0.9317 | 1.9444
Epoch 227/300, seasonal_3 Loss: 0.9317 | 1.9444
Epoch 228/300, seasonal_3 Loss: 0.9317 | 1.9445
Epoch 229/300, seasonal_3 Loss: 0.9317 | 1.9445
Epoch 230/300, seasonal_3 Loss: 0.9317 | 1.9445
Epoch 231/300, seasonal_3 Loss: 0.9316 | 1.9445
Epoch 232/300, seasonal_3 Loss: 0.9316 | 1.9445
Epoch 233/300, seasonal_3 Loss: 0.9316 | 1.9446
Epoch 234/300, seasonal_3 Loss: 0.9316 | 1.9446
Epoch 235/300, seasonal_3 Loss: 0.9316 | 1.9446
Epoch 236/300, seasonal_3 Loss: 0.9316 | 1.9446
Epoch 237/300, seasonal_3 Loss: 0.9316 | 1.9447
Epoch 238/300, seasonal_3 Loss: 0.9316 | 1.9447
Epoch 239/300, seasonal_3 Loss: 0.9316 | 1.9447
Epoch 240/300, seasonal_3 Loss: 0.9316 | 1.9447
Epoch 241/300, seasonal_3 Loss: 0.9315 | 1.9447
Epoch 242/300, seasonal_3 Loss: 0.9315 | 1.9448
Epoch 243/300, seasonal_3 Loss: 0.9315 | 1.9448
Epoch 244/300, seasonal_3 Loss: 0.9315 | 1.9448
Epoch 245/300, seasonal_3 Loss: 0.9315 | 1.9448
Epoch 246/300, seasonal_3 Loss: 0.9314 | 1.9448
Epoch 247/300, seasonal_3 Loss: 0.9314 | 1.9449
Epoch 248/300, seasonal_3 Loss: 0.9314 | 1.9449
Epoch 249/300, seasonal_3 Loss: 0.9314 | 1.9449
Epoch 250/300, seasonal_3 Loss: 0.9314 | 1.9449
Epoch 251/300, seasonal_3 Loss: 0.9314 | 1.9449
Epoch 252/300, seasonal_3 Loss: 0.9314 | 1.9449
Epoch 253/300, seasonal_3 Loss: 0.9314 | 1.9450
Epoch 254/300, seasonal_3 Loss: 0.9314 | 1.9450
Epoch 255/300, seasonal_3 Loss: 0.9314 | 1.9450
Epoch 256/300, seasonal_3 Loss: 0.9314 | 1.9450
Epoch 257/300, seasonal_3 Loss: 0.9314 | 1.9450
Epoch 258/300, seasonal_3 Loss: 0.9314 | 1.9450
Epoch 259/300, seasonal_3 Loss: 0.9314 | 1.9450
Epoch 260/300, seasonal_3 Loss: 0.9314 | 1.9451
Epoch 261/300, seasonal_3 Loss: 0.9313 | 1.9451
Epoch 262/300, seasonal_3 Loss: 0.9313 | 1.9451
Epoch 263/300, seasonal_3 Loss: 0.9313 | 1.9451
Epoch 264/300, seasonal_3 Loss: 0.9313 | 1.9451
Epoch 265/300, seasonal_3 Loss: 0.9313 | 1.9451
Epoch 266/300, seasonal_3 Loss: 0.9313 | 1.9451
Epoch 267/300, seasonal_3 Loss: 0.9313 | 1.9451
Epoch 268/300, seasonal_3 Loss: 0.9313 | 1.9452
Epoch 269/300, seasonal_3 Loss: 0.9313 | 1.9452
Epoch 270/300, seasonal_3 Loss: 0.9313 | 1.9452
Epoch 271/300, seasonal_3 Loss: 0.9313 | 1.9452
Epoch 272/300, seasonal_3 Loss: 0.9313 | 1.9452
Epoch 273/300, seasonal_3 Loss: 0.9313 | 1.9452
Epoch 274/300, seasonal_3 Loss: 0.9313 | 1.9452
Epoch 275/300, seasonal_3 Loss: 0.9313 | 1.9452
Epoch 276/300, seasonal_3 Loss: 0.9312 | 1.9452
Epoch 277/300, seasonal_3 Loss: 0.9312 | 1.9452
Epoch 278/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 279/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 280/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 281/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 282/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 283/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 284/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 285/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 286/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 287/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 288/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 289/300, seasonal_3 Loss: 0.9312 | 1.9453
Epoch 290/300, seasonal_3 Loss: 0.9312 | 1.9454
Epoch 291/300, seasonal_3 Loss: 0.9312 | 1.9454
Epoch 292/300, seasonal_3 Loss: 0.9312 | 1.9454
Epoch 293/300, seasonal_3 Loss: 0.9312 | 1.9454
Epoch 294/300, seasonal_3 Loss: 0.9312 | 1.9454
Epoch 295/300, seasonal_3 Loss: 0.9312 | 1.9454
Epoch 296/300, seasonal_3 Loss: 0.9312 | 1.9454
Epoch 297/300, seasonal_3 Loss: 0.9312 | 1.9454
Epoch 298/300, seasonal_3 Loss: 0.9312 | 1.9454
Epoch 299/300, seasonal_3 Loss: 0.9312 | 1.9454
Epoch 300/300, seasonal_3 Loss: 0.9312 | 1.9454
Training resid component with params: {'observation_period_num': 39, 'train_rates': 0.8315510790338012, 'learning_rate': 0.00012725159713100268, 'batch_size': 79, 'step_size': 3, 'gamma': 0.9896631577781698}
Epoch 1/300, resid Loss: 0.7662 | 0.2460
Epoch 2/300, resid Loss: 0.2376 | 0.1409
Epoch 3/300, resid Loss: 0.1795 | 0.1042
Epoch 4/300, resid Loss: 0.1598 | 0.0960
Epoch 5/300, resid Loss: 0.1501 | 0.0993
Epoch 6/300, resid Loss: 0.1427 | 0.1134
Epoch 7/300, resid Loss: 0.1393 | 0.1158
Epoch 8/300, resid Loss: 0.1829 | 0.2095
Epoch 9/300, resid Loss: 0.2147 | 0.2983
Epoch 10/300, resid Loss: 0.2429 | 0.2578
Epoch 11/300, resid Loss: 0.1476 | 0.1205
Epoch 12/300, resid Loss: 0.1314 | 0.1790
Epoch 13/300, resid Loss: 0.1525 | 0.1310
Epoch 14/300, resid Loss: 0.1590 | 0.1011
Epoch 15/300, resid Loss: 0.1353 | 0.1271
Epoch 16/300, resid Loss: 0.1685 | 0.1111
Epoch 17/300, resid Loss: 0.1811 | 0.1616
Epoch 18/300, resid Loss: 0.1614 | 0.1487
Epoch 19/300, resid Loss: 0.1761 | 0.1241
Epoch 20/300, resid Loss: 0.1753 | 0.1419
Epoch 21/300, resid Loss: 0.2126 | 0.1608
Epoch 22/300, resid Loss: 0.1760 | 0.1295
Epoch 23/300, resid Loss: 0.1700 | 0.1304
Epoch 24/300, resid Loss: 0.1527 | 0.1047
Epoch 25/300, resid Loss: 0.1297 | 0.1238
Epoch 26/300, resid Loss: 0.1414 | 0.0926
Epoch 27/300, resid Loss: 0.1440 | 0.1444
Epoch 28/300, resid Loss: 0.1547 | 0.0793
Epoch 29/300, resid Loss: 0.1092 | 0.0992
Epoch 30/300, resid Loss: 0.1289 | 0.1365
Epoch 31/300, resid Loss: 0.1112 | 0.0792
Epoch 32/300, resid Loss: 0.1099 | 0.0839
Epoch 33/300, resid Loss: 0.1141 | 0.0933
Epoch 34/300, resid Loss: 0.1243 | 0.0796
Epoch 35/300, resid Loss: 0.1163 | 0.0851
Epoch 36/300, resid Loss: 0.1078 | 0.1185
Epoch 37/300, resid Loss: 0.0865 | 0.1625
Epoch 38/300, resid Loss: 0.1058 | 0.0982
Epoch 39/300, resid Loss: 0.1239 | 0.0819
Epoch 40/300, resid Loss: 0.1042 | 0.0787
Epoch 41/300, resid Loss: 0.0989 | 0.0994
Epoch 42/300, resid Loss: 0.0978 | 0.0912
Epoch 43/300, resid Loss: 0.1082 | 0.1170
Epoch 44/300, resid Loss: 0.0925 | 0.0740
Epoch 45/300, resid Loss: 0.0842 | 0.0732
Epoch 46/300, resid Loss: 0.0971 | 0.1027
Epoch 47/300, resid Loss: 0.0909 | 0.1394
Epoch 48/300, resid Loss: 0.0958 | 0.1004
Epoch 49/300, resid Loss: 0.0989 | 0.0837
Epoch 50/300, resid Loss: 0.1227 | 0.0705
Epoch 51/300, resid Loss: 0.1118 | 0.0956
Epoch 52/300, resid Loss: 0.1022 | 0.1142
Epoch 53/300, resid Loss: 0.0873 | 0.0571
Epoch 54/300, resid Loss: 0.0864 | 0.0804
Epoch 55/300, resid Loss: 0.0887 | 0.0747
Epoch 56/300, resid Loss: 0.0889 | 0.1286
Epoch 57/300, resid Loss: 0.0886 | 0.0881
Epoch 58/300, resid Loss: 0.0773 | 0.0788
Epoch 59/300, resid Loss: 0.0851 | 0.0746
Epoch 60/300, resid Loss: 0.0960 | 0.0855
Epoch 61/300, resid Loss: 0.0951 | 0.1297
Epoch 62/300, resid Loss: 0.0999 | 0.0867
Epoch 63/300, resid Loss: 0.0818 | 0.0748
Epoch 64/300, resid Loss: 0.0800 | 0.0697
Epoch 65/300, resid Loss: 0.0841 | 0.0771
Epoch 66/300, resid Loss: 0.0889 | 0.1058
Epoch 67/300, resid Loss: 0.0914 | 0.0963
Epoch 68/300, resid Loss: 0.0951 | 0.1502
Epoch 69/300, resid Loss: 0.0822 | 0.0725
Epoch 70/300, resid Loss: 0.0813 | 0.0738
Epoch 71/300, resid Loss: 0.0939 | 0.0802
Epoch 72/300, resid Loss: 0.0883 | 0.0849
Epoch 73/300, resid Loss: 0.0871 | 0.0860
Epoch 74/300, resid Loss: 0.0797 | 0.1286
Epoch 75/300, resid Loss: 0.1054 | 0.1183
Epoch 76/300, resid Loss: 0.0805 | 0.0699
Epoch 77/300, resid Loss: 0.0834 | 0.0745
Epoch 78/300, resid Loss: 0.0771 | 0.1064
Epoch 79/300, resid Loss: 0.0689 | 0.0759
Epoch 80/300, resid Loss: 0.0632 | 0.0783
Epoch 81/300, resid Loss: 0.0760 | 0.0783
Epoch 82/300, resid Loss: 0.0798 | 0.0581
Epoch 83/300, resid Loss: 0.0739 | 0.0599
Epoch 84/300, resid Loss: 0.0602 | 0.0798
Epoch 85/300, resid Loss: 0.0727 | 0.1300
Epoch 86/300, resid Loss: 0.0860 | 0.1345
Epoch 87/300, resid Loss: 0.0792 | 0.0869
Epoch 88/300, resid Loss: 0.0879 | 0.0681
Epoch 89/300, resid Loss: 0.0908 | 0.0778
Epoch 90/300, resid Loss: 0.0769 | 0.1223
Epoch 91/300, resid Loss: 0.0641 | 0.0770
Epoch 92/300, resid Loss: 0.0642 | 0.0659
Epoch 93/300, resid Loss: 0.0760 | 0.0816
Epoch 94/300, resid Loss: 0.0727 | 0.0561
Epoch 95/300, resid Loss: 0.0627 | 0.0780
Epoch 96/300, resid Loss: 0.0827 | 0.1038
Epoch 97/300, resid Loss: 0.0679 | 0.1152
Epoch 98/300, resid Loss: 0.0769 | 0.0986
Epoch 99/300, resid Loss: 0.0735 | 0.0817
Epoch 100/300, resid Loss: 0.0782 | 0.0769
Epoch 101/300, resid Loss: 0.0812 | 0.1033
Epoch 102/300, resid Loss: 0.0551 | 0.0644
Epoch 103/300, resid Loss: 0.0664 | 0.0740
Epoch 104/300, resid Loss: 0.0683 | 0.1116
Epoch 105/300, resid Loss: 0.0729 | 0.1155
Epoch 106/300, resid Loss: 0.0645 | 0.0878
Epoch 107/300, resid Loss: 0.0578 | 0.0634
Epoch 108/300, resid Loss: 0.0611 | 0.0712
Epoch 109/300, resid Loss: 0.0675 | 0.0693
Epoch 110/300, resid Loss: 0.0528 | 0.0798
Epoch 111/300, resid Loss: 0.0564 | 0.0752
Epoch 112/300, resid Loss: 0.0589 | 0.1146
Epoch 113/300, resid Loss: 0.0595 | 0.0764
Epoch 114/300, resid Loss: 0.0626 | 0.0667
Epoch 115/300, resid Loss: 0.0593 | 0.0781
Epoch 116/300, resid Loss: 0.0674 | 0.0831
Epoch 117/300, resid Loss: 0.0531 | 0.1005
Epoch 118/300, resid Loss: 0.0679 | 0.0781
Epoch 119/300, resid Loss: 0.0733 | 0.0651
Epoch 120/300, resid Loss: 0.0738 | 0.0866
Epoch 121/300, resid Loss: 0.0520 | 0.0931
Epoch 122/300, resid Loss: 0.0505 | 0.0614
Epoch 123/300, resid Loss: 0.0463 | 0.0863
Epoch 124/300, resid Loss: 0.0475 | 0.1060
Epoch 125/300, resid Loss: 0.0533 | 0.0639
Epoch 126/300, resid Loss: 0.0514 | 0.0675
Epoch 127/300, resid Loss: 0.0573 | 0.0717
Epoch 128/300, resid Loss: 0.0518 | 0.0850
Epoch 129/300, resid Loss: 0.0538 | 0.0668
Epoch 130/300, resid Loss: 0.0481 | 0.0827
Epoch 131/300, resid Loss: 0.0454 | 0.0879
Epoch 132/300, resid Loss: 0.0548 | 0.0581
Epoch 133/300, resid Loss: 0.0496 | 0.0686
Epoch 134/300, resid Loss: 0.0504 | 0.0826
Epoch 135/300, resid Loss: 0.0667 | 0.1115
Epoch 136/300, resid Loss: 0.0574 | 0.0649
Epoch 137/300, resid Loss: 0.0638 | 0.0624
Epoch 138/300, resid Loss: 0.0663 | 0.0690
Epoch 139/300, resid Loss: 0.0560 | 0.0723
Epoch 140/300, resid Loss: 0.0596 | 0.0681
Epoch 141/300, resid Loss: 0.0620 | 0.0659
Epoch 142/300, resid Loss: 0.0505 | 0.0985
Epoch 143/300, resid Loss: 0.0480 | 0.0882
Epoch 144/300, resid Loss: 0.0579 | 0.0637
Epoch 145/300, resid Loss: 0.0458 | 0.0834
Epoch 146/300, resid Loss: 0.0494 | 0.0997
Epoch 147/300, resid Loss: 0.0486 | 0.0897
Epoch 148/300, resid Loss: 0.0489 | 0.0609
Epoch 149/300, resid Loss: 0.0549 | 0.0638
Epoch 150/300, resid Loss: 0.0560 | 0.0944
Epoch 151/300, resid Loss: 0.0448 | 0.0688
Epoch 152/300, resid Loss: 0.0530 | 0.0669
Epoch 153/300, resid Loss: 0.0523 | 0.0832
Epoch 154/300, resid Loss: 0.0485 | 0.0864
Epoch 155/300, resid Loss: 0.0404 | 0.0713
Epoch 156/300, resid Loss: 0.0368 | 0.0635
Epoch 157/300, resid Loss: 0.0352 | 0.0758
Epoch 158/300, resid Loss: 0.0340 | 0.0836
Epoch 159/300, resid Loss: 0.0349 | 0.0710
Epoch 160/300, resid Loss: 0.0340 | 0.0624
Epoch 161/300, resid Loss: 0.0346 | 0.0640
Epoch 162/300, resid Loss: 0.0362 | 0.0652
Epoch 163/300, resid Loss: 0.0350 | 0.0668
Epoch 164/300, resid Loss: 0.0380 | 0.0847
Epoch 165/300, resid Loss: 0.0378 | 0.0824
Epoch 166/300, resid Loss: 0.0377 | 0.0629
Epoch 167/300, resid Loss: 0.0359 | 0.0610
Epoch 168/300, resid Loss: 0.0401 | 0.0701
Epoch 169/300, resid Loss: 0.0352 | 0.0774
Epoch 170/300, resid Loss: 0.0355 | 0.0751
Epoch 171/300, resid Loss: 0.0365 | 0.0711
Epoch 172/300, resid Loss: 0.0354 | 0.0696
Epoch 173/300, resid Loss: 0.0376 | 0.0594
Epoch 174/300, resid Loss: 0.0357 | 0.0611
Epoch 175/300, resid Loss: 0.0387 | 0.0798
Epoch 176/300, resid Loss: 0.0376 | 0.0931
Epoch 177/300, resid Loss: 0.0367 | 0.0762
Epoch 178/300, resid Loss: 0.0360 | 0.0584
Epoch 179/300, resid Loss: 0.0368 | 0.0638
Epoch 180/300, resid Loss: 0.0371 | 0.0714
Epoch 181/300, resid Loss: 0.0331 | 0.0731
Epoch 182/300, resid Loss: 0.0402 | 0.0796
Epoch 183/300, resid Loss: 0.0407 | 0.0778
Epoch 184/300, resid Loss: 0.0369 | 0.0727
Epoch 185/300, resid Loss: 0.0362 | 0.0609
Epoch 186/300, resid Loss: 0.0374 | 0.0650
Epoch 187/300, resid Loss: 0.0362 | 0.0803
Epoch 188/300, resid Loss: 0.0365 | 0.1043
Epoch 189/300, resid Loss: 0.0357 | 0.0798
Epoch 190/300, resid Loss: 0.0362 | 0.0564
Epoch 191/300, resid Loss: 0.0370 | 0.0700
Epoch 192/300, resid Loss: 0.0417 | 0.0848
Epoch 193/300, resid Loss: 0.0367 | 0.0711
Epoch 194/300, resid Loss: 0.0445 | 0.0841
Epoch 195/300, resid Loss: 0.0422 | 0.0795
Epoch 196/300, resid Loss: 0.0389 | 0.0733
Epoch 197/300, resid Loss: 0.0333 | 0.0622
Epoch 198/300, resid Loss: 0.0346 | 0.0670
Epoch 199/300, resid Loss: 0.0318 | 0.0737
Epoch 200/300, resid Loss: 0.0314 | 0.0755
Epoch 201/300, resid Loss: 0.0322 | 0.0750
Epoch 202/300, resid Loss: 0.0311 | 0.0704
Epoch 203/300, resid Loss: 0.0309 | 0.0661
Epoch 204/300, resid Loss: 0.0312 | 0.0643
Epoch 205/300, resid Loss: 0.0308 | 0.0657
Epoch 206/300, resid Loss: 0.0311 | 0.0782
Epoch 207/300, resid Loss: 0.0309 | 0.0842
Epoch 208/300, resid Loss: 0.0304 | 0.0723
Epoch 209/300, resid Loss: 0.0303 | 0.0619
Epoch 210/300, resid Loss: 0.0311 | 0.0643
Epoch 211/300, resid Loss: 0.0312 | 0.0712
Epoch 212/300, resid Loss: 0.0300 | 0.0741
Epoch 213/300, resid Loss: 0.0329 | 0.0764
Epoch 214/300, resid Loss: 0.0320 | 0.0768
Epoch 215/300, resid Loss: 0.0320 | 0.0713
Epoch 216/300, resid Loss: 0.0323 | 0.0611
Epoch 217/300, resid Loss: 0.0329 | 0.0655
Epoch 218/300, resid Loss: 0.0328 | 0.0748
Epoch 219/300, resid Loss: 0.0329 | 0.0965
Epoch 220/300, resid Loss: 0.0332 | 0.0882
Epoch 221/300, resid Loss: 0.0332 | 0.0604
Epoch 222/300, resid Loss: 0.0337 | 0.0678
Epoch 223/300, resid Loss: 0.0353 | 0.0801
Epoch 224/300, resid Loss: 0.0324 | 0.0709
Epoch 225/300, resid Loss: 0.0381 | 0.0776
Epoch 226/300, resid Loss: 0.0376 | 0.0773
Epoch 227/300, resid Loss: 0.0370 | 0.0787
Epoch 228/300, resid Loss: 0.0335 | 0.0753
Epoch 229/300, resid Loss: 0.0329 | 0.0619
Epoch 230/300, resid Loss: 0.0320 | 0.0688
Epoch 231/300, resid Loss: 0.0309 | 0.0863
Epoch 232/300, resid Loss: 0.0310 | 0.0743
Epoch 233/300, resid Loss: 0.0307 | 0.0644
Epoch 234/300, resid Loss: 0.0296 | 0.0683
Epoch 235/300, resid Loss: 0.0304 | 0.0710
Epoch 236/300, resid Loss: 0.0295 | 0.0651
Epoch 237/300, resid Loss: 0.0301 | 0.0682
Epoch 238/300, resid Loss: 0.0299 | 0.0762
Epoch 239/300, resid Loss: 0.0300 | 0.0800
Epoch 240/300, resid Loss: 0.0293 | 0.0668
Epoch 241/300, resid Loss: 0.0296 | 0.0615
Epoch 242/300, resid Loss: 0.0299 | 0.0653
Epoch 243/300, resid Loss: 0.0293 | 0.0757
Epoch 244/300, resid Loss: 0.0289 | 0.0744
Epoch 245/300, resid Loss: 0.0299 | 0.0711
Epoch 246/300, resid Loss: 0.0293 | 0.0708
Epoch 247/300, resid Loss: 0.0296 | 0.0691
Epoch 248/300, resid Loss: 0.0294 | 0.0623
Epoch 249/300, resid Loss: 0.0309 | 0.0628
Epoch 250/300, resid Loss: 0.0295 | 0.0721
Epoch 251/300, resid Loss: 0.0302 | 0.0906
Epoch 252/300, resid Loss: 0.0303 | 0.0777
Epoch 253/300, resid Loss: 0.0307 | 0.0591
Epoch 254/300, resid Loss: 0.0308 | 0.0659
Epoch 255/300, resid Loss: 0.0326 | 0.0770
Epoch 256/300, resid Loss: 0.0300 | 0.0698
Epoch 257/300, resid Loss: 0.0344 | 0.0745
Epoch 258/300, resid Loss: 0.0340 | 0.0771
Epoch 259/300, resid Loss: 0.0337 | 0.0837
Epoch 260/300, resid Loss: 0.0307 | 0.0655
Epoch 261/300, resid Loss: 0.0335 | 0.0621
Epoch 262/300, resid Loss: 0.0308 | 0.0726
Epoch 263/300, resid Loss: 0.0309 | 0.0915
Epoch 264/300, resid Loss: 0.0304 | 0.0689
Epoch 265/300, resid Loss: 0.0300 | 0.0637
Epoch 266/300, resid Loss: 0.0294 | 0.0728
Epoch 267/300, resid Loss: 0.0302 | 0.0731
Epoch 268/300, resid Loss: 0.0292 | 0.0638
Epoch 269/300, resid Loss: 0.0297 | 0.0767
Epoch 270/300, resid Loss: 0.0291 | 0.0853
Epoch 271/300, resid Loss: 0.0289 | 0.0692
Epoch 272/300, resid Loss: 0.0282 | 0.0658
Epoch 273/300, resid Loss: 0.0289 | 0.0695
Epoch 274/300, resid Loss: 0.0281 | 0.0708
Epoch 275/300, resid Loss: 0.0278 | 0.0703
Epoch 276/300, resid Loss: 0.0283 | 0.0748
Epoch 277/300, resid Loss: 0.0279 | 0.0741
Epoch 278/300, resid Loss: 0.0282 | 0.0676
Epoch 279/300, resid Loss: 0.0277 | 0.0641
Epoch 280/300, resid Loss: 0.0285 | 0.0685
Epoch 281/300, resid Loss: 0.0277 | 0.0762
Epoch 282/300, resid Loss: 0.0278 | 0.0783
Epoch 283/300, resid Loss: 0.0277 | 0.0676
Epoch 284/300, resid Loss: 0.0273 | 0.0688
Epoch 285/300, resid Loss: 0.0279 | 0.0694
Epoch 286/300, resid Loss: 0.0279 | 0.0665
Epoch 287/300, resid Loss: 0.0282 | 0.0670
Epoch 288/300, resid Loss: 0.0288 | 0.0789
Epoch 289/300, resid Loss: 0.0287 | 0.0823
Epoch 290/300, resid Loss: 0.0286 | 0.0684
Epoch 291/300, resid Loss: 0.0284 | 0.0623
Epoch 292/300, resid Loss: 0.0293 | 0.0716
Epoch 293/300, resid Loss: 0.0284 | 0.0792
Epoch 294/300, resid Loss: 0.0283 | 0.0767
Epoch 295/300, resid Loss: 0.0288 | 0.0690
Epoch 296/300, resid Loss: 0.0282 | 0.0720
Epoch 297/300, resid Loss: 0.0294 | 0.0724
Epoch 298/300, resid Loss: 0.0283 | 0.0653
Epoch 299/300, resid Loss: 0.0303 | 0.0677
Epoch 300/300, resid Loss: 0.0294 | 0.0787
Runtime (seconds): 8430.6585521698
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[162.4934]
[-5.2564683]
[2.0091732]
[-0.8394045]
[-0.9226643]
[14.9972515]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1106.1429899968207
RMSE: 33.25872802734375
MAE: 33.25872802734375
R-squared: nan
[172.48128]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
