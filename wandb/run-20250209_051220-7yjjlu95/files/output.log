ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-09 05:12:25,274][0m A new study created in memory with name: no-name-e18184de-9815-4964-8e9b-6a230d7d7a4b[0m
Early stopping at epoch 44
[32m[I 2025-02-09 05:12:42,473][0m Trial 0 finished with value: 1.0340984483215392 and parameters: {'observation_period_num': 23, 'train_rates': 0.8237265328997488, 'learning_rate': 1.1302283051959488e-05, 'batch_size': 158, 'step_size': 1, 'gamma': 0.7619987611351633}. Best is trial 0 with value: 1.0340984483215392.[0m
[32m[I 2025-02-09 05:13:20,619][0m Trial 1 finished with value: 0.2398426103987002 and parameters: {'observation_period_num': 132, 'train_rates': 0.6618763915780742, 'learning_rate': 0.0007059051497517295, 'batch_size': 127, 'step_size': 5, 'gamma': 0.9747869586852786}. Best is trial 1 with value: 0.2398426103987002.[0m
[32m[I 2025-02-09 05:14:39,020][0m Trial 2 finished with value: 0.4079697643850949 and parameters: {'observation_period_num': 118, 'train_rates': 0.7785337512494042, 'learning_rate': 6.7268075141560324e-06, 'batch_size': 65, 'step_size': 10, 'gamma': 0.9117908030903392}. Best is trial 1 with value: 0.2398426103987002.[0m
[32m[I 2025-02-09 05:16:05,141][0m Trial 3 finished with value: 1.029005616903305 and parameters: {'observation_period_num': 84, 'train_rates': 0.95348362258403, 'learning_rate': 1.5810059170661337e-06, 'batch_size': 67, 'step_size': 2, 'gamma': 0.8784143849771751}. Best is trial 1 with value: 0.2398426103987002.[0m
[32m[I 2025-02-09 05:17:04,996][0m Trial 4 finished with value: 0.5582237243652344 and parameters: {'observation_period_num': 231, 'train_rates': 0.9789595855302891, 'learning_rate': 1.1801940985655145e-06, 'batch_size': 99, 'step_size': 12, 'gamma': 0.861770069188131}. Best is trial 1 with value: 0.2398426103987002.[0m
[32m[I 2025-02-09 05:18:17,677][0m Trial 5 finished with value: 0.8302367372172219 and parameters: {'observation_period_num': 130, 'train_rates': 0.6841621518854404, 'learning_rate': 1.8106286103391676e-06, 'batch_size': 64, 'step_size': 3, 'gamma': 0.9454666967725524}. Best is trial 1 with value: 0.2398426103987002.[0m
[32m[I 2025-02-09 05:21:05,269][0m Trial 6 finished with value: 0.15514083311309793 and parameters: {'observation_period_num': 12, 'train_rates': 0.7317579528765394, 'learning_rate': 0.000433190303420198, 'batch_size': 29, 'step_size': 6, 'gamma': 0.9772477252018845}. Best is trial 6 with value: 0.15514083311309793.[0m
[32m[I 2025-02-09 05:22:56,537][0m Trial 7 finished with value: 0.4629192556981202 and parameters: {'observation_period_num': 138, 'train_rates': 0.7427236862982423, 'learning_rate': 5.994311897969434e-06, 'batch_size': 44, 'step_size': 5, 'gamma': 0.9420775272474896}. Best is trial 6 with value: 0.15514083311309793.[0m
[32m[I 2025-02-09 05:23:41,859][0m Trial 8 finished with value: 3.6754178024515154 and parameters: {'observation_period_num': 170, 'train_rates': 0.8045717006698637, 'learning_rate': 1.0315000927681567e-06, 'batch_size': 116, 'step_size': 5, 'gamma': 0.807069306700845}. Best is trial 6 with value: 0.15514083311309793.[0m
[32m[I 2025-02-09 05:24:34,996][0m Trial 9 finished with value: 0.6354625225067139 and parameters: {'observation_period_num': 202, 'train_rates': 0.9662099879232293, 'learning_rate': 2.4670604729663767e-06, 'batch_size': 107, 'step_size': 13, 'gamma': 0.9458019315150332}. Best is trial 6 with value: 0.15514083311309793.[0m
[32m[I 2025-02-09 05:24:56,754][0m Trial 10 finished with value: 0.12421476058094634 and parameters: {'observation_period_num': 6, 'train_rates': 0.6005632567789971, 'learning_rate': 0.0002780802599659599, 'batch_size': 228, 'step_size': 9, 'gamma': 0.8497171805658478}. Best is trial 10 with value: 0.12421476058094634.[0m
[32m[I 2025-02-09 05:25:17,460][0m Trial 11 finished with value: 0.15806078853050365 and parameters: {'observation_period_num': 18, 'train_rates': 0.6236363384432593, 'learning_rate': 0.00032773226819210265, 'batch_size': 252, 'step_size': 8, 'gamma': 0.8400502857714569}. Best is trial 10 with value: 0.12421476058094634.[0m
[32m[I 2025-02-09 05:25:40,888][0m Trial 12 finished with value: 0.2043883872762438 and parameters: {'observation_period_num': 57, 'train_rates': 0.6014150150585689, 'learning_rate': 0.00011933407475808412, 'batch_size': 200, 'step_size': 8, 'gamma': 0.8154095771411148}. Best is trial 10 with value: 0.12421476058094634.[0m
[32m[I 2025-02-09 05:31:21,921][0m Trial 13 finished with value: 0.0321120023727417 and parameters: {'observation_period_num': 7, 'train_rates': 0.8758546185409937, 'learning_rate': 9.007966235471077e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9028563556732473}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:31:55,680][0m Trial 14 finished with value: 0.0876061587597213 and parameters: {'observation_period_num': 58, 'train_rates': 0.8841336054034519, 'learning_rate': 6.995617931988553e-05, 'batch_size': 182, 'step_size': 15, 'gamma': 0.8984142947352715}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:32:34,085][0m Trial 15 finished with value: 0.10567888461525447 and parameters: {'observation_period_num': 67, 'train_rates': 0.8748116626575902, 'learning_rate': 4.674099259766324e-05, 'batch_size': 155, 'step_size': 15, 'gamma': 0.8959564160173006}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:33:07,689][0m Trial 16 finished with value: 0.09078025072813034 and parameters: {'observation_period_num': 49, 'train_rates': 0.8930539665059491, 'learning_rate': 5.1303412204852154e-05, 'batch_size': 180, 'step_size': 15, 'gamma': 0.9116406750516696}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:33:38,066][0m Trial 17 finished with value: 0.10968979788537057 and parameters: {'observation_period_num': 94, 'train_rates': 0.8951090874046302, 'learning_rate': 9.891596250651891e-05, 'batch_size': 200, 'step_size': 13, 'gamma': 0.88763796061206}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:34:16,197][0m Trial 18 finished with value: 0.11990529226858428 and parameters: {'observation_period_num': 44, 'train_rates': 0.8619436960726399, 'learning_rate': 2.1079378320275504e-05, 'batch_size': 151, 'step_size': 11, 'gamma': 0.9288568124109913}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:38:45,018][0m Trial 19 finished with value: 0.16824139975049954 and parameters: {'observation_period_num': 94, 'train_rates': 0.9329841147136523, 'learning_rate': 0.0001324049829510221, 'batch_size': 21, 'step_size': 14, 'gamma': 0.8269448727751983}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:39:51,758][0m Trial 20 finished with value: 0.07981123404313567 and parameters: {'observation_period_num': 37, 'train_rates': 0.9206255549663528, 'learning_rate': 2.5430012315910172e-05, 'batch_size': 87, 'step_size': 12, 'gamma': 0.7617346244613994}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:41:06,021][0m Trial 21 finished with value: 0.08141657272190378 and parameters: {'observation_period_num': 37, 'train_rates': 0.9222652838238298, 'learning_rate': 2.122692850431271e-05, 'batch_size': 79, 'step_size': 15, 'gamma': 0.7602376243517336}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:42:13,762][0m Trial 22 finished with value: 0.10407523728079265 and parameters: {'observation_period_num': 34, 'train_rates': 0.920297532606512, 'learning_rate': 2.0312096918554387e-05, 'batch_size': 89, 'step_size': 13, 'gamma': 0.7511108096973916}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:43:21,468][0m Trial 23 finished with value: 0.10705912838979013 and parameters: {'observation_period_num': 78, 'train_rates': 0.8395530349281431, 'learning_rate': 2.8578772610529393e-05, 'batch_size': 83, 'step_size': 12, 'gamma': 0.7835807305631659}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:45:22,761][0m Trial 24 finished with value: 0.07846885890914844 and parameters: {'observation_period_num': 28, 'train_rates': 0.9294041166923227, 'learning_rate': 1.2803949552977606e-05, 'batch_size': 48, 'step_size': 14, 'gamma': 0.7823200676628479}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:47:28,864][0m Trial 25 finished with value: 0.08188075291613738 and parameters: {'observation_period_num': 34, 'train_rates': 0.8465715529340071, 'learning_rate': 1.051900513962304e-05, 'batch_size': 43, 'step_size': 11, 'gamma': 0.7847264176112694}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:49:47,417][0m Trial 26 finished with value: 0.1266223385845874 and parameters: {'observation_period_num': 7, 'train_rates': 0.9403185302709463, 'learning_rate': 3.546769022782707e-06, 'batch_size': 43, 'step_size': 14, 'gamma': 0.7838057260975121}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 05:54:45,502][0m Trial 27 finished with value: 0.04708051376652961 and parameters: {'observation_period_num': 28, 'train_rates': 0.9165904381719047, 'learning_rate': 0.0001786319090611037, 'batch_size': 19, 'step_size': 12, 'gamma': 0.8012909865610228}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:00:07,931][0m Trial 28 finished with value: 0.21550602071425495 and parameters: {'observation_period_num': 111, 'train_rates': 0.9882977389805838, 'learning_rate': 0.0001997478009144421, 'batch_size': 18, 'step_size': 14, 'gamma': 0.8009618381864185}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:02:44,495][0m Trial 29 finished with value: 0.19222667323168374 and parameters: {'observation_period_num': 157, 'train_rates': 0.8117758576070723, 'learning_rate': 0.0009191348559642194, 'batch_size': 32, 'step_size': 10, 'gamma': 0.8359271010122449}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:04:19,482][0m Trial 30 finished with value: 0.1748286108308621 and parameters: {'observation_period_num': 22, 'train_rates': 0.7775433330099131, 'learning_rate': 0.00017206954266872388, 'batch_size': 55, 'step_size': 13, 'gamma': 0.8599919841623751}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:09:17,353][0m Trial 31 finished with value: 0.06715294157757479 and parameters: {'observation_period_num': 34, 'train_rates': 0.91318946325971, 'learning_rate': 1.0257145067019753e-05, 'batch_size': 19, 'step_size': 12, 'gamma': 0.7670856815187961}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:14:46,331][0m Trial 32 finished with value: 0.05512739329699527 and parameters: {'observation_period_num': 24, 'train_rates': 0.8996079626065142, 'learning_rate': 1.2800935911988066e-05, 'batch_size': 17, 'step_size': 11, 'gamma': 0.7910651891872774}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:20:13,449][0m Trial 33 finished with value: 0.0768155099875811 and parameters: {'observation_period_num': 69, 'train_rates': 0.9006443032769877, 'learning_rate': 1.203370034215421e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.7970578678973596}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:22:56,304][0m Trial 34 finished with value: 0.06259392523885372 and parameters: {'observation_period_num': 6, 'train_rates': 0.8657277937704685, 'learning_rate': 6.898487390491426e-06, 'batch_size': 34, 'step_size': 11, 'gamma': 0.8207967176725791}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:25:36,051][0m Trial 35 finished with value: 0.07797048250098287 and parameters: {'observation_period_num': 9, 'train_rates': 0.8358295908268238, 'learning_rate': 6.1148805726816505e-06, 'batch_size': 34, 'step_size': 7, 'gamma': 0.8181120839697708}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:27:03,469][0m Trial 36 finished with value: 0.4166280615628596 and parameters: {'observation_period_num': 252, 'train_rates': 0.8679884136217652, 'learning_rate': 3.953064256655e-06, 'batch_size': 59, 'step_size': 11, 'gamma': 0.8773659141032152}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:27:51,505][0m Trial 37 finished with value: 0.09480875730514526 and parameters: {'observation_period_num': 25, 'train_rates': 0.9640102403419083, 'learning_rate': 4.887761162483513e-05, 'batch_size': 134, 'step_size': 9, 'gamma': 0.8283975108988253}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:29:06,305][0m Trial 38 finished with value: 0.19785558471554204 and parameters: {'observation_period_num': 52, 'train_rates': 0.7719122568749579, 'learning_rate': 0.0005302199999470248, 'batch_size': 70, 'step_size': 9, 'gamma': 0.8485842499917899}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:31:53,264][0m Trial 39 finished with value: 0.035414627394339905 and parameters: {'observation_period_num': 5, 'train_rates': 0.8527351585489894, 'learning_rate': 7.041946505773735e-05, 'batch_size': 33, 'step_size': 11, 'gamma': 0.8091470619694044}. Best is trial 13 with value: 0.0321120023727417.[0m
Early stopping at epoch 59
[32m[I 2025-02-09 06:32:55,153][0m Trial 40 finished with value: 0.14531275771913074 and parameters: {'observation_period_num': 19, 'train_rates': 0.822039873607235, 'learning_rate': 6.323788349243986e-05, 'batch_size': 53, 'step_size': 1, 'gamma': 0.8063034664517021}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:35:51,604][0m Trial 41 finished with value: 0.03280208662661882 and parameters: {'observation_period_num': 7, 'train_rates': 0.8568365381862441, 'learning_rate': 7.836075184108837e-05, 'batch_size': 31, 'step_size': 11, 'gamma': 0.792451611098174}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:38:53,469][0m Trial 42 finished with value: 0.03997188692583757 and parameters: {'observation_period_num': 19, 'train_rates': 0.8558836538326597, 'learning_rate': 7.99800610723401e-05, 'batch_size': 30, 'step_size': 10, 'gamma': 0.7723474862080614}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:41:51,003][0m Trial 43 finished with value: 0.034216982318864865 and parameters: {'observation_period_num': 13, 'train_rates': 0.8514212899619789, 'learning_rate': 8.464534621801675e-05, 'batch_size': 31, 'step_size': 10, 'gamma': 0.7714011114081184}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:43:11,109][0m Trial 44 finished with value: 0.05836059930093132 and parameters: {'observation_period_num': 14, 'train_rates': 0.8499589015855629, 'learning_rate': 8.744330515878902e-05, 'batch_size': 71, 'step_size': 7, 'gamma': 0.7766739346148902}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:45:48,111][0m Trial 45 finished with value: 0.041377840450194284 and parameters: {'observation_period_num': 6, 'train_rates': 0.8003834340436942, 'learning_rate': 3.819868110403439e-05, 'batch_size': 33, 'step_size': 10, 'gamma': 0.7623804669906848}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:47:33,327][0m Trial 46 finished with value: 0.2536015037733775 and parameters: {'observation_period_num': 185, 'train_rates': 0.7570569749992921, 'learning_rate': 7.299048781924748e-05, 'batch_size': 45, 'step_size': 8, 'gamma': 0.750866448686602}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:48:29,814][0m Trial 47 finished with value: 0.054212172470655386 and parameters: {'observation_period_num': 46, 'train_rates': 0.8230847493391457, 'learning_rate': 0.00012868430577261044, 'batch_size': 98, 'step_size': 4, 'gamma': 0.9159521448414962}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:49:47,095][0m Trial 48 finished with value: 0.16768275662200083 and parameters: {'observation_period_num': 65, 'train_rates': 0.7181695695404603, 'learning_rate': 0.00026629205632752967, 'batch_size': 64, 'step_size': 9, 'gamma': 0.7702025857434048}. Best is trial 13 with value: 0.0321120023727417.[0m
[32m[I 2025-02-09 06:50:36,980][0m Trial 49 finished with value: 0.20842088207050605 and parameters: {'observation_period_num': 17, 'train_rates': 0.7893554695475054, 'learning_rate': 3.618630881073547e-05, 'batch_size': 115, 'step_size': 10, 'gamma': 0.773652455031248}. Best is trial 13 with value: 0.0321120023727417.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-09 06:50:36,990][0m A new study created in memory with name: no-name-40e87bf2-348d-45cf-b01d-7ea3e9c093d4[0m
[32m[I 2025-02-09 06:54:07,646][0m Trial 0 finished with value: 0.1957151433772687 and parameters: {'observation_period_num': 75, 'train_rates': 0.6877449412358726, 'learning_rate': 1.5761891438108274e-05, 'batch_size': 22, 'step_size': 12, 'gamma': 0.9303310301358231}. Best is trial 0 with value: 0.1957151433772687.[0m
[32m[I 2025-02-09 06:55:14,933][0m Trial 1 finished with value: 0.26754911216019467 and parameters: {'observation_period_num': 28, 'train_rates': 0.7819005878778353, 'learning_rate': 5.363105723434582e-06, 'batch_size': 77, 'step_size': 14, 'gamma': 0.9083607776511258}. Best is trial 0 with value: 0.1957151433772687.[0m
[32m[I 2025-02-09 06:55:40,296][0m Trial 2 finished with value: 0.3074770394925009 and parameters: {'observation_period_num': 179, 'train_rates': 0.722996013647931, 'learning_rate': 0.0007545415052473569, 'batch_size': 213, 'step_size': 2, 'gamma': 0.8506240969446083}. Best is trial 0 with value: 0.1957151433772687.[0m
[32m[I 2025-02-09 06:56:29,760][0m Trial 3 finished with value: 0.32823391636083654 and parameters: {'observation_period_num': 123, 'train_rates': 0.8377150384328873, 'learning_rate': 4.0171867265336923e-05, 'batch_size': 113, 'step_size': 3, 'gamma': 0.7574411454466672}. Best is trial 0 with value: 0.1957151433772687.[0m
[32m[I 2025-02-09 06:57:29,442][0m Trial 4 finished with value: 0.15377499067783357 and parameters: {'observation_period_num': 165, 'train_rates': 0.9555810965145213, 'learning_rate': 0.00034646430389214826, 'batch_size': 99, 'step_size': 6, 'gamma': 0.7659212409306159}. Best is trial 4 with value: 0.15377499067783357.[0m
[32m[I 2025-02-09 06:59:12,498][0m Trial 5 finished with value: 0.10410060106139434 and parameters: {'observation_period_num': 98, 'train_rates': 0.9470578070186533, 'learning_rate': 5.2455311051800675e-05, 'batch_size': 56, 'step_size': 10, 'gamma': 0.8782140269891392}. Best is trial 5 with value: 0.10410060106139434.[0m
[32m[I 2025-02-09 06:59:56,537][0m Trial 6 finished with value: 0.5878396573741551 and parameters: {'observation_period_num': 220, 'train_rates': 0.7660324977442645, 'learning_rate': 9.673850224677299e-06, 'batch_size': 113, 'step_size': 5, 'gamma': 0.8943596022890795}. Best is trial 5 with value: 0.10410060106139434.[0m
[32m[I 2025-02-09 07:00:35,131][0m Trial 7 finished with value: 0.22649991656065854 and parameters: {'observation_period_num': 93, 'train_rates': 0.7132191216414827, 'learning_rate': 0.00027698693156325634, 'batch_size': 131, 'step_size': 3, 'gamma': 0.8266802130162477}. Best is trial 5 with value: 0.10410060106139434.[0m
Early stopping at epoch 82
[32m[I 2025-02-09 07:00:56,050][0m Trial 8 finished with value: 1.736699330207356 and parameters: {'observation_period_num': 56, 'train_rates': 0.6924696424093241, 'learning_rate': 4.103563321031921e-06, 'batch_size': 222, 'step_size': 1, 'gamma': 0.8745155482028866}. Best is trial 5 with value: 0.10410060106139434.[0m
[32m[I 2025-02-09 07:01:33,698][0m Trial 9 finished with value: 0.7916424230776524 and parameters: {'observation_period_num': 223, 'train_rates': 0.7585458363152291, 'learning_rate': 3.3021060375245835e-06, 'batch_size': 133, 'step_size': 11, 'gamma': 0.8327062219806146}. Best is trial 5 with value: 0.10410060106139434.[0m
[32m[I 2025-02-09 07:04:21,410][0m Trial 10 finished with value: 0.04130531517121027 and parameters: {'observation_period_num': 17, 'train_rates': 0.982314727103772, 'learning_rate': 6.319456892487084e-05, 'batch_size': 36, 'step_size': 9, 'gamma': 0.9767411512558213}. Best is trial 10 with value: 0.04130531517121027.[0m
[32m[I 2025-02-09 07:07:49,827][0m Trial 11 finished with value: 0.0452210320667787 and parameters: {'observation_period_num': 9, 'train_rates': 0.9854301005160492, 'learning_rate': 7.422197896840315e-05, 'batch_size': 29, 'step_size': 9, 'gamma': 0.9857702211313252}. Best is trial 10 with value: 0.04130531517121027.[0m
[32m[I 2025-02-09 07:12:12,425][0m Trial 12 finished with value: 0.05128570031421611 and parameters: {'observation_period_num': 7, 'train_rates': 0.8660067170136965, 'learning_rate': 0.00011664357977518836, 'batch_size': 21, 'step_size': 8, 'gamma': 0.9796749384285288}. Best is trial 10 with value: 0.04130531517121027.[0m
[32m[I 2025-02-09 07:12:47,902][0m Trial 13 finished with value: 0.6166683435440063 and parameters: {'observation_period_num': 30, 'train_rates': 0.9864264294268078, 'learning_rate': 1.1437514418984484e-06, 'batch_size': 183, 'step_size': 8, 'gamma': 0.9875250471314025}. Best is trial 10 with value: 0.04130531517121027.[0m
[32m[I 2025-02-09 07:14:34,500][0m Trial 14 finished with value: 0.039036829614698294 and parameters: {'observation_period_num': 5, 'train_rates': 0.8978757054422937, 'learning_rate': 9.478060487974422e-05, 'batch_size': 54, 'step_size': 15, 'gamma': 0.9555251975552548}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:15:43,978][0m Trial 15 finished with value: 0.2049680614709547 and parameters: {'observation_period_num': 54, 'train_rates': 0.6002525149179152, 'learning_rate': 0.00014642430835680947, 'batch_size': 63, 'step_size': 15, 'gamma': 0.9431801921602148}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:16:17,548][0m Trial 16 finished with value: 0.1131292981726508 and parameters: {'observation_period_num': 41, 'train_rates': 0.8939687831509302, 'learning_rate': 2.1348754599674213e-05, 'batch_size': 185, 'step_size': 13, 'gamma': 0.9498763493868795}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:18:01,715][0m Trial 17 finished with value: 0.13250547543344066 and parameters: {'observation_period_num': 135, 'train_rates': 0.9103639156347879, 'learning_rate': 0.0001864101976969107, 'batch_size': 53, 'step_size': 6, 'gamma': 0.9591125723610558}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:19:05,513][0m Trial 18 finished with value: 0.08339241949411538 and parameters: {'observation_period_num': 75, 'train_rates': 0.8517491370832577, 'learning_rate': 0.0007720711515148075, 'batch_size': 86, 'step_size': 15, 'gamma': 0.9176507490698611}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:19:41,187][0m Trial 19 finished with value: 0.2389787228651417 and parameters: {'observation_period_num': 251, 'train_rates': 0.9241318929545975, 'learning_rate': 7.928191811256513e-05, 'batch_size': 162, 'step_size': 12, 'gamma': 0.9626691125618372}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:21:52,166][0m Trial 20 finished with value: 0.050475500083265415 and parameters: {'observation_period_num': 11, 'train_rates': 0.8226081929156575, 'learning_rate': 2.68957587085655e-05, 'batch_size': 41, 'step_size': 10, 'gamma': 0.9161614313761964}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:28:03,711][0m Trial 21 finished with value: 0.05600788159803911 and parameters: {'observation_period_num': 7, 'train_rates': 0.981660001662505, 'learning_rate': 4.677976850688705e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9859150677016824}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:30:50,370][0m Trial 22 finished with value: 0.050526961158865534 and parameters: {'observation_period_num': 31, 'train_rates': 0.9468655006790905, 'learning_rate': 8.353137596967463e-05, 'batch_size': 35, 'step_size': 6, 'gamma': 0.967472912993329}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:32:03,478][0m Trial 23 finished with value: 0.10749874150623447 and parameters: {'observation_period_num': 57, 'train_rates': 0.8894732508714943, 'learning_rate': 0.0002715106008324499, 'batch_size': 77, 'step_size': 9, 'gamma': 0.9447951882583361}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:34:26,402][0m Trial 24 finished with value: 0.05589195230945212 and parameters: {'observation_period_num': 23, 'train_rates': 0.9793523144158464, 'learning_rate': 7.429415454922207e-05, 'batch_size': 42, 'step_size': 7, 'gamma': 0.9875388608186296}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:34:51,800][0m Trial 25 finished with value: 0.33997249603271484 and parameters: {'observation_period_num': 48, 'train_rates': 0.9300599180075226, 'learning_rate': 1.218422868848119e-05, 'batch_size': 256, 'step_size': 13, 'gamma': 0.7845826083641163}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:36:18,093][0m Trial 26 finished with value: 0.1510898584653849 and parameters: {'observation_period_num': 72, 'train_rates': 0.8787530099589075, 'learning_rate': 0.0004653279936835315, 'batch_size': 64, 'step_size': 11, 'gamma': 0.9313668677903806}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:38:52,458][0m Trial 27 finished with value: 0.04534048833331819 and parameters: {'observation_period_num': 6, 'train_rates': 0.9602725360498948, 'learning_rate': 0.00014720466619729818, 'batch_size': 38, 'step_size': 5, 'gamma': 0.9712719773295131}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:39:45,287][0m Trial 28 finished with value: 0.10339891189258496 and parameters: {'observation_period_num': 100, 'train_rates': 0.8140765808602195, 'learning_rate': 3.07626113243523e-05, 'batch_size': 100, 'step_size': 9, 'gamma': 0.9336000297631263}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:45:05,976][0m Trial 29 finished with value: 0.047706517548515244 and parameters: {'observation_period_num': 23, 'train_rates': 0.9118484125149499, 'learning_rate': 1.9853306834676324e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.9571197672161362}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:47:48,054][0m Trial 30 finished with value: 0.09128597378730774 and parameters: {'observation_period_num': 74, 'train_rates': 0.9898998332117233, 'learning_rate': 6.17154860892523e-05, 'batch_size': 37, 'step_size': 11, 'gamma': 0.8943510238634652}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:50:59,040][0m Trial 31 finished with value: 0.04246096542477608 and parameters: {'observation_period_num': 10, 'train_rates': 0.9577386316192755, 'learning_rate': 0.00011592879773312547, 'batch_size': 32, 'step_size': 5, 'gamma': 0.9705969272276588}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:52:27,320][0m Trial 32 finished with value: 0.05883067846298218 and parameters: {'observation_period_num': 38, 'train_rates': 0.9642094625260349, 'learning_rate': 0.00010622230608776256, 'batch_size': 70, 'step_size': 5, 'gamma': 0.9721452322029317}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:55:36,013][0m Trial 33 finished with value: 0.05057403268109934 and parameters: {'observation_period_num': 19, 'train_rates': 0.930832524571589, 'learning_rate': 0.0002033321839287566, 'batch_size': 31, 'step_size': 4, 'gamma': 0.9370154003161018}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:57:07,836][0m Trial 34 finished with value: 0.14931134745716937 and parameters: {'observation_period_num': 32, 'train_rates': 0.637458447574784, 'learning_rate': 4.056450615896964e-05, 'batch_size': 49, 'step_size': 7, 'gamma': 0.9721923615483044}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 07:58:23,354][0m Trial 35 finished with value: 0.059268145468728294 and parameters: {'observation_period_num': 17, 'train_rates': 0.9409711585640138, 'learning_rate': 0.0003987066946690534, 'batch_size': 81, 'step_size': 14, 'gamma': 0.9543575370146616}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 08:01:35,615][0m Trial 36 finished with value: 0.1672660740676211 and parameters: {'observation_period_num': 151, 'train_rates': 0.905077257814491, 'learning_rate': 0.00010508534883698825, 'batch_size': 28, 'step_size': 7, 'gamma': 0.917330199109079}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 08:02:41,033][0m Trial 37 finished with value: 0.09518700816388269 and parameters: {'observation_period_num': 64, 'train_rates': 0.9647361412032776, 'learning_rate': 5.862581401628755e-05, 'batch_size': 95, 'step_size': 3, 'gamma': 0.9887483171326329}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 08:04:34,324][0m Trial 38 finished with value: 0.2878112041950226 and parameters: {'observation_period_num': 88, 'train_rates': 0.9654147816460676, 'learning_rate': 3.7338933150688225e-05, 'batch_size': 52, 'step_size': 1, 'gamma': 0.9007716572527439}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 08:06:04,969][0m Trial 39 finished with value: 0.1933587788751251 and parameters: {'observation_period_num': 43, 'train_rates': 0.8701747313127691, 'learning_rate': 6.831747546964415e-06, 'batch_size': 63, 'step_size': 10, 'gamma': 0.8000552225011207}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 08:06:54,504][0m Trial 40 finished with value: 0.2127211485403585 and parameters: {'observation_period_num': 196, 'train_rates': 0.7859433433902177, 'learning_rate': 0.0002123363244414515, 'batch_size': 108, 'step_size': 2, 'gamma': 0.846214246003199}. Best is trial 14 with value: 0.039036829614698294.[0m
[32m[I 2025-02-09 08:10:43,474][0m Trial 41 finished with value: 0.03463319443442203 and parameters: {'observation_period_num': 5, 'train_rates': 0.9544685337037849, 'learning_rate': 0.00015664560258240673, 'batch_size': 26, 'step_size': 4, 'gamma': 0.9732813499102508}. Best is trial 41 with value: 0.03463319443442203.[0m
[32m[I 2025-02-09 08:14:46,919][0m Trial 42 finished with value: 0.03465800938836063 and parameters: {'observation_period_num': 5, 'train_rates': 0.9442002197820183, 'learning_rate': 0.0001225701379208017, 'batch_size': 24, 'step_size': 4, 'gamma': 0.9777684476619702}. Best is trial 41 with value: 0.03463319443442203.[0m
[32m[I 2025-02-09 08:20:48,876][0m Trial 43 finished with value: 0.05180381465993242 and parameters: {'observation_period_num': 19, 'train_rates': 0.943430998796956, 'learning_rate': 0.00013231453318696552, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9746918685259109}. Best is trial 41 with value: 0.03463319443442203.[0m
[32m[I 2025-02-09 08:22:53,613][0m Trial 44 finished with value: 0.1537606910864512 and parameters: {'observation_period_num': 115, 'train_rates': 0.9210298127989834, 'learning_rate': 0.0003469154777358523, 'batch_size': 46, 'step_size': 4, 'gamma': 0.9495504962871236}. Best is trial 41 with value: 0.03463319443442203.[0m
[32m[I 2025-02-09 08:26:32,528][0m Trial 45 finished with value: 0.03449633073960913 and parameters: {'observation_period_num': 5, 'train_rates': 0.9513334893759032, 'learning_rate': 0.0002644847116314593, 'batch_size': 27, 'step_size': 2, 'gamma': 0.961926233639715}. Best is trial 45 with value: 0.03449633073960913.[0m
[32m[I 2025-02-09 08:30:08,318][0m Trial 46 finished with value: 0.07837740202777696 and parameters: {'observation_period_num': 30, 'train_rates': 0.8459226825602647, 'learning_rate': 0.00047700189866543326, 'batch_size': 25, 'step_size': 2, 'gamma': 0.9631074115451443}. Best is trial 45 with value: 0.03449633073960913.[0m
[32m[I 2025-02-09 08:31:51,345][0m Trial 47 finished with value: 0.06713719517727823 and parameters: {'observation_period_num': 41, 'train_rates': 0.8960338951216325, 'learning_rate': 0.0009946081597857837, 'batch_size': 55, 'step_size': 3, 'gamma': 0.8625386110657843}. Best is trial 45 with value: 0.03449633073960913.[0m
[32m[I 2025-02-09 08:33:15,542][0m Trial 48 finished with value: 0.06666780063972116 and parameters: {'observation_period_num': 22, 'train_rates': 0.936716097861215, 'learning_rate': 0.00024222796035136223, 'batch_size': 72, 'step_size': 1, 'gamma': 0.9231766534263736}. Best is trial 45 with value: 0.03449633073960913.[0m
[32m[I 2025-02-09 08:33:59,518][0m Trial 49 finished with value: 0.1909597030999255 and parameters: {'observation_period_num': 6, 'train_rates': 0.7558205307914265, 'learning_rate': 0.00017060697169589, 'batch_size': 122, 'step_size': 2, 'gamma': 0.8850439240311003}. Best is trial 45 with value: 0.03449633073960913.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-09 08:33:59,528][0m A new study created in memory with name: no-name-d8976453-8a85-45f1-9afe-40388c98a554[0m
[32m[I 2025-02-09 08:34:24,742][0m Trial 0 finished with value: 0.23980565029151232 and parameters: {'observation_period_num': 183, 'train_rates': 0.6667225866458673, 'learning_rate': 0.00011964350208532233, 'batch_size': 195, 'step_size': 14, 'gamma': 0.9106936739653811}. Best is trial 0 with value: 0.23980565029151232.[0m
[32m[I 2025-02-09 08:35:14,539][0m Trial 1 finished with value: 0.19792400300502777 and parameters: {'observation_period_num': 197, 'train_rates': 0.9770278448256691, 'learning_rate': 0.00015198239699930937, 'batch_size': 120, 'step_size': 3, 'gamma': 0.8614616474887108}. Best is trial 1 with value: 0.19792400300502777.[0m
[32m[I 2025-02-09 08:35:55,859][0m Trial 2 finished with value: 0.2768276035785675 and parameters: {'observation_period_num': 215, 'train_rates': 0.9832586628525919, 'learning_rate': 2.6996534892304647e-05, 'batch_size': 143, 'step_size': 10, 'gamma': 0.8512940203306575}. Best is trial 1 with value: 0.19792400300502777.[0m
[32m[I 2025-02-09 08:36:17,002][0m Trial 3 finished with value: 0.38569537337865706 and parameters: {'observation_period_num': 143, 'train_rates': 0.6433372459150827, 'learning_rate': 2.2672391953868395e-05, 'batch_size': 232, 'step_size': 15, 'gamma': 0.7836015561398431}. Best is trial 1 with value: 0.19792400300502777.[0m
[32m[I 2025-02-09 08:36:39,518][0m Trial 4 finished with value: 0.2733639111325525 and parameters: {'observation_period_num': 56, 'train_rates': 0.7572991874423671, 'learning_rate': 1.6609482198490135e-05, 'batch_size': 249, 'step_size': 7, 'gamma': 0.9888993624125597}. Best is trial 1 with value: 0.19792400300502777.[0m
[32m[I 2025-02-09 08:37:59,080][0m Trial 5 finished with value: 0.3307320271741669 and parameters: {'observation_period_num': 251, 'train_rates': 0.6487046553194729, 'learning_rate': 1.7660462638455605e-05, 'batch_size': 54, 'step_size': 15, 'gamma': 0.9594118293743739}. Best is trial 1 with value: 0.19792400300502777.[0m
[32m[I 2025-02-09 08:39:08,408][0m Trial 6 finished with value: 0.18355587396932685 and parameters: {'observation_period_num': 175, 'train_rates': 0.9175395838058618, 'learning_rate': 0.00016945548186959912, 'batch_size': 81, 'step_size': 11, 'gamma': 0.969814035844709}. Best is trial 6 with value: 0.18355587396932685.[0m
[32m[I 2025-02-09 08:39:46,047][0m Trial 7 finished with value: 0.5742843013118815 and parameters: {'observation_period_num': 216, 'train_rates': 0.7840167202637041, 'learning_rate': 3.450106903906225e-06, 'batch_size': 143, 'step_size': 10, 'gamma': 0.9169510912471945}. Best is trial 6 with value: 0.18355587396932685.[0m
[32m[I 2025-02-09 08:41:47,533][0m Trial 8 finished with value: 0.16109291589081343 and parameters: {'observation_period_num': 72, 'train_rates': 0.6313020210042637, 'learning_rate': 0.0002861368065709155, 'batch_size': 36, 'step_size': 3, 'gamma': 0.8371046516877534}. Best is trial 8 with value: 0.16109291589081343.[0m
[32m[I 2025-02-09 08:42:07,515][0m Trial 9 finished with value: 0.7506107628081362 and parameters: {'observation_period_num': 246, 'train_rates': 0.7511718008649004, 'learning_rate': 8.243101935783386e-06, 'batch_size': 256, 'step_size': 9, 'gamma': 0.8414289635438797}. Best is trial 8 with value: 0.16109291589081343.[0m
Early stopping at epoch 62
[32m[I 2025-02-09 08:45:16,689][0m Trial 10 finished with value: 0.047352198864284316 and parameters: {'observation_period_num': 50, 'train_rates': 0.8697549636473575, 'learning_rate': 0.0008909145256403667, 'batch_size': 18, 'step_size': 1, 'gamma': 0.7640538651139933}. Best is trial 10 with value: 0.047352198864284316.[0m
Early stopping at epoch 81
[32m[I 2025-02-09 08:49:38,982][0m Trial 11 finished with value: 0.044768364282743056 and parameters: {'observation_period_num': 41, 'train_rates': 0.8661734424131559, 'learning_rate': 0.0008464539129827491, 'batch_size': 17, 'step_size': 1, 'gamma': 0.764410072350511}. Best is trial 11 with value: 0.044768364282743056.[0m
Early stopping at epoch 77
[32m[I 2025-02-09 08:53:35,629][0m Trial 12 finished with value: 0.0354613907802016 and parameters: {'observation_period_num': 13, 'train_rates': 0.8640063090238981, 'learning_rate': 0.0009036962832196903, 'batch_size': 18, 'step_size': 1, 'gamma': 0.7508281056506337}. Best is trial 12 with value: 0.0354613907802016.[0m
[32m[I 2025-02-09 08:54:47,291][0m Trial 13 finished with value: 0.02669984006825043 and parameters: {'observation_period_num': 11, 'train_rates': 0.8488883692939362, 'learning_rate': 0.0009231381509279011, 'batch_size': 77, 'step_size': 6, 'gamma': 0.7990467959681941}. Best is trial 13 with value: 0.02669984006825043.[0m
[32m[I 2025-02-09 08:55:51,127][0m Trial 14 finished with value: 0.03368049363295237 and parameters: {'observation_period_num': 14, 'train_rates': 0.8414515200089846, 'learning_rate': 0.0003967557506337673, 'batch_size': 86, 'step_size': 6, 'gamma': 0.803635906890444}. Best is trial 13 with value: 0.02669984006825043.[0m
[32m[I 2025-02-09 08:56:53,529][0m Trial 15 finished with value: 0.7422703557905056 and parameters: {'observation_period_num': 94, 'train_rates': 0.8212091571191715, 'learning_rate': 1.0660587107072083e-06, 'batch_size': 85, 'step_size': 6, 'gamma': 0.8042332187305308}. Best is trial 13 with value: 0.02669984006825043.[0m
[32m[I 2025-02-09 08:57:48,122][0m Trial 16 finished with value: 0.1679942414986792 and parameters: {'observation_period_num': 6, 'train_rates': 0.6998995992521072, 'learning_rate': 5.849523261654885e-05, 'batch_size': 94, 'step_size': 5, 'gamma': 0.8009535679673921}. Best is trial 13 with value: 0.02669984006825043.[0m
[32m[I 2025-02-09 08:58:38,387][0m Trial 17 finished with value: 0.09764802430502394 and parameters: {'observation_period_num': 116, 'train_rates': 0.9227893453197528, 'learning_rate': 0.00035502788383940314, 'batch_size': 115, 'step_size': 4, 'gamma': 0.81919500655442}. Best is trial 13 with value: 0.02669984006825043.[0m
[32m[I 2025-02-09 08:59:13,093][0m Trial 18 finished with value: 0.04602349096050924 and parameters: {'observation_period_num': 34, 'train_rates': 0.8305913282851589, 'learning_rate': 0.0003279953709089525, 'batch_size': 169, 'step_size': 8, 'gamma': 0.8860644570623054}. Best is trial 13 with value: 0.02669984006825043.[0m
[32m[I 2025-02-09 09:00:40,058][0m Trial 19 finished with value: 0.10376434091914881 and parameters: {'observation_period_num': 95, 'train_rates': 0.922812776280896, 'learning_rate': 6.569606433055415e-05, 'batch_size': 65, 'step_size': 6, 'gamma': 0.7886213885284659}. Best is trial 13 with value: 0.02669984006825043.[0m
[32m[I 2025-02-09 09:01:28,387][0m Trial 20 finished with value: 0.1518143941236265 and parameters: {'observation_period_num': 27, 'train_rates': 0.719105160791556, 'learning_rate': 0.00045390681864103025, 'batch_size': 109, 'step_size': 13, 'gamma': 0.8209498342150507}. Best is trial 13 with value: 0.02669984006825043.[0m
[32m[I 2025-02-09 09:03:23,254][0m Trial 21 finished with value: 0.030652965078266654 and parameters: {'observation_period_num': 6, 'train_rates': 0.8752576336629965, 'learning_rate': 0.0009694476798089792, 'batch_size': 49, 'step_size': 3, 'gamma': 0.7524786811953432}. Best is trial 13 with value: 0.02669984006825043.[0m
[32m[I 2025-02-09 09:04:49,100][0m Trial 22 finished with value: 0.05879201337408561 and parameters: {'observation_period_num': 72, 'train_rates': 0.8204260624809661, 'learning_rate': 0.0005254142784172369, 'batch_size': 61, 'step_size': 4, 'gamma': 0.7757722037854892}. Best is trial 13 with value: 0.02669984006825043.[0m
[32m[I 2025-02-09 09:06:53,270][0m Trial 23 finished with value: 0.026319731748662888 and parameters: {'observation_period_num': 7, 'train_rates': 0.8920441787148885, 'learning_rate': 0.0005785192857039385, 'batch_size': 46, 'step_size': 7, 'gamma': 0.7508645565849613}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:08:58,975][0m Trial 24 finished with value: 0.03401926022472111 and parameters: {'observation_period_num': 6, 'train_rates': 0.9048092240463499, 'learning_rate': 0.0009934423073115655, 'batch_size': 46, 'step_size': 8, 'gamma': 0.7509245796431939}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:10:21,839][0m Trial 25 finished with value: 0.0757235932712243 and parameters: {'observation_period_num': 66, 'train_rates': 0.8894477591967589, 'learning_rate': 0.0002402055649486685, 'batch_size': 68, 'step_size': 3, 'gamma': 0.7724677657284367}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:12:40,287][0m Trial 26 finished with value: 0.04545332968108735 and parameters: {'observation_period_num': 29, 'train_rates': 0.9600037137162989, 'learning_rate': 0.0005785251733529445, 'batch_size': 43, 'step_size': 5, 'gamma': 0.7514338576197502}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:14:47,070][0m Trial 27 finished with value: 0.09745422821977864 and parameters: {'observation_period_num': 146, 'train_rates': 0.7960496980067615, 'learning_rate': 9.119858653605358e-05, 'batch_size': 40, 'step_size': 7, 'gamma': 0.7902487530419134}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:15:48,468][0m Trial 28 finished with value: 0.10734818847133563 and parameters: {'observation_period_num': 89, 'train_rates': 0.9460763643285818, 'learning_rate': 0.00020848100703862887, 'batch_size': 97, 'step_size': 4, 'gamma': 0.8174046437275327}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:16:22,838][0m Trial 29 finished with value: 0.03930874614466949 and parameters: {'observation_period_num': 26, 'train_rates': 0.8768276766283042, 'learning_rate': 0.0006026274013276559, 'batch_size': 173, 'step_size': 12, 'gamma': 0.885341447465653}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:17:42,457][0m Trial 30 finished with value: 0.11097044472328642 and parameters: {'observation_period_num': 56, 'train_rates': 0.8488036784011733, 'learning_rate': 0.00011556888817051138, 'batch_size': 70, 'step_size': 2, 'gamma': 0.7683862307509095}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:18:48,452][0m Trial 31 finished with value: 0.032739345515345006 and parameters: {'observation_period_num': 16, 'train_rates': 0.8407857037532716, 'learning_rate': 0.0004516720568670974, 'batch_size': 83, 'step_size': 6, 'gamma': 0.8016425876720807}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:19:32,239][0m Trial 32 finished with value: 0.1885897660442608 and parameters: {'observation_period_num': 42, 'train_rates': 0.7903860699779817, 'learning_rate': 0.0006588236630888866, 'batch_size': 127, 'step_size': 8, 'gamma': 0.7935513664493339}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:20:27,707][0m Trial 33 finished with value: 0.040250468951842144 and parameters: {'observation_period_num': 23, 'train_rates': 0.8929948186606732, 'learning_rate': 0.0004603480319323682, 'batch_size': 106, 'step_size': 5, 'gamma': 0.8320711528129338}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:23:45,071][0m Trial 34 finished with value: 0.027880217763595283 and parameters: {'observation_period_num': 5, 'train_rates': 0.9461407891877954, 'learning_rate': 0.00018279191122765618, 'batch_size': 30, 'step_size': 7, 'gamma': 0.8580510118724503}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:27:17,358][0m Trial 35 finished with value: 0.05847782428775515 and parameters: {'observation_period_num': 45, 'train_rates': 0.9832936717097772, 'learning_rate': 4.2456756188126413e-05, 'batch_size': 28, 'step_size': 9, 'gamma': 0.863491741134483}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:29:09,866][0m Trial 36 finished with value: 0.03762128670748911 and parameters: {'observation_period_num': 20, 'train_rates': 0.9356279240634006, 'learning_rate': 0.00014723098618764596, 'batch_size': 52, 'step_size': 7, 'gamma': 0.9219081848100045}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:31:49,957][0m Trial 37 finished with value: 0.028188417044778664 and parameters: {'observation_period_num': 5, 'train_rates': 0.9576022831914073, 'learning_rate': 0.00024451892796112715, 'batch_size': 37, 'step_size': 9, 'gamma': 0.9330289921173851}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:34:42,635][0m Trial 38 finished with value: 0.12493743031348997 and parameters: {'observation_period_num': 63, 'train_rates': 0.9647774316386489, 'learning_rate': 0.0002013673684504508, 'batch_size': 34, 'step_size': 10, 'gamma': 0.9465363588642535}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:35:11,781][0m Trial 39 finished with value: 0.08789493888616562 and parameters: {'observation_period_num': 38, 'train_rates': 0.9475508107369512, 'learning_rate': 8.390950697849971e-05, 'batch_size': 220, 'step_size': 9, 'gamma': 0.8977491008565394}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:38:18,941][0m Trial 40 finished with value: 0.1423468825863857 and parameters: {'observation_period_num': 160, 'train_rates': 0.9093515954336925, 'learning_rate': 0.0002751254865525463, 'batch_size': 29, 'step_size': 11, 'gamma': 0.9400726047322774}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:40:08,916][0m Trial 41 finished with value: 0.03056557493928437 and parameters: {'observation_period_num': 6, 'train_rates': 0.8917658674432933, 'learning_rate': 0.0007377301931809562, 'batch_size': 52, 'step_size': 7, 'gamma': 0.8535919793211763}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:41:26,545][0m Trial 42 finished with value: 0.12133498695180114 and parameters: {'observation_period_num': 5, 'train_rates': 0.6071147849777525, 'learning_rate': 0.00012390002877994693, 'batch_size': 57, 'step_size': 7, 'gamma': 0.8548223889583899}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:42:51,489][0m Trial 43 finished with value: 0.06781157450948624 and parameters: {'observation_period_num': 32, 'train_rates': 0.9359982287454605, 'learning_rate': 0.0006749832599270102, 'batch_size': 71, 'step_size': 8, 'gamma': 0.9023106315633442}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:46:12,795][0m Trial 44 finished with value: 0.05737853685722632 and parameters: {'observation_period_num': 49, 'train_rates': 0.9885251083563366, 'learning_rate': 0.00029099101190106417, 'batch_size': 30, 'step_size': 9, 'gamma': 0.873217538029593}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:47:47,667][0m Trial 45 finished with value: 0.24498973350439754 and parameters: {'observation_period_num': 200, 'train_rates': 0.8989129493483476, 'learning_rate': 1.3447778196021373e-05, 'batch_size': 57, 'step_size': 7, 'gamma': 0.847173900503535}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:51:45,771][0m Trial 46 finished with value: 0.043976388664709196 and parameters: {'observation_period_num': 18, 'train_rates': 0.9696696951690065, 'learning_rate': 0.0001639346642075029, 'batch_size': 25, 'step_size': 11, 'gamma': 0.9737258433924869}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:54:02,820][0m Trial 47 finished with value: 0.13739178830989496 and parameters: {'observation_period_num': 82, 'train_rates': 0.9249852624024432, 'learning_rate': 0.0007583425064308963, 'batch_size': 41, 'step_size': 10, 'gamma': 0.871545750753226}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:58:56,697][0m Trial 48 finished with value: 0.31015906274318694 and parameters: {'observation_period_num': 123, 'train_rates': 0.8066710657883025, 'learning_rate': 3.3539866465864196e-06, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8333479711634478}. Best is trial 23 with value: 0.026319731748662888.[0m
[32m[I 2025-02-09 09:59:34,833][0m Trial 49 finished with value: 0.09549444181620342 and parameters: {'observation_period_num': 109, 'train_rates': 0.8577480149664503, 'learning_rate': 0.00033623835194774985, 'batch_size': 152, 'step_size': 8, 'gamma': 0.9263578002057176}. Best is trial 23 with value: 0.026319731748662888.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-09 09:59:34,844][0m A new study created in memory with name: no-name-6c41e380-022b-41f2-bc31-d9d68e8da6be[0m
[32m[I 2025-02-09 09:59:57,654][0m Trial 0 finished with value: 0.35641368955309116 and parameters: {'observation_period_num': 75, 'train_rates': 0.6811727250992753, 'learning_rate': 3.0870606019490206e-05, 'batch_size': 223, 'step_size': 12, 'gamma': 0.7678846256125962}. Best is trial 0 with value: 0.35641368955309116.[0m
[32m[I 2025-02-09 10:00:19,348][0m Trial 1 finished with value: 0.3379982154181951 and parameters: {'observation_period_num': 43, 'train_rates': 0.6081637629490276, 'learning_rate': 9.370321582931699e-05, 'batch_size': 230, 'step_size': 3, 'gamma': 0.834235514258155}. Best is trial 1 with value: 0.3379982154181951.[0m
[32m[I 2025-02-09 10:02:25,638][0m Trial 2 finished with value: 0.19134905826180212 and parameters: {'observation_period_num': 55, 'train_rates': 0.6271573949797684, 'learning_rate': 3.8070943394966435e-05, 'batch_size': 35, 'step_size': 7, 'gamma': 0.9458669081868376}. Best is trial 2 with value: 0.19134905826180212.[0m
[32m[I 2025-02-09 10:03:15,754][0m Trial 3 finished with value: 0.33870074107619746 and parameters: {'observation_period_num': 200, 'train_rates': 0.7148289733277424, 'learning_rate': 0.00035356338288428964, 'batch_size': 98, 'step_size': 13, 'gamma': 0.955579127734894}. Best is trial 2 with value: 0.19134905826180212.[0m
[32m[I 2025-02-09 10:04:10,545][0m Trial 4 finished with value: 0.7943724187123293 and parameters: {'observation_period_num': 227, 'train_rates': 0.6839679852723693, 'learning_rate': 6.223247356337198e-06, 'batch_size': 85, 'step_size': 4, 'gamma': 0.7584742151483004}. Best is trial 2 with value: 0.19134905826180212.[0m
[32m[I 2025-02-09 10:04:48,299][0m Trial 5 finished with value: 0.0828798399423834 and parameters: {'observation_period_num': 13, 'train_rates': 0.932821848433553, 'learning_rate': 7.190998867643813e-05, 'batch_size': 163, 'step_size': 6, 'gamma': 0.7528206981708148}. Best is trial 5 with value: 0.0828798399423834.[0m
[32m[I 2025-02-09 10:05:18,021][0m Trial 6 finished with value: 0.0976453348994255 and parameters: {'observation_period_num': 38, 'train_rates': 0.9283225213701563, 'learning_rate': 4.635114079582523e-05, 'batch_size': 219, 'step_size': 6, 'gamma': 0.9070142800348855}. Best is trial 5 with value: 0.0828798399423834.[0m
[32m[I 2025-02-09 10:05:59,086][0m Trial 7 finished with value: 0.5836005654138855 and parameters: {'observation_period_num': 111, 'train_rates': 0.8339785051339289, 'learning_rate': 2.9780946872442506e-06, 'batch_size': 139, 'step_size': 5, 'gamma': 0.8827943224604158}. Best is trial 5 with value: 0.0828798399423834.[0m
[32m[I 2025-02-09 10:09:37,398][0m Trial 8 finished with value: 0.17302422646655521 and parameters: {'observation_period_num': 47, 'train_rates': 0.63276518331929, 'learning_rate': 1.0827783980565288e-05, 'batch_size': 20, 'step_size': 14, 'gamma': 0.964989268327886}. Best is trial 5 with value: 0.0828798399423834.[0m
[32m[I 2025-02-09 10:10:37,935][0m Trial 9 finished with value: 0.37907202758491926 and parameters: {'observation_period_num': 207, 'train_rates': 0.6335540912403295, 'learning_rate': 0.00016427833577080264, 'batch_size': 70, 'step_size': 5, 'gamma': 0.8506399378739083}. Best is trial 5 with value: 0.0828798399423834.[0m
Early stopping at epoch 62
[32m[I 2025-02-09 10:11:03,939][0m Trial 10 finished with value: 0.09872571378946304 and parameters: {'observation_period_num': 7, 'train_rates': 0.9878391516060052, 'learning_rate': 0.0007363991142248405, 'batch_size': 168, 'step_size': 1, 'gamma': 0.8067022962526831}. Best is trial 5 with value: 0.0828798399423834.[0m
[32m[I 2025-02-09 10:11:39,053][0m Trial 11 finished with value: 0.2511036753334025 and parameters: {'observation_period_num': 143, 'train_rates': 0.9341445197814948, 'learning_rate': 3.633854965225167e-05, 'batch_size': 169, 'step_size': 9, 'gamma': 0.9057442362284067}. Best is trial 5 with value: 0.0828798399423834.[0m
[32m[I 2025-02-09 10:12:10,594][0m Trial 12 finished with value: 0.048477229118519434 and parameters: {'observation_period_num': 6, 'train_rates': 0.8831389579556715, 'learning_rate': 0.0001238394059014086, 'batch_size': 196, 'step_size': 8, 'gamma': 0.9123616681427833}. Best is trial 12 with value: 0.048477229118519434.[0m
[32m[I 2025-02-09 10:12:43,380][0m Trial 13 finished with value: 0.10900260279844473 and parameters: {'observation_period_num': 111, 'train_rates': 0.8444526810091962, 'learning_rate': 0.00018000575805267137, 'batch_size': 184, 'step_size': 10, 'gamma': 0.8043468580514677}. Best is trial 12 with value: 0.048477229118519434.[0m
[32m[I 2025-02-09 10:13:27,756][0m Trial 14 finished with value: 0.04464603704465947 and parameters: {'observation_period_num': 19, 'train_rates': 0.8795987115501407, 'learning_rate': 0.0009092625508469969, 'batch_size': 131, 'step_size': 10, 'gamma': 0.9138048985839724}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:14:09,081][0m Trial 15 finished with value: 0.23267598639423856 and parameters: {'observation_period_num': 83, 'train_rates': 0.7827767754996175, 'learning_rate': 0.0008701003478575874, 'batch_size': 126, 'step_size': 11, 'gamma': 0.9239611316318606}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:14:32,412][0m Trial 16 finished with value: 0.134358570107028 and parameters: {'observation_period_num': 160, 'train_rates': 0.874946862015071, 'learning_rate': 0.0003114136402969737, 'batch_size': 254, 'step_size': 9, 'gamma': 0.9890992511394975}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:15:16,862][0m Trial 17 finished with value: 0.1690227363545161 and parameters: {'observation_period_num': 14, 'train_rates': 0.7708555229038004, 'learning_rate': 0.0004543431819857782, 'batch_size': 126, 'step_size': 15, 'gamma': 0.8770507427798168}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:15:50,885][0m Trial 18 finished with value: 0.19879465364479088 and parameters: {'observation_period_num': 79, 'train_rates': 0.8847099389385318, 'learning_rate': 1.7076038379649333e-05, 'batch_size': 193, 'step_size': 9, 'gamma': 0.9137027825160948}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:16:22,067][0m Trial 19 finished with value: 0.17046035826206207 and parameters: {'observation_period_num': 171, 'train_rates': 0.9836211376610245, 'learning_rate': 0.0001716203112310419, 'batch_size': 197, 'step_size': 8, 'gamma': 0.9407954554650049}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:17:03,143][0m Trial 20 finished with value: 0.9934167414080373 and parameters: {'observation_period_num': 29, 'train_rates': 0.8152354422817665, 'learning_rate': 1.1227261933433923e-06, 'batch_size': 140, 'step_size': 11, 'gamma': 0.8876515737362377}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:17:40,813][0m Trial 21 finished with value: 0.050593345309607685 and parameters: {'observation_period_num': 6, 'train_rates': 0.9242863430368756, 'learning_rate': 9.814820431647544e-05, 'batch_size': 161, 'step_size': 7, 'gamma': 0.8507198734021982}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:18:31,390][0m Trial 22 finished with value: 0.05131865987804399 and parameters: {'observation_period_num': 7, 'train_rates': 0.8868322852583004, 'learning_rate': 9.395573071387594e-05, 'batch_size': 115, 'step_size': 7, 'gamma': 0.8492784888880488}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:19:12,254][0m Trial 23 finished with value: 0.06439563980761995 and parameters: {'observation_period_num': 62, 'train_rates': 0.9190282622951708, 'learning_rate': 0.00023550377923963568, 'batch_size': 152, 'step_size': 8, 'gamma': 0.8555789768750377}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:19:44,678][0m Trial 24 finished with value: 0.04546917974948883 and parameters: {'observation_period_num': 27, 'train_rates': 0.9595502755022589, 'learning_rate': 0.0005051505712452531, 'batch_size': 211, 'step_size': 11, 'gamma': 0.8201901302408425}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:20:11,837][0m Trial 25 finished with value: 0.057986028492450714 and parameters: {'observation_period_num': 32, 'train_rates': 0.9645755692986556, 'learning_rate': 0.0005683284869229243, 'batch_size': 252, 'step_size': 12, 'gamma': 0.8045286948815444}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:20:42,132][0m Trial 26 finished with value: 0.09222578732041349 and parameters: {'observation_period_num': 104, 'train_rates': 0.8601025823933629, 'learning_rate': 0.0008715638779608617, 'batch_size': 201, 'step_size': 10, 'gamma': 0.8302217476908871}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:21:11,851][0m Trial 27 finished with value: 0.08284506302427601 and parameters: {'observation_period_num': 28, 'train_rates': 0.8993026874506839, 'learning_rate': 0.00040866840005890023, 'batch_size': 216, 'step_size': 11, 'gamma': 0.928216223846596}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:21:39,468][0m Trial 28 finished with value: 0.1582525372505188 and parameters: {'observation_period_num': 65, 'train_rates': 0.9609863414988987, 'learning_rate': 0.0002822069495390529, 'batch_size': 237, 'step_size': 13, 'gamma': 0.8961039851518673}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:22:10,230][0m Trial 29 finished with value: 0.0698116163376474 and parameters: {'observation_period_num': 85, 'train_rates': 0.8099622739552659, 'learning_rate': 0.0004902347304157495, 'batch_size': 184, 'step_size': 12, 'gamma': 0.7822916692083026}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:22:36,621][0m Trial 30 finished with value: 0.25755019981972616 and parameters: {'observation_period_num': 25, 'train_rates': 0.7529636022511815, 'learning_rate': 2.0089115725549867e-05, 'batch_size': 211, 'step_size': 10, 'gamma': 0.8221777354299534}. Best is trial 14 with value: 0.04464603704465947.[0m
[32m[I 2025-02-09 10:23:18,742][0m Trial 31 finished with value: 0.043794789543474905 and parameters: {'observation_period_num': 5, 'train_rates': 0.9116008856535824, 'learning_rate': 0.0001277518092734227, 'batch_size': 149, 'step_size': 7, 'gamma': 0.865536363738714}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:24:16,060][0m Trial 32 finished with value: 0.08904743734889357 and parameters: {'observation_period_num': 48, 'train_rates': 0.9552810871469072, 'learning_rate': 6.999798662680142e-05, 'batch_size': 109, 'step_size': 8, 'gamma': 0.7739623726419144}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:24:49,721][0m Trial 33 finished with value: 0.056211556267479194 and parameters: {'observation_period_num': 21, 'train_rates': 0.9063460944183275, 'learning_rate': 0.0001437116336090969, 'batch_size': 180, 'step_size': 9, 'gamma': 0.8656409119798638}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:26:16,444][0m Trial 34 finished with value: 0.072753742101449 and parameters: {'observation_period_num': 44, 'train_rates': 0.8376730066719057, 'learning_rate': 0.0009602164385108572, 'batch_size': 63, 'step_size': 6, 'gamma': 0.9286183803357255}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:26:41,771][0m Trial 35 finished with value: 0.059413881925395644 and parameters: {'observation_period_num': 63, 'train_rates': 0.8636078728254928, 'learning_rate': 0.0005842911960309669, 'batch_size': 239, 'step_size': 3, 'gamma': 0.8681529778458508}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:27:22,024][0m Trial 36 finished with value: 0.18143025040626526 and parameters: {'observation_period_num': 252, 'train_rates': 0.9478743553818815, 'learning_rate': 0.00024227888315423864, 'batch_size': 149, 'step_size': 13, 'gamma': 0.8963423334592675}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:28:22,180][0m Trial 37 finished with value: 0.05058102688421046 and parameters: {'observation_period_num': 19, 'train_rates': 0.8978900024723301, 'learning_rate': 5.1078523610606284e-05, 'batch_size': 99, 'step_size': 8, 'gamma': 0.9713046156878348}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:28:51,831][0m Trial 38 finished with value: 0.09614114357930859 and parameters: {'observation_period_num': 38, 'train_rates': 0.8600195834129164, 'learning_rate': 0.000117412287168557, 'batch_size': 207, 'step_size': 10, 'gamma': 0.9446026367230786}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:29:19,801][0m Trial 39 finished with value: 0.09831046148441559 and parameters: {'observation_period_num': 98, 'train_rates': 0.9102479053452728, 'learning_rate': 0.00032274952909407454, 'batch_size': 231, 'step_size': 5, 'gamma': 0.8350711938834793}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:30:28,288][0m Trial 40 finished with value: 0.08732197484509512 and parameters: {'observation_period_num': 57, 'train_rates': 0.9397682758940575, 'learning_rate': 6.449033082222465e-05, 'batch_size': 90, 'step_size': 7, 'gamma': 0.7894701630823212}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:31:49,232][0m Trial 41 finished with value: 0.04850891022498061 and parameters: {'observation_period_num': 19, 'train_rates': 0.8934683285044951, 'learning_rate': 5.468540274504502e-05, 'batch_size': 73, 'step_size': 7, 'gamma': 0.9553765912191047}. Best is trial 31 with value: 0.043794789543474905.[0m
[32m[I 2025-02-09 10:33:25,603][0m Trial 42 finished with value: 0.04322066013648938 and parameters: {'observation_period_num': 5, 'train_rates': 0.8799291761648752, 'learning_rate': 2.523363135847365e-05, 'batch_size': 58, 'step_size': 6, 'gamma': 0.962912909024133}. Best is trial 42 with value: 0.04322066013648938.[0m
[32m[I 2025-02-09 10:36:18,593][0m Trial 43 finished with value: 0.03936531756028353 and parameters: {'observation_period_num': 6, 'train_rates': 0.9721057059041374, 'learning_rate': 2.6986725893176912e-05, 'batch_size': 35, 'step_size': 4, 'gamma': 0.9798039034905054}. Best is trial 43 with value: 0.03936531756028353.[0m
[32m[I 2025-02-09 10:38:31,394][0m Trial 44 finished with value: 0.06648437306284904 and parameters: {'observation_period_num': 40, 'train_rates': 0.9795619377220576, 'learning_rate': 2.2647850967439216e-05, 'batch_size': 46, 'step_size': 3, 'gamma': 0.988416023037515}. Best is trial 43 with value: 0.03936531756028353.[0m
[32m[I 2025-02-09 10:44:01,699][0m Trial 45 finished with value: 0.05118497770823789 and parameters: {'observation_period_num': 17, 'train_rates': 0.9698140465783146, 'learning_rate': 9.562800732324577e-06, 'batch_size': 18, 'step_size': 4, 'gamma': 0.9737599840737734}. Best is trial 43 with value: 0.03936531756028353.[0m
[32m[I 2025-02-09 10:46:09,085][0m Trial 46 finished with value: 0.0761478459965941 and parameters: {'observation_period_num': 50, 'train_rates': 0.9494118542405963, 'learning_rate': 3.3787293695059246e-05, 'batch_size': 47, 'step_size': 1, 'gamma': 0.9785115793728916}. Best is trial 43 with value: 0.03936531756028353.[0m
[32m[I 2025-02-09 10:48:19,373][0m Trial 47 finished with value: 0.1616782371849169 and parameters: {'observation_period_num': 5, 'train_rates': 0.7102786280177372, 'learning_rate': 1.1653909653917197e-05, 'batch_size': 37, 'step_size': 4, 'gamma': 0.9574017088419653}. Best is trial 43 with value: 0.03936531756028353.[0m
[32m[I 2025-02-09 10:51:39,145][0m Trial 48 finished with value: 0.06520280857601077 and parameters: {'observation_period_num': 37, 'train_rates': 0.9273606860671909, 'learning_rate': 1.4552896562163204e-05, 'batch_size': 29, 'step_size': 5, 'gamma': 0.935931181987071}. Best is trial 43 with value: 0.03936531756028353.[0m
[32m[I 2025-02-09 10:53:10,683][0m Trial 49 finished with value: 0.1749231670435299 and parameters: {'observation_period_num': 27, 'train_rates': 0.814999775200034, 'learning_rate': 2.6209230163692514e-05, 'batch_size': 59, 'step_size': 2, 'gamma': 0.817033171042777}. Best is trial 43 with value: 0.03936531756028353.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-09 10:53:10,695][0m A new study created in memory with name: no-name-2f85cb1e-9cd4-45e7-a573-a47ebd142ed7[0m
[32m[I 2025-02-09 10:53:39,000][0m Trial 0 finished with value: 0.8261034488677979 and parameters: {'observation_period_num': 111, 'train_rates': 0.967939762644245, 'learning_rate': 1.2949708889935955e-05, 'batch_size': 237, 'step_size': 1, 'gamma': 0.9397222737050386}. Best is trial 0 with value: 0.8261034488677979.[0m
[32m[I 2025-02-09 10:56:26,732][0m Trial 1 finished with value: 0.5245319438568434 and parameters: {'observation_period_num': 139, 'train_rates': 0.604492446821603, 'learning_rate': 5.268992057490468e-05, 'batch_size': 25, 'step_size': 12, 'gamma': 0.7583282474978523}. Best is trial 1 with value: 0.5245319438568434.[0m
[32m[I 2025-02-09 10:56:57,858][0m Trial 2 finished with value: 0.27125699855007634 and parameters: {'observation_period_num': 121, 'train_rates': 0.758482057321169, 'learning_rate': 4.957366187833043e-05, 'batch_size': 170, 'step_size': 9, 'gamma': 0.9473821273203454}. Best is trial 2 with value: 0.27125699855007634.[0m
[32m[I 2025-02-09 10:57:20,895][0m Trial 3 finished with value: 0.4615952670574188 and parameters: {'observation_period_num': 206, 'train_rates': 0.9230230399569075, 'learning_rate': 8.202769404253898e-06, 'batch_size': 255, 'step_size': 9, 'gamma': 0.9884638327026108}. Best is trial 2 with value: 0.27125699855007634.[0m
[32m[I 2025-02-09 10:58:02,254][0m Trial 4 finished with value: 0.8532376394887861 and parameters: {'observation_period_num': 169, 'train_rates': 0.7010078378470334, 'learning_rate': 1.2644418461314058e-06, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8352543324784166}. Best is trial 2 with value: 0.27125699855007634.[0m
[32m[I 2025-02-09 10:58:31,656][0m Trial 5 finished with value: 0.6435589790344238 and parameters: {'observation_period_num': 121, 'train_rates': 0.9832935578696417, 'learning_rate': 4.985245094028412e-06, 'batch_size': 219, 'step_size': 4, 'gamma': 0.880397785740872}. Best is trial 2 with value: 0.27125699855007634.[0m
Early stopping at epoch 51
[32m[I 2025-02-09 10:58:54,866][0m Trial 6 finished with value: 0.38216932081495936 and parameters: {'observation_period_num': 214, 'train_rates': 0.8663652511475692, 'learning_rate': 0.00022833884147794822, 'batch_size': 129, 'step_size': 1, 'gamma': 0.7622128439312686}. Best is trial 2 with value: 0.27125699855007634.[0m
[32m[I 2025-02-09 11:00:10,328][0m Trial 7 finished with value: 0.08997327531759555 and parameters: {'observation_period_num': 66, 'train_rates': 0.8880835669573057, 'learning_rate': 0.0002951263520914769, 'batch_size': 74, 'step_size': 6, 'gamma': 0.9339125265823134}. Best is trial 7 with value: 0.08997327531759555.[0m
[32m[I 2025-02-09 11:00:45,656][0m Trial 8 finished with value: 0.11101898644119501 and parameters: {'observation_period_num': 133, 'train_rates': 0.9324389582978952, 'learning_rate': 0.0001956949256746113, 'batch_size': 167, 'step_size': 11, 'gamma': 0.8699292511436265}. Best is trial 7 with value: 0.08997327531759555.[0m
[32m[I 2025-02-09 11:01:40,606][0m Trial 9 finished with value: 1.2428339953335483 and parameters: {'observation_period_num': 235, 'train_rates': 0.7393617218485385, 'learning_rate': 2.7910153677058825e-06, 'batch_size': 87, 'step_size': 2, 'gamma': 0.8720089303095312}. Best is trial 7 with value: 0.08997327531759555.[0m
[32m[I 2025-02-09 11:03:59,720][0m Trial 10 finished with value: 0.0655216056232651 and parameters: {'observation_period_num': 33, 'train_rates': 0.8403856341759146, 'learning_rate': 0.0004917633882522309, 'batch_size': 39, 'step_size': 6, 'gamma': 0.926473898140831}. Best is trial 10 with value: 0.0655216056232651.[0m
[32m[I 2025-02-09 11:06:23,320][0m Trial 11 finished with value: 0.047315313215304286 and parameters: {'observation_period_num': 29, 'train_rates': 0.8497145256251438, 'learning_rate': 0.0009007917568572587, 'batch_size': 38, 'step_size': 6, 'gamma': 0.9271643287870444}. Best is trial 11 with value: 0.047315313215304286.[0m
[32m[I 2025-02-09 11:11:01,106][0m Trial 12 finished with value: 0.03139281869447959 and parameters: {'observation_period_num': 6, 'train_rates': 0.8283829661707363, 'learning_rate': 0.0009201202878626857, 'batch_size': 19, 'step_size': 6, 'gamma': 0.9144951337031751}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:12:27,570][0m Trial 13 finished with value: 0.03240048031552674 and parameters: {'observation_period_num': 7, 'train_rates': 0.8043276322403754, 'learning_rate': 0.0009927590159205747, 'batch_size': 61, 'step_size': 7, 'gamma': 0.9019899543110595}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:13:49,628][0m Trial 14 finished with value: 0.050273134734253495 and parameters: {'observation_period_num': 12, 'train_rates': 0.8011140616476407, 'learning_rate': 0.00010418573605003682, 'batch_size': 66, 'step_size': 4, 'gamma': 0.8160309357489693}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:14:39,935][0m Trial 15 finished with value: 0.17718095273884457 and parameters: {'observation_period_num': 67, 'train_rates': 0.6838032040808943, 'learning_rate': 0.000966588854716708, 'batch_size': 97, 'step_size': 15, 'gamma': 0.896982278081144}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:19:19,726][0m Trial 16 finished with value: 0.08884820777755295 and parameters: {'observation_period_num': 66, 'train_rates': 0.8008042765943626, 'learning_rate': 0.00011070614343262485, 'batch_size': 18, 'step_size': 8, 'gamma': 0.9717154611250434}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:20:52,765][0m Trial 17 finished with value: 0.15799297237147888 and parameters: {'observation_period_num': 10, 'train_rates': 0.7725122250906007, 'learning_rate': 0.0005410433069676164, 'batch_size': 56, 'step_size': 4, 'gamma': 0.8351125198940453}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:21:44,401][0m Trial 18 finished with value: 0.08190093585071599 and parameters: {'observation_period_num': 42, 'train_rates': 0.8178612932801608, 'learning_rate': 2.051190724469884e-05, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9042448529345888}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:23:20,086][0m Trial 19 finished with value: 0.19890451286897134 and parameters: {'observation_period_num': 90, 'train_rates': 0.7094178110109035, 'learning_rate': 0.0003973912735663794, 'batch_size': 50, 'step_size': 10, 'gamma': 0.8469815506666667}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:23:52,667][0m Trial 20 finished with value: 0.15264269294600716 and parameters: {'observation_period_num': 5, 'train_rates': 0.6323101256112861, 'learning_rate': 9.56055663819686e-05, 'batch_size': 154, 'step_size': 7, 'gamma': 0.9049769853075728}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:26:33,950][0m Trial 21 finished with value: 0.04673231941998418 and parameters: {'observation_period_num': 36, 'train_rates': 0.8521394684370903, 'learning_rate': 0.0007230971408619998, 'batch_size': 34, 'step_size': 6, 'gamma': 0.9574905712364372}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:30:57,208][0m Trial 22 finished with value: 0.0606242118008209 and parameters: {'observation_period_num': 47, 'train_rates': 0.8984912344525848, 'learning_rate': 0.0009065274798196968, 'batch_size': 21, 'step_size': 3, 'gamma': 0.9621524322668499}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:32:07,295][0m Trial 23 finished with value: 0.046254133511907776 and parameters: {'observation_period_num': 27, 'train_rates': 0.8232839826104088, 'learning_rate': 0.000521980605510113, 'batch_size': 77, 'step_size': 5, 'gamma': 0.9124128719209637}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:33:14,580][0m Trial 24 finished with value: 0.20294764357985873 and parameters: {'observation_period_num': 91, 'train_rates': 0.7705982287690615, 'learning_rate': 0.0002283556014494418, 'batch_size': 76, 'step_size': 5, 'gamma': 0.9151173033722237}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:34:59,937][0m Trial 25 finished with value: 0.04266700501052233 and parameters: {'observation_period_num': 19, 'train_rates': 0.8235720121512689, 'learning_rate': 0.0003665593471617879, 'batch_size': 51, 'step_size': 7, 'gamma': 0.8855900708156421}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:36:49,394][0m Trial 26 finished with value: 0.07236738326518159 and parameters: {'observation_period_num': 53, 'train_rates': 0.889083889874255, 'learning_rate': 0.00015813991135225295, 'batch_size': 51, 'step_size': 9, 'gamma': 0.8899009856428882}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:41:52,420][0m Trial 27 finished with value: 0.15884762462608668 and parameters: {'observation_period_num': 5, 'train_rates': 0.7353715881655142, 'learning_rate': 0.0003862760942608623, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8576100461700771}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:42:18,413][0m Trial 28 finished with value: 0.21012390653292337 and parameters: {'observation_period_num': 81, 'train_rates': 0.7858465297338645, 'learning_rate': 0.0003114148265153535, 'batch_size': 206, 'step_size': 7, 'gamma': 0.8151382254835403}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:44:00,023][0m Trial 29 finished with value: 0.07122246878352134 and parameters: {'observation_period_num': 29, 'train_rates': 0.9465322102450754, 'learning_rate': 2.7696811547452507e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.7810981894942195}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:44:56,318][0m Trial 30 finished with value: 0.10102970164555769 and parameters: {'observation_period_num': 106, 'train_rates': 0.8183696709084535, 'learning_rate': 6.953425176467234e-05, 'batch_size': 96, 'step_size': 5, 'gamma': 0.8875621507579023}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:46:05,142][0m Trial 31 finished with value: 0.049475503316305854 and parameters: {'observation_period_num': 21, 'train_rates': 0.8264497577317034, 'learning_rate': 0.0005093600399004084, 'batch_size': 81, 'step_size': 5, 'gamma': 0.921902407854288}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:48:22,820][0m Trial 32 finished with value: 0.061141403205692765 and parameters: {'observation_period_num': 24, 'train_rates': 0.8641497159857663, 'learning_rate': 0.0007463574161499736, 'batch_size': 40, 'step_size': 3, 'gamma': 0.9068748238984848}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:49:41,765][0m Trial 33 finished with value: 0.20428381801302078 and parameters: {'observation_period_num': 54, 'train_rates': 0.7482477285823508, 'learning_rate': 0.0005489338173452345, 'batch_size': 64, 'step_size': 7, 'gamma': 0.9369653658412606}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:52:34,667][0m Trial 34 finished with value: 0.039352676760749654 and parameters: {'observation_period_num': 21, 'train_rates': 0.7980896498290038, 'learning_rate': 0.0001421879259412457, 'batch_size': 30, 'step_size': 8, 'gamma': 0.8685338849103291}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:54:53,168][0m Trial 35 finished with value: 0.11489757701334818 and parameters: {'observation_period_num': 154, 'train_rates': 0.7981270676830376, 'learning_rate': 4.7862183830216664e-05, 'batch_size': 36, 'step_size': 8, 'gamma': 0.8651425647407718}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:57:41,184][0m Trial 36 finished with value: 0.27548796405955667 and parameters: {'observation_period_num': 179, 'train_rates': 0.7775865581742599, 'learning_rate': 0.00016068688218352848, 'batch_size': 29, 'step_size': 9, 'gamma': 0.8452498215964251}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:58:27,927][0m Trial 37 finished with value: 0.04074928157453267 and parameters: {'observation_period_num': 19, 'train_rates': 0.8741030224838476, 'learning_rate': 0.0003169940016493712, 'batch_size': 127, 'step_size': 10, 'gamma': 0.8812132675517041}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:59:14,172][0m Trial 38 finished with value: 0.05941061472743174 and parameters: {'observation_period_num': 55, 'train_rates': 0.9112821334019598, 'learning_rate': 0.0002600081003182541, 'batch_size': 131, 'step_size': 12, 'gamma': 0.94525821815479}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 11:59:45,542][0m Trial 39 finished with value: 0.06191610963268076 and parameters: {'observation_period_num': 44, 'train_rates': 0.8720947636687033, 'learning_rate': 0.00012530404321860753, 'batch_size': 188, 'step_size': 11, 'gamma': 0.8752872288543017}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 12:00:21,032][0m Trial 40 finished with value: 0.3958970165707235 and parameters: {'observation_period_num': 75, 'train_rates': 0.7187468307269482, 'learning_rate': 7.906235787706714e-06, 'batch_size': 142, 'step_size': 13, 'gamma': 0.8223891665972596}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 12:01:10,710][0m Trial 41 finished with value: 0.035760507704573195 and parameters: {'observation_period_num': 13, 'train_rates': 0.838217679007223, 'learning_rate': 0.0003371540204894765, 'batch_size': 116, 'step_size': 9, 'gamma': 0.8585190746835552}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 12:01:50,387][0m Trial 42 finished with value: 0.03657239551345507 and parameters: {'observation_period_num': 18, 'train_rates': 0.8778682983474047, 'learning_rate': 0.00027517901305442306, 'batch_size': 145, 'step_size': 10, 'gamma': 0.8583660254619041}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 12:02:24,428][0m Trial 43 finished with value: 0.03199808972252157 and parameters: {'observation_period_num': 15, 'train_rates': 0.8422577186945164, 'learning_rate': 0.0007474259504703223, 'batch_size': 169, 'step_size': 9, 'gamma': 0.8550879073192051}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 12:02:58,794][0m Trial 44 finished with value: 0.03404575618541945 and parameters: {'observation_period_num': 5, 'train_rates': 0.8401304664639013, 'learning_rate': 0.0007438511379064274, 'batch_size': 169, 'step_size': 9, 'gamma': 0.8534937521373562}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 12:03:31,216][0m Trial 45 finished with value: 0.034487540666091894 and parameters: {'observation_period_num': 5, 'train_rates': 0.8387308584210348, 'learning_rate': 0.0006697699239068625, 'batch_size': 178, 'step_size': 9, 'gamma': 0.8393859351072699}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 12:04:03,546][0m Trial 46 finished with value: 0.04439398379290485 and parameters: {'observation_period_num': 37, 'train_rates': 0.8504701507869893, 'learning_rate': 0.0006799623261269874, 'batch_size': 181, 'step_size': 11, 'gamma': 0.7834441065516691}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 12:04:38,687][0m Trial 47 finished with value: 0.15138430257256213 and parameters: {'observation_period_num': 8, 'train_rates': 0.7582767264734779, 'learning_rate': 0.0009928584515439395, 'batch_size': 164, 'step_size': 9, 'gamma': 0.7977738854518492}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 12:05:07,980][0m Trial 48 finished with value: 0.8173482095608946 and parameters: {'observation_period_num': 245, 'train_rates': 0.9104521625305009, 'learning_rate': 1.6811132134776053e-06, 'batch_size': 200, 'step_size': 8, 'gamma': 0.8291013562555789}. Best is trial 12 with value: 0.03139281869447959.[0m
[32m[I 2025-02-09 12:05:33,993][0m Trial 49 finished with value: 0.04716520940369748 and parameters: {'observation_period_num': 38, 'train_rates': 0.8397123037534051, 'learning_rate': 0.0006544610569968903, 'batch_size': 231, 'step_size': 6, 'gamma': 0.8408025552716697}. Best is trial 12 with value: 0.03139281869447959.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-09 12:05:34,003][0m A new study created in memory with name: no-name-ce9c7d47-674f-412c-acb2-a347b9af910a[0m
[32m[I 2025-02-09 12:06:00,565][0m Trial 0 finished with value: 0.7162479408037891 and parameters: {'observation_period_num': 39, 'train_rates': 0.7854520522123389, 'learning_rate': 2.59157013637451e-06, 'batch_size': 227, 'step_size': 10, 'gamma': 0.7679786076354712}. Best is trial 0 with value: 0.7162479408037891.[0m
[32m[I 2025-02-09 12:07:36,833][0m Trial 1 finished with value: 0.3749607185239894 and parameters: {'observation_period_num': 199, 'train_rates': 0.6951833121025296, 'learning_rate': 1.519709028093194e-05, 'batch_size': 47, 'step_size': 12, 'gamma': 0.8476460163130692}. Best is trial 1 with value: 0.3749607185239894.[0m
[32m[I 2025-02-09 12:11:10,551][0m Trial 2 finished with value: 0.407331013516204 and parameters: {'observation_period_num': 122, 'train_rates': 0.640727161705506, 'learning_rate': 1.756971995169921e-06, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8815657317616681}. Best is trial 1 with value: 0.3749607185239894.[0m
[32m[I 2025-02-09 12:12:06,640][0m Trial 3 finished with value: 0.2290076695432599 and parameters: {'observation_period_num': 197, 'train_rates': 0.946434424318657, 'learning_rate': 3.290487036221029e-05, 'batch_size': 104, 'step_size': 7, 'gamma': 0.9815301220728209}. Best is trial 3 with value: 0.2290076695432599.[0m
Early stopping at epoch 45
[32m[I 2025-02-09 12:12:20,999][0m Trial 4 finished with value: 0.9535307446612588 and parameters: {'observation_period_num': 123, 'train_rates': 0.6942908536681708, 'learning_rate': 1.1200936001912123e-05, 'batch_size': 170, 'step_size': 1, 'gamma': 0.7797375373155255}. Best is trial 3 with value: 0.2290076695432599.[0m
[32m[I 2025-02-09 12:13:14,030][0m Trial 5 finished with value: 0.18542829167748254 and parameters: {'observation_period_num': 43, 'train_rates': 0.7412141747745509, 'learning_rate': 7.135769883114447e-05, 'batch_size': 96, 'step_size': 5, 'gamma': 0.901006884171529}. Best is trial 5 with value: 0.18542829167748254.[0m
[32m[I 2025-02-09 12:14:50,426][0m Trial 6 finished with value: 0.09219690163930257 and parameters: {'observation_period_num': 75, 'train_rates': 0.9657457974114925, 'learning_rate': 0.0004766317838405677, 'batch_size': 66, 'step_size': 5, 'gamma': 0.8380447294061178}. Best is trial 6 with value: 0.09219690163930257.[0m
[32m[I 2025-02-09 12:15:29,171][0m Trial 7 finished with value: 0.4945429752926325 and parameters: {'observation_period_num': 23, 'train_rates': 0.6087113584230861, 'learning_rate': 6.965269543008966e-05, 'batch_size': 122, 'step_size': 12, 'gamma': 0.8692113216421161}. Best is trial 6 with value: 0.09219690163930257.[0m
[32m[I 2025-02-09 12:16:14,578][0m Trial 8 finished with value: 0.9105168481136032 and parameters: {'observation_period_num': 232, 'train_rates': 0.6583174156466627, 'learning_rate': 1.3134638063919067e-06, 'batch_size': 99, 'step_size': 9, 'gamma': 0.8642229883474988}. Best is trial 6 with value: 0.09219690163930257.[0m
[32m[I 2025-02-09 12:16:55,677][0m Trial 9 finished with value: 0.12491847292796986 and parameters: {'observation_period_num': 116, 'train_rates': 0.8442693013265454, 'learning_rate': 7.524563838306203e-05, 'batch_size': 138, 'step_size': 11, 'gamma': 0.774118041557311}. Best is trial 6 with value: 0.09219690163930257.[0m
[32m[I 2025-02-09 12:18:43,155][0m Trial 10 finished with value: 0.11836094874888659 and parameters: {'observation_period_num': 71, 'train_rates': 0.9779618045842876, 'learning_rate': 0.0008757310021543228, 'batch_size': 56, 'step_size': 15, 'gamma': 0.8244290603352457}. Best is trial 6 with value: 0.09219690163930257.[0m
[32m[I 2025-02-09 12:20:28,306][0m Trial 11 finished with value: 0.17266753315925598 and parameters: {'observation_period_num': 81, 'train_rates': 0.9863897695259682, 'learning_rate': 0.000780237711330748, 'batch_size': 57, 'step_size': 15, 'gamma': 0.8194911970285012}. Best is trial 6 with value: 0.09219690163930257.[0m
[32m[I 2025-02-09 12:22:05,415][0m Trial 12 finished with value: 0.07371460935591083 and parameters: {'observation_period_num': 67, 'train_rates': 0.8953835775949754, 'learning_rate': 0.0009658369439177242, 'batch_size': 58, 'step_size': 3, 'gamma': 0.8192786881131534}. Best is trial 12 with value: 0.07371460935591083.[0m
[32m[I 2025-02-09 12:26:26,632][0m Trial 13 finished with value: 0.14790897413554333 and parameters: {'observation_period_num': 82, 'train_rates': 0.8937408358666004, 'learning_rate': 0.0003246509259738288, 'batch_size': 21, 'step_size': 3, 'gamma': 0.9334517901242383}. Best is trial 12 with value: 0.07371460935591083.[0m
[32m[I 2025-02-09 12:27:02,082][0m Trial 14 finished with value: 0.05495011051361625 and parameters: {'observation_period_num': 5, 'train_rates': 0.8877212029344783, 'learning_rate': 0.000328679660932162, 'batch_size': 171, 'step_size': 4, 'gamma': 0.8110880001861966}. Best is trial 14 with value: 0.05495011051361625.[0m
Early stopping at epoch 59
[32m[I 2025-02-09 12:27:20,973][0m Trial 15 finished with value: 0.1677381478888648 and parameters: {'observation_period_num': 13, 'train_rates': 0.8814106047747883, 'learning_rate': 0.00018848187546951562, 'batch_size': 204, 'step_size': 1, 'gamma': 0.7951597598920576}. Best is trial 14 with value: 0.05495011051361625.[0m
[32m[I 2025-02-09 12:27:56,081][0m Trial 16 finished with value: 0.0679343486816146 and parameters: {'observation_period_num': 9, 'train_rates': 0.8229952200092916, 'learning_rate': 0.0002233954490654384, 'batch_size': 169, 'step_size': 3, 'gamma': 0.8136452868769236}. Best is trial 14 with value: 0.05495011051361625.[0m
[32m[I 2025-02-09 12:28:29,965][0m Trial 17 finished with value: 0.06876328199339088 and parameters: {'observation_period_num': 5, 'train_rates': 0.8154225502930513, 'learning_rate': 0.00019568992608116014, 'batch_size': 170, 'step_size': 3, 'gamma': 0.8026410229177178}. Best is trial 14 with value: 0.05495011051361625.[0m
[32m[I 2025-02-09 12:28:52,034][0m Trial 18 finished with value: 0.28428775797082956 and parameters: {'observation_period_num': 161, 'train_rates': 0.7750096235699151, 'learning_rate': 0.00014216120084154486, 'batch_size': 255, 'step_size': 4, 'gamma': 0.9076444803724341}. Best is trial 14 with value: 0.05495011051361625.[0m
[32m[I 2025-02-09 12:29:28,313][0m Trial 19 finished with value: 0.057927876393490306 and parameters: {'observation_period_num': 36, 'train_rates': 0.923841426066804, 'learning_rate': 0.0003807162523252374, 'batch_size': 173, 'step_size': 8, 'gamma': 0.7566368644757685}. Best is trial 14 with value: 0.05495011051361625.[0m
[32m[I 2025-02-09 12:29:59,867][0m Trial 20 finished with value: 0.06828254461288452 and parameters: {'observation_period_num': 47, 'train_rates': 0.9321472451578245, 'learning_rate': 0.0004309915154894069, 'batch_size': 204, 'step_size': 8, 'gamma': 0.7533471253224445}. Best is trial 14 with value: 0.05495011051361625.[0m
[32m[I 2025-02-09 12:30:35,516][0m Trial 21 finished with value: 0.04567154443933876 and parameters: {'observation_period_num': 5, 'train_rates': 0.8528232953442247, 'learning_rate': 0.0002910510552658303, 'batch_size': 162, 'step_size': 7, 'gamma': 0.7961036859824715}. Best is trial 21 with value: 0.04567154443933876.[0m
[32m[I 2025-02-09 12:31:16,287][0m Trial 22 finished with value: 0.06510032150967449 and parameters: {'observation_period_num': 30, 'train_rates': 0.8610317896514857, 'learning_rate': 0.00011775548523780137, 'batch_size': 148, 'step_size': 7, 'gamma': 0.7573321818823556}. Best is trial 21 with value: 0.04567154443933876.[0m
[32m[I 2025-02-09 12:31:47,772][0m Trial 23 finished with value: 0.06865483826917151 and parameters: {'observation_period_num': 52, 'train_rates': 0.9210634960796802, 'learning_rate': 0.00037103552292714443, 'batch_size': 194, 'step_size': 8, 'gamma': 0.7907509744201952}. Best is trial 21 with value: 0.04567154443933876.[0m
[32m[I 2025-02-09 12:32:23,977][0m Trial 24 finished with value: 0.10180335666885099 and parameters: {'observation_period_num': 30, 'train_rates': 0.85926314889087, 'learning_rate': 3.8222864910559264e-05, 'batch_size': 158, 'step_size': 6, 'gamma': 0.7839697000270789}. Best is trial 21 with value: 0.04567154443933876.[0m
[32m[I 2025-02-09 12:32:56,849][0m Trial 25 finished with value: 0.04064591845561718 and parameters: {'observation_period_num': 6, 'train_rates': 0.902039361018106, 'learning_rate': 0.000533066168028788, 'batch_size': 186, 'step_size': 9, 'gamma': 0.7502892734898186}. Best is trial 25 with value: 0.04064591845561718.[0m
[32m[I 2025-02-09 12:33:22,899][0m Trial 26 finished with value: 0.10785683460773961 and parameters: {'observation_period_num': 107, 'train_rates': 0.8264499437678946, 'learning_rate': 0.0005823355559981441, 'batch_size': 224, 'step_size': 10, 'gamma': 0.839470417594233}. Best is trial 25 with value: 0.04064591845561718.[0m
[32m[I 2025-02-09 12:33:54,590][0m Trial 27 finished with value: 0.4376051264230385 and parameters: {'observation_period_num': 150, 'train_rates': 0.8874473325407619, 'learning_rate': 1.3882420844316859e-05, 'batch_size': 189, 'step_size': 6, 'gamma': 0.7982273306986329}. Best is trial 25 with value: 0.04064591845561718.[0m
[32m[I 2025-02-09 12:34:36,331][0m Trial 28 finished with value: 0.5101206569228998 and parameters: {'observation_period_num': 59, 'train_rates': 0.7553334528149056, 'learning_rate': 4.966341710129532e-06, 'batch_size': 129, 'step_size': 9, 'gamma': 0.7503926746202032}. Best is trial 25 with value: 0.04064591845561718.[0m
[32m[I 2025-02-09 12:35:00,397][0m Trial 29 finished with value: 0.10335253998637199 and parameters: {'observation_period_num': 98, 'train_rates': 0.7906588123019629, 'learning_rate': 0.00027000105586759887, 'batch_size': 235, 'step_size': 10, 'gamma': 0.7720601799583151}. Best is trial 25 with value: 0.04064591845561718.[0m
[32m[I 2025-02-09 12:35:28,070][0m Trial 30 finished with value: 0.0956099750541097 and parameters: {'observation_period_num': 21, 'train_rates': 0.905679747551827, 'learning_rate': 0.00011364304104005823, 'batch_size': 228, 'step_size': 5, 'gamma': 0.7699028851751218}. Best is trial 25 with value: 0.04064591845561718.[0m
[32m[I 2025-02-09 12:36:03,103][0m Trial 31 finished with value: 0.06174552068114281 and parameters: {'observation_period_num': 36, 'train_rates': 0.9503731025334056, 'learning_rate': 0.0005968917738311812, 'batch_size': 183, 'step_size': 9, 'gamma': 0.7585522999747193}. Best is trial 25 with value: 0.04064591845561718.[0m
[32m[I 2025-02-09 12:36:46,189][0m Trial 32 finished with value: 0.04799667750564167 and parameters: {'observation_period_num': 9, 'train_rates': 0.9201979523387059, 'learning_rate': 0.0003022346629908388, 'batch_size': 151, 'step_size': 7, 'gamma': 0.7685572654896363}. Best is trial 25 with value: 0.04064591845561718.[0m
[32m[I 2025-02-09 12:37:24,745][0m Trial 33 finished with value: 0.03820139693802801 and parameters: {'observation_period_num': 5, 'train_rates': 0.8532207686105857, 'learning_rate': 0.0002646308826891474, 'batch_size': 151, 'step_size': 13, 'gamma': 0.8020851996082073}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:38:15,615][0m Trial 34 finished with value: 0.047487434224378545 and parameters: {'observation_period_num': 20, 'train_rates': 0.8367035380382817, 'learning_rate': 0.00015047422496010737, 'batch_size': 115, 'step_size': 13, 'gamma': 0.7870948126523685}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:39:07,421][0m Trial 35 finished with value: 0.056778871406202025 and parameters: {'observation_period_num': 21, 'train_rates': 0.8554057393500439, 'learning_rate': 5.376134960942621e-05, 'batch_size': 113, 'step_size': 14, 'gamma': 0.7864525171414873}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:40:18,160][0m Trial 36 finished with value: 0.0620725966870466 and parameters: {'observation_period_num': 53, 'train_rates': 0.8073541124667055, 'learning_rate': 0.0001559133818845968, 'batch_size': 76, 'step_size': 13, 'gamma': 0.8520187530947758}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:40:59,949][0m Trial 37 finished with value: 0.12523857671521307 and parameters: {'observation_period_num': 141, 'train_rates': 0.8394219207391646, 'learning_rate': 9.639997949248825e-05, 'batch_size': 139, 'step_size': 12, 'gamma': 0.8333981030637106}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:41:43,631][0m Trial 38 finished with value: 0.22310979473859743 and parameters: {'observation_period_num': 41, 'train_rates': 0.7728120599164137, 'learning_rate': 1.9590661313088533e-05, 'batch_size': 120, 'step_size': 13, 'gamma': 0.9615827123230183}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:42:37,855][0m Trial 39 finished with value: 0.24253586420012668 and parameters: {'observation_period_num': 171, 'train_rates': 0.7175390464026622, 'learning_rate': 0.0005850884260029313, 'batch_size': 91, 'step_size': 12, 'gamma': 0.805368205551194}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:43:41,254][0m Trial 40 finished with value: 0.16390071190189154 and parameters: {'observation_period_num': 240, 'train_rates': 0.8722055457576083, 'learning_rate': 4.8054836503096674e-05, 'batch_size': 84, 'step_size': 11, 'gamma': 0.8852078021966665}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:44:20,729][0m Trial 41 finished with value: 0.05065888589418742 and parameters: {'observation_period_num': 17, 'train_rates': 0.9150966550816676, 'learning_rate': 0.00022449727932664543, 'batch_size': 155, 'step_size': 7, 'gamma': 0.7792940871291913}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:45:01,967][0m Trial 42 finished with value: 0.047241080552339554 and parameters: {'observation_period_num': 25, 'train_rates': 0.9505366237699255, 'learning_rate': 0.0002687671921327777, 'batch_size': 150, 'step_size': 13, 'gamma': 0.7628276259919735}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:46:01,246][0m Trial 43 finished with value: 0.05162120446048934 and parameters: {'observation_period_num': 31, 'train_rates': 0.9507032200569099, 'learning_rate': 0.0005225070238677521, 'batch_size': 106, 'step_size': 14, 'gamma': 0.7647906310167193}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:46:50,572][0m Trial 44 finished with value: 0.06105101853609085 and parameters: {'observation_period_num': 24, 'train_rates': 0.9703417358481989, 'learning_rate': 8.879120785478675e-05, 'batch_size': 133, 'step_size': 13, 'gamma': 0.7954114104031257}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:47:32,197][0m Trial 45 finished with value: 0.06094391132178514 and parameters: {'observation_period_num': 59, 'train_rates': 0.8419543424920893, 'learning_rate': 0.00015764973038023963, 'batch_size': 143, 'step_size': 14, 'gamma': 0.7775265178625961}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:48:08,213][0m Trial 46 finished with value: 0.08768181370571256 and parameters: {'observation_period_num': 16, 'train_rates': 0.8643503692627361, 'learning_rate': 2.6149365079264837e-05, 'batch_size': 161, 'step_size': 11, 'gamma': 0.7860980110188996}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:48:58,565][0m Trial 47 finished with value: 0.2744293126715235 and parameters: {'observation_period_num': 41, 'train_rates': 0.9434168618698733, 'learning_rate': 8.92016418399281e-06, 'batch_size': 124, 'step_size': 15, 'gamma': 0.8297922380870898}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:49:25,381][0m Trial 48 finished with value: 0.1772010505639319 and parameters: {'observation_period_num': 201, 'train_rates': 0.8040642681626664, 'learning_rate': 0.0007456898046928395, 'batch_size': 208, 'step_size': 11, 'gamma': 0.8543697346910345}. Best is trial 33 with value: 0.03820139693802801.[0m
[32m[I 2025-02-09 12:49:58,505][0m Trial 49 finished with value: 0.09336239850018924 and parameters: {'observation_period_num': 87, 'train_rates': 0.9054383667060248, 'learning_rate': 0.0002553783755478725, 'batch_size': 181, 'step_size': 13, 'gamma': 0.7655910902145355}. Best is trial 33 with value: 0.03820139693802801.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 7, 'train_rates': 0.8758546185409937, 'learning_rate': 9.007966235471077e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9028563556732473}
Epoch 1/300, trend Loss: 0.2768 | 0.1151
Epoch 2/300, trend Loss: 0.1251 | 0.0851
Epoch 3/300, trend Loss: 0.1127 | 0.0751
Epoch 4/300, trend Loss: 0.1071 | 0.0726
Epoch 5/300, trend Loss: 0.1035 | 0.0702
Epoch 6/300, trend Loss: 0.1004 | 0.0672
Epoch 7/300, trend Loss: 0.0973 | 0.0643
Epoch 8/300, trend Loss: 0.0937 | 0.0618
Epoch 9/300, trend Loss: 0.0900 | 0.0572
Epoch 10/300, trend Loss: 0.0869 | 0.0548
Epoch 11/300, trend Loss: 0.0840 | 0.0520
Epoch 12/300, trend Loss: 0.0818 | 0.0496
Epoch 13/300, trend Loss: 0.0802 | 0.0478
Epoch 14/300, trend Loss: 0.0786 | 0.0463
Epoch 15/300, trend Loss: 0.0772 | 0.0448
Epoch 16/300, trend Loss: 0.0754 | 0.0427
Epoch 17/300, trend Loss: 0.0741 | 0.0417
Epoch 18/300, trend Loss: 0.0731 | 0.0411
Epoch 19/300, trend Loss: 0.0723 | 0.0405
Epoch 20/300, trend Loss: 0.0717 | 0.0399
Epoch 21/300, trend Loss: 0.0711 | 0.0394
Epoch 22/300, trend Loss: 0.0706 | 0.0389
Epoch 23/300, trend Loss: 0.0702 | 0.0383
Epoch 24/300, trend Loss: 0.0695 | 0.0374
Epoch 25/300, trend Loss: 0.0690 | 0.0372
Epoch 26/300, trend Loss: 0.0685 | 0.0369
Epoch 27/300, trend Loss: 0.0681 | 0.0367
Epoch 28/300, trend Loss: 0.0677 | 0.0365
Epoch 29/300, trend Loss: 0.0673 | 0.0363
Epoch 30/300, trend Loss: 0.0669 | 0.0361
Epoch 31/300, trend Loss: 0.0663 | 0.0355
Epoch 32/300, trend Loss: 0.0658 | 0.0353
Epoch 33/300, trend Loss: 0.0654 | 0.0351
Epoch 34/300, trend Loss: 0.0650 | 0.0348
Epoch 35/300, trend Loss: 0.0646 | 0.0346
Epoch 36/300, trend Loss: 0.0642 | 0.0343
Epoch 37/300, trend Loss: 0.0639 | 0.0341
Epoch 38/300, trend Loss: 0.0635 | 0.0339
Epoch 39/300, trend Loss: 0.0631 | 0.0335
Epoch 40/300, trend Loss: 0.0627 | 0.0331
Epoch 41/300, trend Loss: 0.0624 | 0.0329
Epoch 42/300, trend Loss: 0.0620 | 0.0326
Epoch 43/300, trend Loss: 0.0617 | 0.0323
Epoch 44/300, trend Loss: 0.0613 | 0.0320
Epoch 45/300, trend Loss: 0.0610 | 0.0318
Epoch 46/300, trend Loss: 0.0606 | 0.0309
Epoch 47/300, trend Loss: 0.0602 | 0.0307
Epoch 48/300, trend Loss: 0.0599 | 0.0305
Epoch 49/300, trend Loss: 0.0596 | 0.0304
Epoch 50/300, trend Loss: 0.0594 | 0.0303
Epoch 51/300, trend Loss: 0.0591 | 0.0301
Epoch 52/300, trend Loss: 0.0588 | 0.0300
Epoch 53/300, trend Loss: 0.0585 | 0.0300
Epoch 54/300, trend Loss: 0.0582 | 0.0297
Epoch 55/300, trend Loss: 0.0579 | 0.0296
Epoch 56/300, trend Loss: 0.0576 | 0.0296
Epoch 57/300, trend Loss: 0.0574 | 0.0297
Epoch 58/300, trend Loss: 0.0572 | 0.0296
Epoch 59/300, trend Loss: 0.0569 | 0.0297
Epoch 60/300, trend Loss: 0.0567 | 0.0296
Epoch 61/300, trend Loss: 0.0564 | 0.0290
Epoch 62/300, trend Loss: 0.0561 | 0.0289
Epoch 63/300, trend Loss: 0.0558 | 0.0288
Epoch 64/300, trend Loss: 0.0556 | 0.0288
Epoch 65/300, trend Loss: 0.0553 | 0.0287
Epoch 66/300, trend Loss: 0.0551 | 0.0287
Epoch 67/300, trend Loss: 0.0549 | 0.0287
Epoch 68/300, trend Loss: 0.0547 | 0.0287
Epoch 69/300, trend Loss: 0.0545 | 0.0283
Epoch 70/300, trend Loss: 0.0543 | 0.0283
Epoch 71/300, trend Loss: 0.0542 | 0.0283
Epoch 72/300, trend Loss: 0.0540 | 0.0284
Epoch 73/300, trend Loss: 0.0538 | 0.0285
Epoch 74/300, trend Loss: 0.0537 | 0.0285
Epoch 75/300, trend Loss: 0.0535 | 0.0286
Epoch 76/300, trend Loss: 0.0534 | 0.0281
Epoch 77/300, trend Loss: 0.0533 | 0.0282
Epoch 78/300, trend Loss: 0.0531 | 0.0282
Epoch 79/300, trend Loss: 0.0530 | 0.0283
Epoch 80/300, trend Loss: 0.0528 | 0.0283
Epoch 81/300, trend Loss: 0.0527 | 0.0284
Epoch 82/300, trend Loss: 0.0526 | 0.0285
Epoch 83/300, trend Loss: 0.0525 | 0.0286
Epoch 84/300, trend Loss: 0.0523 | 0.0282
Epoch 85/300, trend Loss: 0.0522 | 0.0283
Epoch 86/300, trend Loss: 0.0521 | 0.0284
Epoch 87/300, trend Loss: 0.0520 | 0.0285
Epoch 88/300, trend Loss: 0.0518 | 0.0285
Epoch 89/300, trend Loss: 0.0517 | 0.0286
Epoch 90/300, trend Loss: 0.0516 | 0.0287
Epoch 91/300, trend Loss: 0.0515 | 0.0285
Epoch 92/300, trend Loss: 0.0514 | 0.0286
Epoch 93/300, trend Loss: 0.0513 | 0.0287
Epoch 94/300, trend Loss: 0.0512 | 0.0288
Epoch 95/300, trend Loss: 0.0511 | 0.0290
Epoch 96/300, trend Loss: 0.0510 | 0.0291
Epoch 97/300, trend Loss: 0.0509 | 0.0293
Epoch 98/300, trend Loss: 0.0507 | 0.0295
Epoch 99/300, trend Loss: 0.0506 | 0.0295
Epoch 100/300, trend Loss: 0.0505 | 0.0298
Epoch 101/300, trend Loss: 0.0504 | 0.0302
Epoch 102/300, trend Loss: 0.0503 | 0.0307
Epoch 103/300, trend Loss: 0.0501 | 0.0312
Epoch 104/300, trend Loss: 0.0498 | 0.0316
Epoch 105/300, trend Loss: 0.0495 | 0.0322
Epoch 106/300, trend Loss: 0.0491 | 0.0327
Epoch 107/300, trend Loss: 0.0486 | 0.0332
Epoch 108/300, trend Loss: 0.0479 | 0.0336
Epoch 109/300, trend Loss: 0.0471 | 0.0338
Epoch 110/300, trend Loss: 0.0462 | 0.0335
Epoch 111/300, trend Loss: 0.0453 | 0.0324
Epoch 112/300, trend Loss: 0.0446 | 0.0313
Epoch 113/300, trend Loss: 0.0439 | 0.0308
Epoch 114/300, trend Loss: 0.0433 | 0.0304
Epoch 115/300, trend Loss: 0.0429 | 0.0310
Epoch 116/300, trend Loss: 0.0425 | 0.0295
Epoch 117/300, trend Loss: 0.0459 | 0.0472
Epoch 118/300, trend Loss: 0.0442 | 0.0287
Epoch 119/300, trend Loss: 0.0426 | 0.0287
Epoch 120/300, trend Loss: 0.0420 | 0.0286
Epoch 121/300, trend Loss: 0.0416 | 0.0287
Epoch 122/300, trend Loss: 0.0412 | 0.0290
Epoch 123/300, trend Loss: 0.0409 | 0.0293
Epoch 124/300, trend Loss: 0.0407 | 0.0294
Epoch 125/300, trend Loss: 0.0405 | 0.0296
Epoch 126/300, trend Loss: 0.0403 | 0.0296
Epoch 127/300, trend Loss: 0.0402 | 0.0298
Epoch 128/300, trend Loss: 0.0401 | 0.0294
Epoch 129/300, trend Loss: 0.0400 | 0.0317
Epoch 130/300, trend Loss: 0.0408 | 0.0279
Epoch 131/300, trend Loss: 0.0408 | 0.0430
Epoch 132/300, trend Loss: 0.0417 | 0.0285
Epoch 133/300, trend Loss: 0.0401 | 0.0283
Epoch 134/300, trend Loss: 0.0396 | 0.0290
Epoch 135/300, trend Loss: 0.0395 | 0.0290
Epoch 136/300, trend Loss: 0.0393 | 0.0292
Epoch 137/300, trend Loss: 0.0392 | 0.0290
Epoch 138/300, trend Loss: 0.0391 | 0.0295
Epoch 139/300, trend Loss: 0.0391 | 0.0291
Epoch 140/300, trend Loss: 0.0390 | 0.0300
Epoch 141/300, trend Loss: 0.0390 | 0.0289
Epoch 142/300, trend Loss: 0.0390 | 0.0316
Epoch 143/300, trend Loss: 0.0395 | 0.0282
Epoch 144/300, trend Loss: 0.0388 | 0.0305
Epoch 145/300, trend Loss: 0.0390 | 0.0285
Epoch 146/300, trend Loss: 0.0385 | 0.0294
Epoch 147/300, trend Loss: 0.0385 | 0.0291
Epoch 148/300, trend Loss: 0.0384 | 0.0297
Epoch 149/300, trend Loss: 0.0384 | 0.0292
Epoch 150/300, trend Loss: 0.0383 | 0.0303
Epoch 151/300, trend Loss: 0.0383 | 0.0288
Epoch 152/300, trend Loss: 0.0382 | 0.0302
Epoch 153/300, trend Loss: 0.0383 | 0.0287
Epoch 154/300, trend Loss: 0.0381 | 0.0303
Epoch 155/300, trend Loss: 0.0382 | 0.0288
Epoch 156/300, trend Loss: 0.0379 | 0.0303
Epoch 157/300, trend Loss: 0.0380 | 0.0289
Epoch 158/300, trend Loss: 0.0378 | 0.0303
Epoch 159/300, trend Loss: 0.0379 | 0.0288
Epoch 160/300, trend Loss: 0.0377 | 0.0298
Epoch 161/300, trend Loss: 0.0377 | 0.0290
Epoch 162/300, trend Loss: 0.0376 | 0.0301
Epoch 163/300, trend Loss: 0.0376 | 0.0291
Epoch 164/300, trend Loss: 0.0375 | 0.0304
Epoch 165/300, trend Loss: 0.0375 | 0.0291
Epoch 166/300, trend Loss: 0.0374 | 0.0302
Epoch 167/300, trend Loss: 0.0374 | 0.0290
Epoch 168/300, trend Loss: 0.0372 | 0.0301
Epoch 169/300, trend Loss: 0.0372 | 0.0292
Epoch 170/300, trend Loss: 0.0371 | 0.0302
Epoch 171/300, trend Loss: 0.0371 | 0.0293
Epoch 172/300, trend Loss: 0.0370 | 0.0305
Epoch 173/300, trend Loss: 0.0370 | 0.0294
Epoch 174/300, trend Loss: 0.0369 | 0.0305
Epoch 175/300, trend Loss: 0.0369 | 0.0294
Epoch 176/300, trend Loss: 0.0368 | 0.0304
Epoch 177/300, trend Loss: 0.0368 | 0.0296
Epoch 178/300, trend Loss: 0.0367 | 0.0305
Epoch 179/300, trend Loss: 0.0367 | 0.0297
Epoch 180/300, trend Loss: 0.0366 | 0.0307
Epoch 181/300, trend Loss: 0.0366 | 0.0296
Epoch 182/300, trend Loss: 0.0365 | 0.0306
Epoch 183/300, trend Loss: 0.0365 | 0.0298
Epoch 184/300, trend Loss: 0.0364 | 0.0307
Epoch 185/300, trend Loss: 0.0364 | 0.0299
Epoch 186/300, trend Loss: 0.0364 | 0.0309
Epoch 187/300, trend Loss: 0.0363 | 0.0300
Epoch 188/300, trend Loss: 0.0363 | 0.0311
Epoch 189/300, trend Loss: 0.0363 | 0.0299
Epoch 190/300, trend Loss: 0.0362 | 0.0308
Epoch 191/300, trend Loss: 0.0362 | 0.0301
Epoch 192/300, trend Loss: 0.0361 | 0.0309
Epoch 193/300, trend Loss: 0.0361 | 0.0302
Epoch 194/300, trend Loss: 0.0360 | 0.0310
Epoch 195/300, trend Loss: 0.0360 | 0.0302
Epoch 196/300, trend Loss: 0.0359 | 0.0309
Epoch 197/300, trend Loss: 0.0359 | 0.0301
Epoch 198/300, trend Loss: 0.0359 | 0.0309
Epoch 199/300, trend Loss: 0.0359 | 0.0302
Epoch 200/300, trend Loss: 0.0358 | 0.0309
Epoch 201/300, trend Loss: 0.0358 | 0.0303
Epoch 202/300, trend Loss: 0.0357 | 0.0310
Epoch 203/300, trend Loss: 0.0357 | 0.0303
Epoch 204/300, trend Loss: 0.0357 | 0.0309
Epoch 205/300, trend Loss: 0.0357 | 0.0302
Epoch 206/300, trend Loss: 0.0356 | 0.0308
Epoch 207/300, trend Loss: 0.0356 | 0.0302
Epoch 208/300, trend Loss: 0.0355 | 0.0308
Epoch 209/300, trend Loss: 0.0355 | 0.0303
Epoch 210/300, trend Loss: 0.0355 | 0.0309
Epoch 211/300, trend Loss: 0.0354 | 0.0302
Epoch 212/300, trend Loss: 0.0354 | 0.0307
Epoch 213/300, trend Loss: 0.0354 | 0.0302
Epoch 214/300, trend Loss: 0.0353 | 0.0307
Epoch 215/300, trend Loss: 0.0353 | 0.0302
Epoch 216/300, trend Loss: 0.0353 | 0.0307
Epoch 217/300, trend Loss: 0.0353 | 0.0302
Epoch 218/300, trend Loss: 0.0352 | 0.0308
Epoch 219/300, trend Loss: 0.0352 | 0.0302
Epoch 220/300, trend Loss: 0.0352 | 0.0307
Epoch 221/300, trend Loss: 0.0352 | 0.0303
Epoch 222/300, trend Loss: 0.0351 | 0.0307
Epoch 223/300, trend Loss: 0.0351 | 0.0303
Epoch 224/300, trend Loss: 0.0351 | 0.0307
Epoch 225/300, trend Loss: 0.0350 | 0.0303
Epoch 226/300, trend Loss: 0.0350 | 0.0307
Epoch 227/300, trend Loss: 0.0350 | 0.0304
Epoch 228/300, trend Loss: 0.0350 | 0.0307
Epoch 229/300, trend Loss: 0.0349 | 0.0304
Epoch 230/300, trend Loss: 0.0349 | 0.0308
Epoch 231/300, trend Loss: 0.0349 | 0.0305
Epoch 232/300, trend Loss: 0.0349 | 0.0308
Epoch 233/300, trend Loss: 0.0348 | 0.0305
Epoch 234/300, trend Loss: 0.0348 | 0.0308
Epoch 235/300, trend Loss: 0.0348 | 0.0306
Epoch 236/300, trend Loss: 0.0348 | 0.0309
Epoch 237/300, trend Loss: 0.0347 | 0.0306
Epoch 238/300, trend Loss: 0.0347 | 0.0309
Epoch 239/300, trend Loss: 0.0347 | 0.0307
Epoch 240/300, trend Loss: 0.0347 | 0.0310
Epoch 241/300, trend Loss: 0.0346 | 0.0307
Epoch 242/300, trend Loss: 0.0346 | 0.0310
Epoch 243/300, trend Loss: 0.0346 | 0.0308
Epoch 244/300, trend Loss: 0.0346 | 0.0311
Epoch 245/300, trend Loss: 0.0345 | 0.0309
Epoch 246/300, trend Loss: 0.0345 | 0.0311
Epoch 247/300, trend Loss: 0.0345 | 0.0310
Epoch 248/300, trend Loss: 0.0345 | 0.0312
Epoch 249/300, trend Loss: 0.0345 | 0.0310
Epoch 250/300, trend Loss: 0.0344 | 0.0312
Epoch 251/300, trend Loss: 0.0344 | 0.0311
Epoch 252/300, trend Loss: 0.0344 | 0.0313
Epoch 253/300, trend Loss: 0.0344 | 0.0311
Epoch 254/300, trend Loss: 0.0344 | 0.0314
Epoch 255/300, trend Loss: 0.0343 | 0.0312
Epoch 256/300, trend Loss: 0.0343 | 0.0314
Epoch 257/300, trend Loss: 0.0343 | 0.0313
Epoch 258/300, trend Loss: 0.0343 | 0.0315
Epoch 259/300, trend Loss: 0.0343 | 0.0314
Epoch 260/300, trend Loss: 0.0342 | 0.0315
Epoch 261/300, trend Loss: 0.0342 | 0.0314
Epoch 262/300, trend Loss: 0.0342 | 0.0316
Epoch 263/300, trend Loss: 0.0342 | 0.0315
Epoch 264/300, trend Loss: 0.0342 | 0.0316
Epoch 265/300, trend Loss: 0.0342 | 0.0316
Epoch 266/300, trend Loss: 0.0341 | 0.0317
Epoch 267/300, trend Loss: 0.0341 | 0.0316
Epoch 268/300, trend Loss: 0.0341 | 0.0317
Epoch 269/300, trend Loss: 0.0341 | 0.0317
Epoch 270/300, trend Loss: 0.0341 | 0.0318
Epoch 271/300, trend Loss: 0.0341 | 0.0318
Epoch 272/300, trend Loss: 0.0340 | 0.0319
Epoch 273/300, trend Loss: 0.0340 | 0.0318
Epoch 274/300, trend Loss: 0.0340 | 0.0319
Epoch 275/300, trend Loss: 0.0340 | 0.0318
Epoch 276/300, trend Loss: 0.0340 | 0.0319
Epoch 277/300, trend Loss: 0.0340 | 0.0319
Epoch 278/300, trend Loss: 0.0340 | 0.0320
Epoch 279/300, trend Loss: 0.0339 | 0.0320
Epoch 280/300, trend Loss: 0.0339 | 0.0320
Epoch 281/300, trend Loss: 0.0339 | 0.0320
Epoch 282/300, trend Loss: 0.0339 | 0.0321
Epoch 283/300, trend Loss: 0.0339 | 0.0321
Epoch 284/300, trend Loss: 0.0339 | 0.0321
Epoch 285/300, trend Loss: 0.0339 | 0.0321
Epoch 286/300, trend Loss: 0.0338 | 0.0322
Epoch 287/300, trend Loss: 0.0338 | 0.0321
Epoch 288/300, trend Loss: 0.0338 | 0.0322
Epoch 289/300, trend Loss: 0.0338 | 0.0322
Epoch 290/300, trend Loss: 0.0338 | 0.0322
Epoch 291/300, trend Loss: 0.0338 | 0.0322
Epoch 292/300, trend Loss: 0.0338 | 0.0323
Epoch 293/300, trend Loss: 0.0338 | 0.0323
Epoch 294/300, trend Loss: 0.0337 | 0.0323
Epoch 295/300, trend Loss: 0.0337 | 0.0323
Epoch 296/300, trend Loss: 0.0337 | 0.0323
Epoch 297/300, trend Loss: 0.0337 | 0.0323
Epoch 298/300, trend Loss: 0.0337 | 0.0324
Epoch 299/300, trend Loss: 0.0337 | 0.0324
Epoch 300/300, trend Loss: 0.0337 | 0.0324
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.9513334893759032, 'learning_rate': 0.0002644847116314593, 'batch_size': 27, 'step_size': 2, 'gamma': 0.961926233639715}
Epoch 1/300, seasonal_0 Loss: 0.2067 | 0.1311
Epoch 2/300, seasonal_0 Loss: 0.1175 | 0.0947
Epoch 3/300, seasonal_0 Loss: 0.1088 | 0.0918
Epoch 4/300, seasonal_0 Loss: 0.1038 | 0.0847
Epoch 5/300, seasonal_0 Loss: 0.0986 | 0.0759
Epoch 6/300, seasonal_0 Loss: 0.0929 | 0.0680
Epoch 7/300, seasonal_0 Loss: 0.0887 | 0.0627
Epoch 8/300, seasonal_0 Loss: 0.0860 | 0.0594
Epoch 9/300, seasonal_0 Loss: 0.0840 | 0.0563
Epoch 10/300, seasonal_0 Loss: 0.0823 | 0.0537
Epoch 11/300, seasonal_0 Loss: 0.0811 | 0.0520
Epoch 12/300, seasonal_0 Loss: 0.0801 | 0.0510
Epoch 13/300, seasonal_0 Loss: 0.0785 | 0.0498
Epoch 14/300, seasonal_0 Loss: 0.0766 | 0.0484
Epoch 15/300, seasonal_0 Loss: 0.0750 | 0.0471
Epoch 16/300, seasonal_0 Loss: 0.0738 | 0.0468
Epoch 17/300, seasonal_0 Loss: 0.0728 | 0.0463
Epoch 18/300, seasonal_0 Loss: 0.0721 | 0.0460
Epoch 19/300, seasonal_0 Loss: 0.0714 | 0.0456
Epoch 20/300, seasonal_0 Loss: 0.0708 | 0.0452
Epoch 21/300, seasonal_0 Loss: 0.0702 | 0.0448
Epoch 22/300, seasonal_0 Loss: 0.0697 | 0.0445
Epoch 23/300, seasonal_0 Loss: 0.0690 | 0.0439
Epoch 24/300, seasonal_0 Loss: 0.0685 | 0.0432
Epoch 25/300, seasonal_0 Loss: 0.0683 | 0.0424
Epoch 26/300, seasonal_0 Loss: 0.0672 | 0.0417
Epoch 27/300, seasonal_0 Loss: 0.0666 | 0.0411
Epoch 28/300, seasonal_0 Loss: 0.0661 | 0.0406
Epoch 29/300, seasonal_0 Loss: 0.0657 | 0.0402
Epoch 30/300, seasonal_0 Loss: 0.0653 | 0.0398
Epoch 31/300, seasonal_0 Loss: 0.0650 | 0.0394
Epoch 32/300, seasonal_0 Loss: 0.0647 | 0.0389
Epoch 33/300, seasonal_0 Loss: 0.0645 | 0.0386
Epoch 34/300, seasonal_0 Loss: 0.0643 | 0.0390
Epoch 35/300, seasonal_0 Loss: 0.0641 | 0.0393
Epoch 36/300, seasonal_0 Loss: 0.0638 | 0.0388
Epoch 37/300, seasonal_0 Loss: 0.0634 | 0.0381
Epoch 38/300, seasonal_0 Loss: 0.0630 | 0.0379
Epoch 39/300, seasonal_0 Loss: 0.0627 | 0.0377
Epoch 40/300, seasonal_0 Loss: 0.0625 | 0.0374
Epoch 41/300, seasonal_0 Loss: 0.0623 | 0.0370
Epoch 42/300, seasonal_0 Loss: 0.0621 | 0.0368
Epoch 43/300, seasonal_0 Loss: 0.0618 | 0.0366
Epoch 44/300, seasonal_0 Loss: 0.0615 | 0.0364
Epoch 45/300, seasonal_0 Loss: 0.0612 | 0.0361
Epoch 46/300, seasonal_0 Loss: 0.0609 | 0.0357
Epoch 47/300, seasonal_0 Loss: 0.0606 | 0.0354
Epoch 48/300, seasonal_0 Loss: 0.0603 | 0.0352
Epoch 49/300, seasonal_0 Loss: 0.0600 | 0.0351
Epoch 50/300, seasonal_0 Loss: 0.0597 | 0.0350
Epoch 51/300, seasonal_0 Loss: 0.0594 | 0.0349
Epoch 52/300, seasonal_0 Loss: 0.0592 | 0.0347
Epoch 53/300, seasonal_0 Loss: 0.0589 | 0.0345
Epoch 54/300, seasonal_0 Loss: 0.0587 | 0.0343
Epoch 55/300, seasonal_0 Loss: 0.0585 | 0.0341
Epoch 56/300, seasonal_0 Loss: 0.0584 | 0.0340
Epoch 57/300, seasonal_0 Loss: 0.0582 | 0.0339
Epoch 58/300, seasonal_0 Loss: 0.0581 | 0.0337
Epoch 59/300, seasonal_0 Loss: 0.0580 | 0.0336
Epoch 60/300, seasonal_0 Loss: 0.0578 | 0.0335
Epoch 61/300, seasonal_0 Loss: 0.0577 | 0.0334
Epoch 62/300, seasonal_0 Loss: 0.0576 | 0.0333
Epoch 63/300, seasonal_0 Loss: 0.0575 | 0.0332
Epoch 64/300, seasonal_0 Loss: 0.0574 | 0.0331
Epoch 65/300, seasonal_0 Loss: 0.0573 | 0.0331
Epoch 66/300, seasonal_0 Loss: 0.0572 | 0.0331
Epoch 67/300, seasonal_0 Loss: 0.0571 | 0.0332
Epoch 68/300, seasonal_0 Loss: 0.0570 | 0.0334
Epoch 69/300, seasonal_0 Loss: 0.0569 | 0.0336
Epoch 70/300, seasonal_0 Loss: 0.0568 | 0.0337
Epoch 71/300, seasonal_0 Loss: 0.0567 | 0.0337
Epoch 72/300, seasonal_0 Loss: 0.0567 | 0.0337
Epoch 73/300, seasonal_0 Loss: 0.0566 | 0.0336
Epoch 74/300, seasonal_0 Loss: 0.0565 | 0.0336
Epoch 75/300, seasonal_0 Loss: 0.0565 | 0.0334
Epoch 76/300, seasonal_0 Loss: 0.0564 | 0.0333
Epoch 77/300, seasonal_0 Loss: 0.0564 | 0.0332
Epoch 78/300, seasonal_0 Loss: 0.0563 | 0.0331
Epoch 79/300, seasonal_0 Loss: 0.0563 | 0.0330
Epoch 80/300, seasonal_0 Loss: 0.0562 | 0.0329
Epoch 81/300, seasonal_0 Loss: 0.0562 | 0.0329
Epoch 82/300, seasonal_0 Loss: 0.0561 | 0.0328
Epoch 83/300, seasonal_0 Loss: 0.0561 | 0.0328
Epoch 84/300, seasonal_0 Loss: 0.0560 | 0.0327
Epoch 85/300, seasonal_0 Loss: 0.0560 | 0.0327
Epoch 86/300, seasonal_0 Loss: 0.0559 | 0.0327
Epoch 87/300, seasonal_0 Loss: 0.0559 | 0.0327
Epoch 88/300, seasonal_0 Loss: 0.0558 | 0.0326
Epoch 89/300, seasonal_0 Loss: 0.0558 | 0.0326
Epoch 90/300, seasonal_0 Loss: 0.0557 | 0.0326
Epoch 91/300, seasonal_0 Loss: 0.0557 | 0.0326
Epoch 92/300, seasonal_0 Loss: 0.0557 | 0.0326
Epoch 93/300, seasonal_0 Loss: 0.0556 | 0.0326
Epoch 94/300, seasonal_0 Loss: 0.0556 | 0.0326
Epoch 95/300, seasonal_0 Loss: 0.0555 | 0.0326
Epoch 96/300, seasonal_0 Loss: 0.0555 | 0.0325
Epoch 97/300, seasonal_0 Loss: 0.0555 | 0.0325
Epoch 98/300, seasonal_0 Loss: 0.0554 | 0.0325
Epoch 99/300, seasonal_0 Loss: 0.0554 | 0.0325
Epoch 100/300, seasonal_0 Loss: 0.0554 | 0.0325
Epoch 101/300, seasonal_0 Loss: 0.0553 | 0.0325
Epoch 102/300, seasonal_0 Loss: 0.0553 | 0.0325
Epoch 103/300, seasonal_0 Loss: 0.0553 | 0.0324
Epoch 104/300, seasonal_0 Loss: 0.0552 | 0.0324
Epoch 105/300, seasonal_0 Loss: 0.0552 | 0.0324
Epoch 106/300, seasonal_0 Loss: 0.0552 | 0.0324
Epoch 107/300, seasonal_0 Loss: 0.0552 | 0.0324
Epoch 108/300, seasonal_0 Loss: 0.0551 | 0.0324
Epoch 109/300, seasonal_0 Loss: 0.0551 | 0.0323
Epoch 110/300, seasonal_0 Loss: 0.0551 | 0.0323
Epoch 111/300, seasonal_0 Loss: 0.0551 | 0.0323
Epoch 112/300, seasonal_0 Loss: 0.0551 | 0.0323
Epoch 113/300, seasonal_0 Loss: 0.0550 | 0.0323
Epoch 114/300, seasonal_0 Loss: 0.0550 | 0.0323
Epoch 115/300, seasonal_0 Loss: 0.0550 | 0.0322
Epoch 116/300, seasonal_0 Loss: 0.0550 | 0.0322
Epoch 117/300, seasonal_0 Loss: 0.0550 | 0.0322
Epoch 118/300, seasonal_0 Loss: 0.0549 | 0.0322
Epoch 119/300, seasonal_0 Loss: 0.0549 | 0.0322
Epoch 120/300, seasonal_0 Loss: 0.0549 | 0.0322
Epoch 121/300, seasonal_0 Loss: 0.0549 | 0.0322
Epoch 122/300, seasonal_0 Loss: 0.0549 | 0.0322
Epoch 123/300, seasonal_0 Loss: 0.0549 | 0.0322
Epoch 124/300, seasonal_0 Loss: 0.0549 | 0.0322
Epoch 125/300, seasonal_0 Loss: 0.0548 | 0.0322
Epoch 126/300, seasonal_0 Loss: 0.0548 | 0.0322
Epoch 127/300, seasonal_0 Loss: 0.0548 | 0.0322
Epoch 128/300, seasonal_0 Loss: 0.0548 | 0.0322
Epoch 129/300, seasonal_0 Loss: 0.0548 | 0.0322
Epoch 130/300, seasonal_0 Loss: 0.0548 | 0.0322
Epoch 131/300, seasonal_0 Loss: 0.0548 | 0.0322
Epoch 132/300, seasonal_0 Loss: 0.0548 | 0.0322
Epoch 133/300, seasonal_0 Loss: 0.0548 | 0.0322
Epoch 134/300, seasonal_0 Loss: 0.0548 | 0.0322
Epoch 135/300, seasonal_0 Loss: 0.0547 | 0.0322
Epoch 136/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 137/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 138/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 139/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 140/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 141/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 142/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 143/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 144/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 145/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 146/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 147/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 148/300, seasonal_0 Loss: 0.0547 | 0.0321
Epoch 149/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 150/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 151/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 152/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 153/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 154/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 155/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 156/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 157/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 158/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 159/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 160/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 161/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 162/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 163/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 164/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 165/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 166/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 167/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 168/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 169/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 170/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 171/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 172/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 173/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 174/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 175/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 176/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 177/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 178/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 179/300, seasonal_0 Loss: 0.0546 | 0.0321
Epoch 180/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 181/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 182/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 183/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 184/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 185/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 186/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 187/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 188/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 189/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 190/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 191/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 192/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 193/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 194/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 195/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 196/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 197/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 198/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 199/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 200/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 201/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 202/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 203/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 204/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 205/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 206/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 207/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 208/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 209/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 210/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 211/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 212/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 213/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 214/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 215/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 216/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 217/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 218/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 219/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 220/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 221/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 222/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 223/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 224/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 225/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 226/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 227/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 228/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 229/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 230/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 231/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 232/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 233/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 234/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 235/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 236/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 237/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 238/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 239/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 240/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 241/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 242/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 243/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 244/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 245/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 246/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 247/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 248/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 249/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 250/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 251/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 252/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 253/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 254/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 255/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 256/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 257/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 258/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 259/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 260/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 261/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 262/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 263/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 264/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 265/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 266/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 267/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 268/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 269/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 270/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 271/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 272/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 273/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 274/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 275/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 276/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 277/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 278/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 279/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 280/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 281/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 282/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 283/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 284/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 285/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 286/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 287/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 288/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 289/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 290/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 291/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 292/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 293/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 294/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 295/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 296/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 297/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 298/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 299/300, seasonal_0 Loss: 0.0545 | 0.0321
Epoch 300/300, seasonal_0 Loss: 0.0545 | 0.0321
Training seasonal_1 component with params: {'observation_period_num': 7, 'train_rates': 0.8920441787148885, 'learning_rate': 0.0005785192857039385, 'batch_size': 46, 'step_size': 7, 'gamma': 0.7508645565849613}
Epoch 1/300, seasonal_1 Loss: 0.2252 | 0.0959
Epoch 2/300, seasonal_1 Loss: 0.1169 | 0.0813
Epoch 3/300, seasonal_1 Loss: 0.1067 | 0.0737
Epoch 4/300, seasonal_1 Loss: 0.0990 | 0.0650
Epoch 5/300, seasonal_1 Loss: 0.0920 | 0.0584
Epoch 6/300, seasonal_1 Loss: 0.0874 | 0.0537
Epoch 7/300, seasonal_1 Loss: 0.0851 | 0.0512
Epoch 8/300, seasonal_1 Loss: 0.0834 | 0.0501
Epoch 9/300, seasonal_1 Loss: 0.0819 | 0.0498
Epoch 10/300, seasonal_1 Loss: 0.0791 | 0.0474
Epoch 11/300, seasonal_1 Loss: 0.0771 | 0.0459
Epoch 12/300, seasonal_1 Loss: 0.0742 | 0.0381
Epoch 13/300, seasonal_1 Loss: 0.0739 | 0.0386
Epoch 14/300, seasonal_1 Loss: 0.0735 | 0.0387
Epoch 15/300, seasonal_1 Loss: 0.0720 | 0.0396
Epoch 16/300, seasonal_1 Loss: 0.0710 | 0.0384
Epoch 17/300, seasonal_1 Loss: 0.0706 | 0.0370
Epoch 18/300, seasonal_1 Loss: 0.0695 | 0.0366
Epoch 19/300, seasonal_1 Loss: 0.0683 | 0.0362
Epoch 20/300, seasonal_1 Loss: 0.0680 | 0.0359
Epoch 21/300, seasonal_1 Loss: 0.0675 | 0.0353
Epoch 22/300, seasonal_1 Loss: 0.0669 | 0.0356
Epoch 23/300, seasonal_1 Loss: 0.0668 | 0.0352
Epoch 24/300, seasonal_1 Loss: 0.0662 | 0.0349
Epoch 25/300, seasonal_1 Loss: 0.0657 | 0.0344
Epoch 26/300, seasonal_1 Loss: 0.0653 | 0.0341
Epoch 27/300, seasonal_1 Loss: 0.0652 | 0.0339
Epoch 28/300, seasonal_1 Loss: 0.0646 | 0.0336
Epoch 29/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 30/300, seasonal_1 Loss: 0.0640 | 0.0327
Epoch 31/300, seasonal_1 Loss: 0.0636 | 0.0325
Epoch 32/300, seasonal_1 Loss: 0.0633 | 0.0325
Epoch 33/300, seasonal_1 Loss: 0.0633 | 0.0321
Epoch 34/300, seasonal_1 Loss: 0.0632 | 0.0321
Epoch 35/300, seasonal_1 Loss: 0.0632 | 0.0320
Epoch 36/300, seasonal_1 Loss: 0.0634 | 0.0328
Epoch 37/300, seasonal_1 Loss: 0.0638 | 0.0330
Epoch 38/300, seasonal_1 Loss: 0.0635 | 0.0330
Epoch 39/300, seasonal_1 Loss: 0.0631 | 0.0329
Epoch 40/300, seasonal_1 Loss: 0.0626 | 0.0322
Epoch 41/300, seasonal_1 Loss: 0.0623 | 0.0320
Epoch 42/300, seasonal_1 Loss: 0.0619 | 0.0320
Epoch 43/300, seasonal_1 Loss: 0.0617 | 0.0317
Epoch 44/300, seasonal_1 Loss: 0.0616 | 0.0317
Epoch 45/300, seasonal_1 Loss: 0.0615 | 0.0316
Epoch 46/300, seasonal_1 Loss: 0.0614 | 0.0316
Epoch 47/300, seasonal_1 Loss: 0.0613 | 0.0313
Epoch 48/300, seasonal_1 Loss: 0.0612 | 0.0312
Epoch 49/300, seasonal_1 Loss: 0.0612 | 0.0311
Epoch 50/300, seasonal_1 Loss: 0.0611 | 0.0310
Epoch 51/300, seasonal_1 Loss: 0.0611 | 0.0310
Epoch 52/300, seasonal_1 Loss: 0.0610 | 0.0309
Epoch 53/300, seasonal_1 Loss: 0.0609 | 0.0309
Epoch 54/300, seasonal_1 Loss: 0.0608 | 0.0310
Epoch 55/300, seasonal_1 Loss: 0.0608 | 0.0310
Epoch 56/300, seasonal_1 Loss: 0.0607 | 0.0309
Epoch 57/300, seasonal_1 Loss: 0.0607 | 0.0311
Epoch 58/300, seasonal_1 Loss: 0.0607 | 0.0311
Epoch 59/300, seasonal_1 Loss: 0.0606 | 0.0311
Epoch 60/300, seasonal_1 Loss: 0.0606 | 0.0310
Epoch 61/300, seasonal_1 Loss: 0.0606 | 0.0312
Epoch 62/300, seasonal_1 Loss: 0.0606 | 0.0312
Epoch 63/300, seasonal_1 Loss: 0.0605 | 0.0312
Epoch 64/300, seasonal_1 Loss: 0.0605 | 0.0311
Epoch 65/300, seasonal_1 Loss: 0.0605 | 0.0311
Epoch 66/300, seasonal_1 Loss: 0.0605 | 0.0311
Epoch 67/300, seasonal_1 Loss: 0.0605 | 0.0310
Epoch 68/300, seasonal_1 Loss: 0.0605 | 0.0310
Epoch 69/300, seasonal_1 Loss: 0.0604 | 0.0310
Epoch 70/300, seasonal_1 Loss: 0.0604 | 0.0310
Epoch 71/300, seasonal_1 Loss: 0.0605 | 0.0310
Epoch 72/300, seasonal_1 Loss: 0.0605 | 0.0310
Epoch 73/300, seasonal_1 Loss: 0.0605 | 0.0310
Epoch 74/300, seasonal_1 Loss: 0.0604 | 0.0310
Epoch 75/300, seasonal_1 Loss: 0.0605 | 0.0310
Epoch 76/300, seasonal_1 Loss: 0.0605 | 0.0310
Epoch 77/300, seasonal_1 Loss: 0.0605 | 0.0310
Epoch 78/300, seasonal_1 Loss: 0.0604 | 0.0310
Epoch 79/300, seasonal_1 Loss: 0.0604 | 0.0310
Epoch 80/300, seasonal_1 Loss: 0.0604 | 0.0310
Epoch 81/300, seasonal_1 Loss: 0.0604 | 0.0310
Epoch 82/300, seasonal_1 Loss: 0.0603 | 0.0310
Epoch 83/300, seasonal_1 Loss: 0.0603 | 0.0310
Epoch 84/300, seasonal_1 Loss: 0.0603 | 0.0310
Epoch 85/300, seasonal_1 Loss: 0.0603 | 0.0310
Epoch 86/300, seasonal_1 Loss: 0.0603 | 0.0310
Epoch 87/300, seasonal_1 Loss: 0.0603 | 0.0310
Epoch 88/300, seasonal_1 Loss: 0.0603 | 0.0309
Epoch 89/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 90/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 91/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 92/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 93/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 94/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 95/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 96/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 97/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 98/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 99/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 100/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 101/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 102/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 103/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 104/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 105/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 106/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 107/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 108/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 109/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 110/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 111/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 112/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 113/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 114/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 115/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 116/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 117/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 118/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 119/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 120/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 121/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 122/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 123/300, seasonal_1 Loss: 0.0602 | 0.0309
Epoch 124/300, seasonal_1 Loss: 0.0602 | 0.0308
Epoch 125/300, seasonal_1 Loss: 0.0602 | 0.0308
Epoch 126/300, seasonal_1 Loss: 0.0602 | 0.0308
Epoch 127/300, seasonal_1 Loss: 0.0602 | 0.0308
Epoch 128/300, seasonal_1 Loss: 0.0602 | 0.0308
Epoch 129/300, seasonal_1 Loss: 0.0602 | 0.0308
Epoch 130/300, seasonal_1 Loss: 0.0602 | 0.0308
Epoch 131/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 132/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 133/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 134/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 135/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 136/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 137/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 138/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 139/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 140/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 141/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 142/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 143/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 144/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 145/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 146/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 147/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 148/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 149/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 150/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 151/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 152/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 153/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 154/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 155/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 156/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 157/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 158/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 159/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 160/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 161/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 162/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 163/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 164/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 165/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 166/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 167/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 168/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 169/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 170/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 171/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 172/300, seasonal_1 Loss: 0.0601 | 0.0308
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.9721057059041374, 'learning_rate': 2.6986725893176912e-05, 'batch_size': 35, 'step_size': 4, 'gamma': 0.9798039034905054}
Epoch 1/300, seasonal_2 Loss: 0.2769 | 0.2335
Epoch 2/300, seasonal_2 Loss: 0.1600 | 0.1434
Epoch 3/300, seasonal_2 Loss: 0.1359 | 0.1215
Epoch 4/300, seasonal_2 Loss: 0.1274 | 0.1079
Epoch 5/300, seasonal_2 Loss: 0.1218 | 0.0980
Epoch 6/300, seasonal_2 Loss: 0.1178 | 0.0909
Epoch 7/300, seasonal_2 Loss: 0.1147 | 0.0858
Epoch 8/300, seasonal_2 Loss: 0.1116 | 0.0821
Epoch 9/300, seasonal_2 Loss: 0.1088 | 0.0793
Epoch 10/300, seasonal_2 Loss: 0.1063 | 0.0771
Epoch 11/300, seasonal_2 Loss: 0.1043 | 0.0752
Epoch 12/300, seasonal_2 Loss: 0.1027 | 0.0736
Epoch 13/300, seasonal_2 Loss: 0.1012 | 0.0722
Epoch 14/300, seasonal_2 Loss: 0.1000 | 0.0708
Epoch 15/300, seasonal_2 Loss: 0.0988 | 0.0697
Epoch 16/300, seasonal_2 Loss: 0.0977 | 0.0686
Epoch 17/300, seasonal_2 Loss: 0.0967 | 0.0676
Epoch 18/300, seasonal_2 Loss: 0.0956 | 0.0666
Epoch 19/300, seasonal_2 Loss: 0.0947 | 0.0658
Epoch 20/300, seasonal_2 Loss: 0.0937 | 0.0650
Epoch 21/300, seasonal_2 Loss: 0.0927 | 0.0641
Epoch 22/300, seasonal_2 Loss: 0.0918 | 0.0634
Epoch 23/300, seasonal_2 Loss: 0.0908 | 0.0626
Epoch 24/300, seasonal_2 Loss: 0.0898 | 0.0619
Epoch 25/300, seasonal_2 Loss: 0.0888 | 0.0612
Epoch 26/300, seasonal_2 Loss: 0.0879 | 0.0605
Epoch 27/300, seasonal_2 Loss: 0.0869 | 0.0598
Epoch 28/300, seasonal_2 Loss: 0.0860 | 0.0592
Epoch 29/300, seasonal_2 Loss: 0.0851 | 0.0585
Epoch 30/300, seasonal_2 Loss: 0.0842 | 0.0578
Epoch 31/300, seasonal_2 Loss: 0.0834 | 0.0571
Epoch 32/300, seasonal_2 Loss: 0.0827 | 0.0564
Epoch 33/300, seasonal_2 Loss: 0.0819 | 0.0557
Epoch 34/300, seasonal_2 Loss: 0.0812 | 0.0550
Epoch 35/300, seasonal_2 Loss: 0.0805 | 0.0543
Epoch 36/300, seasonal_2 Loss: 0.0799 | 0.0537
Epoch 37/300, seasonal_2 Loss: 0.0793 | 0.0531
Epoch 38/300, seasonal_2 Loss: 0.0787 | 0.0525
Epoch 39/300, seasonal_2 Loss: 0.0782 | 0.0519
Epoch 40/300, seasonal_2 Loss: 0.0777 | 0.0513
Epoch 41/300, seasonal_2 Loss: 0.0772 | 0.0507
Epoch 42/300, seasonal_2 Loss: 0.0767 | 0.0501
Epoch 43/300, seasonal_2 Loss: 0.0763 | 0.0496
Epoch 44/300, seasonal_2 Loss: 0.0759 | 0.0490
Epoch 45/300, seasonal_2 Loss: 0.0755 | 0.0485
Epoch 46/300, seasonal_2 Loss: 0.0751 | 0.0480
Epoch 47/300, seasonal_2 Loss: 0.0748 | 0.0475
Epoch 48/300, seasonal_2 Loss: 0.0745 | 0.0470
Epoch 49/300, seasonal_2 Loss: 0.0741 | 0.0466
Epoch 50/300, seasonal_2 Loss: 0.0738 | 0.0461
Epoch 51/300, seasonal_2 Loss: 0.0735 | 0.0457
Epoch 52/300, seasonal_2 Loss: 0.0732 | 0.0453
Epoch 53/300, seasonal_2 Loss: 0.0730 | 0.0450
Epoch 54/300, seasonal_2 Loss: 0.0727 | 0.0446
Epoch 55/300, seasonal_2 Loss: 0.0724 | 0.0443
Epoch 56/300, seasonal_2 Loss: 0.0721 | 0.0439
Epoch 57/300, seasonal_2 Loss: 0.0718 | 0.0436
Epoch 58/300, seasonal_2 Loss: 0.0716 | 0.0433
Epoch 59/300, seasonal_2 Loss: 0.0713 | 0.0429
Epoch 60/300, seasonal_2 Loss: 0.0710 | 0.0426
Epoch 61/300, seasonal_2 Loss: 0.0707 | 0.0423
Epoch 62/300, seasonal_2 Loss: 0.0705 | 0.0420
Epoch 63/300, seasonal_2 Loss: 0.0702 | 0.0416
Epoch 64/300, seasonal_2 Loss: 0.0699 | 0.0414
Epoch 65/300, seasonal_2 Loss: 0.0696 | 0.0411
Epoch 66/300, seasonal_2 Loss: 0.0694 | 0.0408
Epoch 67/300, seasonal_2 Loss: 0.0691 | 0.0405
Epoch 68/300, seasonal_2 Loss: 0.0689 | 0.0403
Epoch 69/300, seasonal_2 Loss: 0.0687 | 0.0401
Epoch 70/300, seasonal_2 Loss: 0.0685 | 0.0399
Epoch 71/300, seasonal_2 Loss: 0.0683 | 0.0397
Epoch 72/300, seasonal_2 Loss: 0.0681 | 0.0396
Epoch 73/300, seasonal_2 Loss: 0.0679 | 0.0394
Epoch 74/300, seasonal_2 Loss: 0.0678 | 0.0392
Epoch 75/300, seasonal_2 Loss: 0.0676 | 0.0391
Epoch 76/300, seasonal_2 Loss: 0.0675 | 0.0389
Epoch 77/300, seasonal_2 Loss: 0.0673 | 0.0388
Epoch 78/300, seasonal_2 Loss: 0.0672 | 0.0387
Epoch 79/300, seasonal_2 Loss: 0.0671 | 0.0386
Epoch 80/300, seasonal_2 Loss: 0.0669 | 0.0384
Epoch 81/300, seasonal_2 Loss: 0.0668 | 0.0383
Epoch 82/300, seasonal_2 Loss: 0.0667 | 0.0382
Epoch 83/300, seasonal_2 Loss: 0.0666 | 0.0381
Epoch 84/300, seasonal_2 Loss: 0.0665 | 0.0380
Epoch 85/300, seasonal_2 Loss: 0.0664 | 0.0379
Epoch 86/300, seasonal_2 Loss: 0.0663 | 0.0378
Epoch 87/300, seasonal_2 Loss: 0.0662 | 0.0377
Epoch 88/300, seasonal_2 Loss: 0.0661 | 0.0376
Epoch 89/300, seasonal_2 Loss: 0.0660 | 0.0375
Epoch 90/300, seasonal_2 Loss: 0.0659 | 0.0374
Epoch 91/300, seasonal_2 Loss: 0.0658 | 0.0374
Epoch 92/300, seasonal_2 Loss: 0.0657 | 0.0373
Epoch 93/300, seasonal_2 Loss: 0.0656 | 0.0372
Epoch 94/300, seasonal_2 Loss: 0.0655 | 0.0371
Epoch 95/300, seasonal_2 Loss: 0.0654 | 0.0371
Epoch 96/300, seasonal_2 Loss: 0.0654 | 0.0370
Epoch 97/300, seasonal_2 Loss: 0.0653 | 0.0369
Epoch 98/300, seasonal_2 Loss: 0.0652 | 0.0369
Epoch 99/300, seasonal_2 Loss: 0.0651 | 0.0368
Epoch 100/300, seasonal_2 Loss: 0.0650 | 0.0368
Epoch 101/300, seasonal_2 Loss: 0.0649 | 0.0367
Epoch 102/300, seasonal_2 Loss: 0.0648 | 0.0366
Epoch 103/300, seasonal_2 Loss: 0.0648 | 0.0365
Epoch 104/300, seasonal_2 Loss: 0.0647 | 0.0365
Epoch 105/300, seasonal_2 Loss: 0.0646 | 0.0364
Epoch 106/300, seasonal_2 Loss: 0.0645 | 0.0363
Epoch 107/300, seasonal_2 Loss: 0.0644 | 0.0362
Epoch 108/300, seasonal_2 Loss: 0.0644 | 0.0362
Epoch 109/300, seasonal_2 Loss: 0.0643 | 0.0361
Epoch 110/300, seasonal_2 Loss: 0.0642 | 0.0360
Epoch 111/300, seasonal_2 Loss: 0.0641 | 0.0359
Epoch 112/300, seasonal_2 Loss: 0.0640 | 0.0359
Epoch 113/300, seasonal_2 Loss: 0.0640 | 0.0358
Epoch 114/300, seasonal_2 Loss: 0.0639 | 0.0357
Epoch 115/300, seasonal_2 Loss: 0.0638 | 0.0357
Epoch 116/300, seasonal_2 Loss: 0.0637 | 0.0356
Epoch 117/300, seasonal_2 Loss: 0.0637 | 0.0355
Epoch 118/300, seasonal_2 Loss: 0.0636 | 0.0355
Epoch 119/300, seasonal_2 Loss: 0.0635 | 0.0354
Epoch 120/300, seasonal_2 Loss: 0.0635 | 0.0354
Epoch 121/300, seasonal_2 Loss: 0.0634 | 0.0353
Epoch 122/300, seasonal_2 Loss: 0.0633 | 0.0352
Epoch 123/300, seasonal_2 Loss: 0.0633 | 0.0352
Epoch 124/300, seasonal_2 Loss: 0.0632 | 0.0351
Epoch 125/300, seasonal_2 Loss: 0.0632 | 0.0351
Epoch 126/300, seasonal_2 Loss: 0.0631 | 0.0350
Epoch 127/300, seasonal_2 Loss: 0.0630 | 0.0350
Epoch 128/300, seasonal_2 Loss: 0.0630 | 0.0349
Epoch 129/300, seasonal_2 Loss: 0.0629 | 0.0349
Epoch 130/300, seasonal_2 Loss: 0.0629 | 0.0349
Epoch 131/300, seasonal_2 Loss: 0.0628 | 0.0348
Epoch 132/300, seasonal_2 Loss: 0.0628 | 0.0348
Epoch 133/300, seasonal_2 Loss: 0.0627 | 0.0348
Epoch 134/300, seasonal_2 Loss: 0.0627 | 0.0347
Epoch 135/300, seasonal_2 Loss: 0.0627 | 0.0347
Epoch 136/300, seasonal_2 Loss: 0.0626 | 0.0347
Epoch 137/300, seasonal_2 Loss: 0.0626 | 0.0346
Epoch 138/300, seasonal_2 Loss: 0.0625 | 0.0346
Epoch 139/300, seasonal_2 Loss: 0.0625 | 0.0346
Epoch 140/300, seasonal_2 Loss: 0.0625 | 0.0346
Epoch 141/300, seasonal_2 Loss: 0.0624 | 0.0346
Epoch 142/300, seasonal_2 Loss: 0.0624 | 0.0345
Epoch 143/300, seasonal_2 Loss: 0.0624 | 0.0345
Epoch 144/300, seasonal_2 Loss: 0.0623 | 0.0345
Epoch 145/300, seasonal_2 Loss: 0.0623 | 0.0345
Epoch 146/300, seasonal_2 Loss: 0.0623 | 0.0345
Epoch 147/300, seasonal_2 Loss: 0.0623 | 0.0344
Epoch 148/300, seasonal_2 Loss: 0.0622 | 0.0344
Epoch 149/300, seasonal_2 Loss: 0.0622 | 0.0343
Epoch 150/300, seasonal_2 Loss: 0.0621 | 0.0343
Epoch 151/300, seasonal_2 Loss: 0.0621 | 0.0342
Epoch 152/300, seasonal_2 Loss: 0.0621 | 0.0342
Epoch 153/300, seasonal_2 Loss: 0.0620 | 0.0341
Epoch 154/300, seasonal_2 Loss: 0.0620 | 0.0341
Epoch 155/300, seasonal_2 Loss: 0.0619 | 0.0341
Epoch 156/300, seasonal_2 Loss: 0.0619 | 0.0341
Epoch 157/300, seasonal_2 Loss: 0.0619 | 0.0341
Epoch 158/300, seasonal_2 Loss: 0.0618 | 0.0340
Epoch 159/300, seasonal_2 Loss: 0.0618 | 0.0340
Epoch 160/300, seasonal_2 Loss: 0.0618 | 0.0340
Epoch 161/300, seasonal_2 Loss: 0.0617 | 0.0340
Epoch 162/300, seasonal_2 Loss: 0.0617 | 0.0340
Epoch 163/300, seasonal_2 Loss: 0.0617 | 0.0340
Epoch 164/300, seasonal_2 Loss: 0.0616 | 0.0340
Epoch 165/300, seasonal_2 Loss: 0.0616 | 0.0340
Epoch 166/300, seasonal_2 Loss: 0.0616 | 0.0340
Epoch 167/300, seasonal_2 Loss: 0.0616 | 0.0340
Epoch 168/300, seasonal_2 Loss: 0.0616 | 0.0340
Epoch 169/300, seasonal_2 Loss: 0.0616 | 0.0340
Epoch 170/300, seasonal_2 Loss: 0.0615 | 0.0340
Epoch 171/300, seasonal_2 Loss: 0.0615 | 0.0340
Epoch 172/300, seasonal_2 Loss: 0.0615 | 0.0340
Epoch 173/300, seasonal_2 Loss: 0.0615 | 0.0341
Epoch 174/300, seasonal_2 Loss: 0.0615 | 0.0341
Epoch 175/300, seasonal_2 Loss: 0.0616 | 0.0341
Epoch 176/300, seasonal_2 Loss: 0.0616 | 0.0341
Epoch 177/300, seasonal_2 Loss: 0.0617 | 0.0341
Epoch 178/300, seasonal_2 Loss: 0.0618 | 0.0339
Epoch 179/300, seasonal_2 Loss: 0.0621 | 0.0338
Epoch 180/300, seasonal_2 Loss: 0.0624 | 0.0337
Epoch 181/300, seasonal_2 Loss: 0.0623 | 0.0337
Epoch 182/300, seasonal_2 Loss: 0.0617 | 0.0338
Epoch 183/300, seasonal_2 Loss: 0.0615 | 0.0339
Epoch 184/300, seasonal_2 Loss: 0.0614 | 0.0339
Epoch 185/300, seasonal_2 Loss: 0.0613 | 0.0339
Epoch 186/300, seasonal_2 Loss: 0.0613 | 0.0339
Epoch 187/300, seasonal_2 Loss: 0.0613 | 0.0338
Epoch 188/300, seasonal_2 Loss: 0.0612 | 0.0338
Epoch 189/300, seasonal_2 Loss: 0.0612 | 0.0337
Epoch 190/300, seasonal_2 Loss: 0.0612 | 0.0337
Epoch 191/300, seasonal_2 Loss: 0.0612 | 0.0336
Epoch 192/300, seasonal_2 Loss: 0.0611 | 0.0336
Epoch 193/300, seasonal_2 Loss: 0.0611 | 0.0335
Epoch 194/300, seasonal_2 Loss: 0.0611 | 0.0335
Epoch 195/300, seasonal_2 Loss: 0.0611 | 0.0334
Epoch 196/300, seasonal_2 Loss: 0.0611 | 0.0334
Epoch 197/300, seasonal_2 Loss: 0.0610 | 0.0333
Epoch 198/300, seasonal_2 Loss: 0.0610 | 0.0333
Epoch 199/300, seasonal_2 Loss: 0.0610 | 0.0333
Epoch 200/300, seasonal_2 Loss: 0.0610 | 0.0332
Epoch 201/300, seasonal_2 Loss: 0.0610 | 0.0332
Epoch 202/300, seasonal_2 Loss: 0.0610 | 0.0332
Epoch 203/300, seasonal_2 Loss: 0.0610 | 0.0331
Epoch 204/300, seasonal_2 Loss: 0.0609 | 0.0331
Epoch 205/300, seasonal_2 Loss: 0.0609 | 0.0331
Epoch 206/300, seasonal_2 Loss: 0.0609 | 0.0331
Epoch 207/300, seasonal_2 Loss: 0.0609 | 0.0330
Epoch 208/300, seasonal_2 Loss: 0.0609 | 0.0330
Epoch 209/300, seasonal_2 Loss: 0.0608 | 0.0330
Epoch 210/300, seasonal_2 Loss: 0.0608 | 0.0330
Epoch 211/300, seasonal_2 Loss: 0.0608 | 0.0330
Epoch 212/300, seasonal_2 Loss: 0.0608 | 0.0329
Epoch 213/300, seasonal_2 Loss: 0.0608 | 0.0329
Epoch 214/300, seasonal_2 Loss: 0.0608 | 0.0329
Epoch 215/300, seasonal_2 Loss: 0.0607 | 0.0329
Epoch 216/300, seasonal_2 Loss: 0.0607 | 0.0329
Epoch 217/300, seasonal_2 Loss: 0.0607 | 0.0329
Epoch 218/300, seasonal_2 Loss: 0.0607 | 0.0328
Epoch 219/300, seasonal_2 Loss: 0.0607 | 0.0328
Epoch 220/300, seasonal_2 Loss: 0.0606 | 0.0328
Epoch 221/300, seasonal_2 Loss: 0.0606 | 0.0328
Epoch 222/300, seasonal_2 Loss: 0.0606 | 0.0328
Epoch 223/300, seasonal_2 Loss: 0.0606 | 0.0328
Epoch 224/300, seasonal_2 Loss: 0.0606 | 0.0327
Epoch 225/300, seasonal_2 Loss: 0.0606 | 0.0327
Epoch 226/300, seasonal_2 Loss: 0.0605 | 0.0327
Epoch 227/300, seasonal_2 Loss: 0.0605 | 0.0327
Epoch 228/300, seasonal_2 Loss: 0.0605 | 0.0327
Epoch 229/300, seasonal_2 Loss: 0.0605 | 0.0327
Epoch 230/300, seasonal_2 Loss: 0.0605 | 0.0327
Epoch 231/300, seasonal_2 Loss: 0.0605 | 0.0326
Epoch 232/300, seasonal_2 Loss: 0.0605 | 0.0326
Epoch 233/300, seasonal_2 Loss: 0.0604 | 0.0326
Epoch 234/300, seasonal_2 Loss: 0.0604 | 0.0326
Epoch 235/300, seasonal_2 Loss: 0.0604 | 0.0326
Epoch 236/300, seasonal_2 Loss: 0.0604 | 0.0326
Epoch 237/300, seasonal_2 Loss: 0.0604 | 0.0326
Epoch 238/300, seasonal_2 Loss: 0.0604 | 0.0326
Epoch 239/300, seasonal_2 Loss: 0.0604 | 0.0325
Epoch 240/300, seasonal_2 Loss: 0.0604 | 0.0325
Epoch 241/300, seasonal_2 Loss: 0.0604 | 0.0325
Epoch 242/300, seasonal_2 Loss: 0.0603 | 0.0325
Epoch 243/300, seasonal_2 Loss: 0.0603 | 0.0325
Epoch 244/300, seasonal_2 Loss: 0.0603 | 0.0325
Epoch 245/300, seasonal_2 Loss: 0.0603 | 0.0324
Epoch 246/300, seasonal_2 Loss: 0.0603 | 0.0324
Epoch 247/300, seasonal_2 Loss: 0.0603 | 0.0324
Epoch 248/300, seasonal_2 Loss: 0.0603 | 0.0324
Epoch 249/300, seasonal_2 Loss: 0.0603 | 0.0324
Epoch 250/300, seasonal_2 Loss: 0.0603 | 0.0323
Epoch 251/300, seasonal_2 Loss: 0.0603 | 0.0323
Epoch 252/300, seasonal_2 Loss: 0.0603 | 0.0323
Epoch 253/300, seasonal_2 Loss: 0.0602 | 0.0323
Epoch 254/300, seasonal_2 Loss: 0.0602 | 0.0323
Epoch 255/300, seasonal_2 Loss: 0.0602 | 0.0322
Epoch 256/300, seasonal_2 Loss: 0.0602 | 0.0322
Epoch 257/300, seasonal_2 Loss: 0.0602 | 0.0322
Epoch 258/300, seasonal_2 Loss: 0.0602 | 0.0322
Epoch 259/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 260/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 261/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 262/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 263/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 264/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 265/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 266/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 267/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 268/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 269/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 270/300, seasonal_2 Loss: 0.0602 | 0.0321
Epoch 271/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 272/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 273/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 274/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 275/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 276/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 277/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 278/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 279/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 280/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 281/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 282/300, seasonal_2 Loss: 0.0601 | 0.0321
Epoch 283/300, seasonal_2 Loss: 0.0600 | 0.0321
Epoch 284/300, seasonal_2 Loss: 0.0600 | 0.0321
Epoch 285/300, seasonal_2 Loss: 0.0600 | 0.0320
Epoch 286/300, seasonal_2 Loss: 0.0600 | 0.0320
Epoch 287/300, seasonal_2 Loss: 0.0600 | 0.0320
Epoch 288/300, seasonal_2 Loss: 0.0600 | 0.0320
Epoch 289/300, seasonal_2 Loss: 0.0600 | 0.0320
Epoch 290/300, seasonal_2 Loss: 0.0600 | 0.0320
Epoch 291/300, seasonal_2 Loss: 0.0600 | 0.0320
Epoch 292/300, seasonal_2 Loss: 0.0600 | 0.0320
Epoch 293/300, seasonal_2 Loss: 0.0600 | 0.0319
Epoch 294/300, seasonal_2 Loss: 0.0599 | 0.0319
Epoch 295/300, seasonal_2 Loss: 0.0599 | 0.0319
Epoch 296/300, seasonal_2 Loss: 0.0599 | 0.0319
Epoch 297/300, seasonal_2 Loss: 0.0599 | 0.0319
Epoch 298/300, seasonal_2 Loss: 0.0599 | 0.0319
Epoch 299/300, seasonal_2 Loss: 0.0599 | 0.0319
Epoch 300/300, seasonal_2 Loss: 0.0599 | 0.0319
Training seasonal_3 component with params: {'observation_period_num': 6, 'train_rates': 0.8283829661707363, 'learning_rate': 0.0009201202878626857, 'batch_size': 19, 'step_size': 6, 'gamma': 0.9144951337031751}
Epoch 1/300, seasonal_3 Loss: 0.2154 | 0.0807
Epoch 2/300, seasonal_3 Loss: 0.1133 | 0.0974
Epoch 3/300, seasonal_3 Loss: 0.1112 | 0.0719
Epoch 4/300, seasonal_3 Loss: 0.0998 | 0.0635
Epoch 5/300, seasonal_3 Loss: 0.0955 | 0.0573
Epoch 6/300, seasonal_3 Loss: 0.0916 | 0.0546
Epoch 7/300, seasonal_3 Loss: 0.0895 | 0.0587
Epoch 8/300, seasonal_3 Loss: 0.0872 | 0.0519
Epoch 9/300, seasonal_3 Loss: 0.0855 | 0.0538
Epoch 10/300, seasonal_3 Loss: 0.0827 | 0.0462
Epoch 11/300, seasonal_3 Loss: 0.0807 | 0.0461
Epoch 12/300, seasonal_3 Loss: 0.0800 | 0.0473
Epoch 13/300, seasonal_3 Loss: 0.0785 | 0.0511
Epoch 14/300, seasonal_3 Loss: 0.0766 | 0.0523
Epoch 15/300, seasonal_3 Loss: 0.0745 | 0.0498
Epoch 16/300, seasonal_3 Loss: 0.0724 | 0.0438
Epoch 17/300, seasonal_3 Loss: 0.0714 | 0.0464
Epoch 18/300, seasonal_3 Loss: 0.0728 | 0.0444
Epoch 19/300, seasonal_3 Loss: 0.0719 | 0.0473
Epoch 20/300, seasonal_3 Loss: 0.0694 | 0.0431
Epoch 21/300, seasonal_3 Loss: 0.0686 | 0.0398
Epoch 22/300, seasonal_3 Loss: 0.0667 | 0.0380
Epoch 23/300, seasonal_3 Loss: 0.0650 | 0.0391
Epoch 24/300, seasonal_3 Loss: 0.0638 | 0.0382
Epoch 25/300, seasonal_3 Loss: 0.0638 | 0.0350
Epoch 26/300, seasonal_3 Loss: 0.0623 | 0.0358
Epoch 27/300, seasonal_3 Loss: 0.0621 | 0.0392
Epoch 28/300, seasonal_3 Loss: 0.0624 | 0.0348
Epoch 29/300, seasonal_3 Loss: 0.0623 | 0.0340
Epoch 30/300, seasonal_3 Loss: 0.0604 | 0.0336
Epoch 31/300, seasonal_3 Loss: 0.0593 | 0.0326
Epoch 32/300, seasonal_3 Loss: 0.0596 | 0.0355
Epoch 33/300, seasonal_3 Loss: 0.0594 | 0.0322
Epoch 34/300, seasonal_3 Loss: 0.0589 | 0.0312
Epoch 35/300, seasonal_3 Loss: 0.0584 | 0.0319
Epoch 36/300, seasonal_3 Loss: 0.0570 | 0.0312
Epoch 37/300, seasonal_3 Loss: 0.0560 | 0.0353
Epoch 38/300, seasonal_3 Loss: 0.0583 | 0.0308
Epoch 39/300, seasonal_3 Loss: 0.0559 | 0.0322
Epoch 40/300, seasonal_3 Loss: 0.0548 | 0.0379
Epoch 41/300, seasonal_3 Loss: 0.0532 | 0.0498
Epoch 42/300, seasonal_3 Loss: 0.0655 | 0.0376
Epoch 43/300, seasonal_3 Loss: 0.0581 | 0.0331
Epoch 44/300, seasonal_3 Loss: 0.0557 | 0.0390
Epoch 45/300, seasonal_3 Loss: 0.0578 | 0.0472
Epoch 46/300, seasonal_3 Loss: 0.0549 | 0.0646
Epoch 47/300, seasonal_3 Loss: 0.0574 | 0.1391
Epoch 48/300, seasonal_3 Loss: 0.0582 | 0.0392
Epoch 49/300, seasonal_3 Loss: 0.0530 | 0.0370
Epoch 50/300, seasonal_3 Loss: 0.0513 | 0.0314
Epoch 51/300, seasonal_3 Loss: 0.0536 | 0.0319
Epoch 52/300, seasonal_3 Loss: 0.0519 | 0.0314
Epoch 53/300, seasonal_3 Loss: 0.0452 | 0.0545
Epoch 54/300, seasonal_3 Loss: 0.0456 | 0.0332
Epoch 55/300, seasonal_3 Loss: 0.0523 | 0.0315
Epoch 56/300, seasonal_3 Loss: 0.0509 | 0.0320
Epoch 57/300, seasonal_3 Loss: 0.0504 | 0.0307
Epoch 58/300, seasonal_3 Loss: 0.0503 | 0.0321
Epoch 59/300, seasonal_3 Loss: 0.0501 | 0.0341
Epoch 60/300, seasonal_3 Loss: 0.0495 | 0.0610
Epoch 61/300, seasonal_3 Loss: 0.0422 | 0.0311
Epoch 62/300, seasonal_3 Loss: 0.0429 | 0.0292
Epoch 63/300, seasonal_3 Loss: 0.0493 | 0.0304
Epoch 64/300, seasonal_3 Loss: 0.0483 | 0.0313
Epoch 65/300, seasonal_3 Loss: 0.0477 | 0.0336
Epoch 66/300, seasonal_3 Loss: 0.0451 | 0.2260
Epoch 67/300, seasonal_3 Loss: 0.0555 | 0.0397
Epoch 68/300, seasonal_3 Loss: 0.0551 | 0.0329
Epoch 69/300, seasonal_3 Loss: 0.0507 | 0.0317
Epoch 70/300, seasonal_3 Loss: 0.0492 | 0.0316
Epoch 71/300, seasonal_3 Loss: 0.0485 | 0.0307
Epoch 72/300, seasonal_3 Loss: 0.0480 | 0.0322
Epoch 73/300, seasonal_3 Loss: 0.0476 | 0.0309
Epoch 74/300, seasonal_3 Loss: 0.0473 | 0.0325
Epoch 75/300, seasonal_3 Loss: 0.0470 | 0.0318
Epoch 76/300, seasonal_3 Loss: 0.0467 | 0.0327
Epoch 77/300, seasonal_3 Loss: 0.0463 | 0.0324
Epoch 78/300, seasonal_3 Loss: 0.0458 | 0.0350
Epoch 79/300, seasonal_3 Loss: 0.0398 | 0.0919
Epoch 80/300, seasonal_3 Loss: 0.0409 | 0.0330
Epoch 81/300, seasonal_3 Loss: 0.0364 | 0.0328
Epoch 82/300, seasonal_3 Loss: 0.0356 | 0.0332
Epoch 83/300, seasonal_3 Loss: 0.0351 | 0.0323
Epoch 84/300, seasonal_3 Loss: 0.0347 | 0.0331
Epoch 85/300, seasonal_3 Loss: 0.0344 | 0.0319
Epoch 86/300, seasonal_3 Loss: 0.0341 | 0.0330
Epoch 87/300, seasonal_3 Loss: 0.0338 | 0.0321
Epoch 88/300, seasonal_3 Loss: 0.0335 | 0.0330
Epoch 89/300, seasonal_3 Loss: 0.0332 | 0.0322
Epoch 90/300, seasonal_3 Loss: 0.0330 | 0.0330
Epoch 91/300, seasonal_3 Loss: 0.0328 | 0.0323
Epoch 92/300, seasonal_3 Loss: 0.0325 | 0.0331
Epoch 93/300, seasonal_3 Loss: 0.0323 | 0.0326
Epoch 94/300, seasonal_3 Loss: 0.0320 | 0.0333
Epoch 95/300, seasonal_3 Loss: 0.0319 | 0.0328
Epoch 96/300, seasonal_3 Loss: 0.0317 | 0.0335
Epoch 97/300, seasonal_3 Loss: 0.0316 | 0.0331
Epoch 98/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 99/300, seasonal_3 Loss: 0.0312 | 0.0335
Epoch 100/300, seasonal_3 Loss: 0.0309 | 0.0341
Epoch 101/300, seasonal_3 Loss: 0.0307 | 0.0337
Epoch 102/300, seasonal_3 Loss: 0.0305 | 0.0341
Epoch 103/300, seasonal_3 Loss: 0.0303 | 0.0339
Epoch 104/300, seasonal_3 Loss: 0.0301 | 0.0343
Epoch 105/300, seasonal_3 Loss: 0.0300 | 0.0342
Epoch 106/300, seasonal_3 Loss: 0.0298 | 0.0344
Epoch 107/300, seasonal_3 Loss: 0.0296 | 0.0344
Epoch 108/300, seasonal_3 Loss: 0.0295 | 0.0346
Epoch 109/300, seasonal_3 Loss: 0.0294 | 0.0346
Epoch 110/300, seasonal_3 Loss: 0.0292 | 0.0348
Epoch 111/300, seasonal_3 Loss: 0.0291 | 0.0347
Epoch 112/300, seasonal_3 Loss: 0.0289 | 0.0350
Epoch 113/300, seasonal_3 Loss: 0.0288 | 0.0350
Epoch 114/300, seasonal_3 Loss: 0.0287 | 0.0351
Epoch 115/300, seasonal_3 Loss: 0.0285 | 0.0350
Epoch 116/300, seasonal_3 Loss: 0.0284 | 0.0353
Epoch 117/300, seasonal_3 Loss: 0.0283 | 0.0354
Epoch 118/300, seasonal_3 Loss: 0.0282 | 0.0354
Epoch 119/300, seasonal_3 Loss: 0.0281 | 0.0353
Epoch 120/300, seasonal_3 Loss: 0.0280 | 0.0355
Epoch 121/300, seasonal_3 Loss: 0.0279 | 0.0355
Epoch 122/300, seasonal_3 Loss: 0.0278 | 0.0355
Epoch 123/300, seasonal_3 Loss: 0.0278 | 0.0355
Epoch 124/300, seasonal_3 Loss: 0.0277 | 0.0355
Epoch 125/300, seasonal_3 Loss: 0.0276 | 0.0354
Epoch 126/300, seasonal_3 Loss: 0.0276 | 0.0354
Epoch 127/300, seasonal_3 Loss: 0.0275 | 0.0355
Epoch 128/300, seasonal_3 Loss: 0.0275 | 0.0355
Epoch 129/300, seasonal_3 Loss: 0.0274 | 0.0357
Epoch 130/300, seasonal_3 Loss: 0.0273 | 0.0358
Epoch 131/300, seasonal_3 Loss: 0.0272 | 0.0361
Epoch 132/300, seasonal_3 Loss: 0.0271 | 0.0360
Epoch 133/300, seasonal_3 Loss: 0.0270 | 0.0365
Epoch 134/300, seasonal_3 Loss: 0.0269 | 0.0363
Epoch 135/300, seasonal_3 Loss: 0.0268 | 0.0366
Epoch 136/300, seasonal_3 Loss: 0.0267 | 0.0365
Epoch 137/300, seasonal_3 Loss: 0.0266 | 0.0367
Epoch 138/300, seasonal_3 Loss: 0.0266 | 0.0366
Epoch 139/300, seasonal_3 Loss: 0.0265 | 0.0368
Epoch 140/300, seasonal_3 Loss: 0.0264 | 0.0368
Epoch 141/300, seasonal_3 Loss: 0.0264 | 0.0369
Epoch 142/300, seasonal_3 Loss: 0.0263 | 0.0369
Epoch 143/300, seasonal_3 Loss: 0.0263 | 0.0370
Epoch 144/300, seasonal_3 Loss: 0.0262 | 0.0371
Epoch 145/300, seasonal_3 Loss: 0.0261 | 0.0372
Epoch 146/300, seasonal_3 Loss: 0.0261 | 0.0372
Epoch 147/300, seasonal_3 Loss: 0.0260 | 0.0373
Epoch 148/300, seasonal_3 Loss: 0.0260 | 0.0374
Epoch 149/300, seasonal_3 Loss: 0.0259 | 0.0375
Epoch 150/300, seasonal_3 Loss: 0.0259 | 0.0375
Epoch 151/300, seasonal_3 Loss: 0.0259 | 0.0375
Epoch 152/300, seasonal_3 Loss: 0.0258 | 0.0376
Epoch 153/300, seasonal_3 Loss: 0.0258 | 0.0377
Epoch 154/300, seasonal_3 Loss: 0.0257 | 0.0376
Epoch 155/300, seasonal_3 Loss: 0.0257 | 0.0377
Epoch 156/300, seasonal_3 Loss: 0.0257 | 0.0377
Epoch 157/300, seasonal_3 Loss: 0.0256 | 0.0376
Epoch 158/300, seasonal_3 Loss: 0.0256 | 0.0376
Epoch 159/300, seasonal_3 Loss: 0.0256 | 0.0376
Epoch 160/300, seasonal_3 Loss: 0.0256 | 0.0375
Epoch 161/300, seasonal_3 Loss: 0.0255 | 0.0375
Epoch 162/300, seasonal_3 Loss: 0.0255 | 0.0375
Epoch 163/300, seasonal_3 Loss: 0.0255 | 0.0374
Epoch 164/300, seasonal_3 Loss: 0.0255 | 0.0374
Epoch 165/300, seasonal_3 Loss: 0.0254 | 0.0373
Epoch 166/300, seasonal_3 Loss: 0.0254 | 0.0373
Epoch 167/300, seasonal_3 Loss: 0.0254 | 0.0373
Epoch 168/300, seasonal_3 Loss: 0.0254 | 0.0373
Epoch 169/300, seasonal_3 Loss: 0.0254 | 0.0372
Epoch 170/300, seasonal_3 Loss: 0.0254 | 0.0372
Epoch 171/300, seasonal_3 Loss: 0.0253 | 0.0373
Epoch 172/300, seasonal_3 Loss: 0.0253 | 0.0373
Epoch 173/300, seasonal_3 Loss: 0.0253 | 0.0373
Epoch 174/300, seasonal_3 Loss: 0.0253 | 0.0373
Epoch 175/300, seasonal_3 Loss: 0.0252 | 0.0373
Epoch 176/300, seasonal_3 Loss: 0.0252 | 0.0373
Epoch 177/300, seasonal_3 Loss: 0.0252 | 0.0373
Epoch 178/300, seasonal_3 Loss: 0.0251 | 0.0373
Epoch 179/300, seasonal_3 Loss: 0.0251 | 0.0373
Epoch 180/300, seasonal_3 Loss: 0.0251 | 0.0373
Epoch 181/300, seasonal_3 Loss: 0.0251 | 0.0373
Epoch 182/300, seasonal_3 Loss: 0.0250 | 0.0373
Epoch 183/300, seasonal_3 Loss: 0.0250 | 0.0373
Epoch 184/300, seasonal_3 Loss: 0.0250 | 0.0373
Epoch 185/300, seasonal_3 Loss: 0.0250 | 0.0373
Epoch 186/300, seasonal_3 Loss: 0.0250 | 0.0373
Epoch 187/300, seasonal_3 Loss: 0.0249 | 0.0373
Epoch 188/300, seasonal_3 Loss: 0.0249 | 0.0373
Epoch 189/300, seasonal_3 Loss: 0.0249 | 0.0373
Epoch 190/300, seasonal_3 Loss: 0.0249 | 0.0373
Epoch 191/300, seasonal_3 Loss: 0.0249 | 0.0373
Epoch 192/300, seasonal_3 Loss: 0.0249 | 0.0373
Epoch 193/300, seasonal_3 Loss: 0.0249 | 0.0373
Epoch 194/300, seasonal_3 Loss: 0.0249 | 0.0373
Epoch 195/300, seasonal_3 Loss: 0.0248 | 0.0373
Epoch 196/300, seasonal_3 Loss: 0.0248 | 0.0373
Epoch 197/300, seasonal_3 Loss: 0.0248 | 0.0373
Epoch 198/300, seasonal_3 Loss: 0.0248 | 0.0373
Epoch 199/300, seasonal_3 Loss: 0.0248 | 0.0373
Epoch 200/300, seasonal_3 Loss: 0.0248 | 0.0373
Epoch 201/300, seasonal_3 Loss: 0.0248 | 0.0373
Epoch 202/300, seasonal_3 Loss: 0.0248 | 0.0374
Epoch 203/300, seasonal_3 Loss: 0.0248 | 0.0374
Epoch 204/300, seasonal_3 Loss: 0.0247 | 0.0374
Epoch 205/300, seasonal_3 Loss: 0.0247 | 0.0374
Epoch 206/300, seasonal_3 Loss: 0.0247 | 0.0374
Epoch 207/300, seasonal_3 Loss: 0.0247 | 0.0374
Epoch 208/300, seasonal_3 Loss: 0.0247 | 0.0375
Epoch 209/300, seasonal_3 Loss: 0.0247 | 0.0375
Epoch 210/300, seasonal_3 Loss: 0.0247 | 0.0375
Epoch 211/300, seasonal_3 Loss: 0.0247 | 0.0375
Epoch 212/300, seasonal_3 Loss: 0.0247 | 0.0375
Epoch 213/300, seasonal_3 Loss: 0.0247 | 0.0375
Epoch 214/300, seasonal_3 Loss: 0.0246 | 0.0375
Epoch 215/300, seasonal_3 Loss: 0.0246 | 0.0375
Epoch 216/300, seasonal_3 Loss: 0.0246 | 0.0375
Epoch 217/300, seasonal_3 Loss: 0.0246 | 0.0376
Epoch 218/300, seasonal_3 Loss: 0.0246 | 0.0376
Epoch 219/300, seasonal_3 Loss: 0.0246 | 0.0376
Epoch 220/300, seasonal_3 Loss: 0.0246 | 0.0376
Epoch 221/300, seasonal_3 Loss: 0.0246 | 0.0376
Epoch 222/300, seasonal_3 Loss: 0.0246 | 0.0376
Epoch 223/300, seasonal_3 Loss: 0.0246 | 0.0376
Epoch 224/300, seasonal_3 Loss: 0.0246 | 0.0376
Epoch 225/300, seasonal_3 Loss: 0.0246 | 0.0376
Epoch 226/300, seasonal_3 Loss: 0.0246 | 0.0376
Epoch 227/300, seasonal_3 Loss: 0.0245 | 0.0376
Epoch 228/300, seasonal_3 Loss: 0.0245 | 0.0376
Epoch 229/300, seasonal_3 Loss: 0.0245 | 0.0376
Epoch 230/300, seasonal_3 Loss: 0.0245 | 0.0376
Epoch 231/300, seasonal_3 Loss: 0.0245 | 0.0376
Epoch 232/300, seasonal_3 Loss: 0.0245 | 0.0376
Epoch 233/300, seasonal_3 Loss: 0.0245 | 0.0376
Epoch 234/300, seasonal_3 Loss: 0.0245 | 0.0376
Epoch 235/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 236/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 237/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 238/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 239/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 240/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 241/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 242/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 243/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 244/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 245/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 246/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 247/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 248/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 249/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 250/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 251/300, seasonal_3 Loss: 0.0245 | 0.0377
Epoch 252/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 253/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 254/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 255/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 256/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 257/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 258/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 259/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 260/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 261/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 262/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 263/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 264/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 265/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 266/300, seasonal_3 Loss: 0.0244 | 0.0377
Epoch 267/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 268/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 269/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 270/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 271/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 272/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 273/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 274/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 275/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 276/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 277/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 278/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 279/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 280/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 281/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 282/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 283/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 284/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 285/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 286/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 287/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 288/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 289/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 290/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 291/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 292/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 293/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 294/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 295/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 296/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 297/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 298/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 299/300, seasonal_3 Loss: 0.0244 | 0.0378
Epoch 300/300, seasonal_3 Loss: 0.0244 | 0.0378
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.8532207686105857, 'learning_rate': 0.0002646308826891474, 'batch_size': 151, 'step_size': 13, 'gamma': 0.8020851996082073}
Epoch 1/300, resid Loss: 0.2924 | 0.2404
Epoch 2/300, resid Loss: 0.1710 | 0.1311
Epoch 3/300, resid Loss: 0.1387 | 0.1364
Epoch 4/300, resid Loss: 0.1432 | 0.1731
Epoch 5/300, resid Loss: 0.1755 | 0.1654
Epoch 6/300, resid Loss: 0.2191 | 0.1510
Epoch 7/300, resid Loss: 0.1899 | 0.1828
Epoch 8/300, resid Loss: 0.2022 | 0.1011
Epoch 9/300, resid Loss: 0.1613 | 0.1348
Epoch 10/300, resid Loss: 0.1458 | 0.1049
Epoch 11/300, resid Loss: 0.1321 | 0.0837
Epoch 12/300, resid Loss: 0.1239 | 0.0829
Epoch 13/300, resid Loss: 0.1249 | 0.0788
Epoch 14/300, resid Loss: 0.1220 | 0.0788
Epoch 15/300, resid Loss: 0.1231 | 0.0744
Epoch 16/300, resid Loss: 0.1105 | 0.0727
Epoch 17/300, resid Loss: 0.1267 | 0.0855
Epoch 18/300, resid Loss: 0.1401 | 0.0717
Epoch 19/300, resid Loss: 0.1221 | 0.0722
Epoch 20/300, resid Loss: 0.1127 | 0.0775
Epoch 21/300, resid Loss: 0.1255 | 0.0782
Epoch 22/300, resid Loss: 0.1086 | 0.0692
Epoch 23/300, resid Loss: 0.1041 | 0.0712
Epoch 24/300, resid Loss: 0.1055 | 0.0704
Epoch 25/300, resid Loss: 0.1029 | 0.0688
Epoch 26/300, resid Loss: 0.1014 | 0.0693
Epoch 27/300, resid Loss: 0.1001 | 0.0671
Epoch 28/300, resid Loss: 0.0989 | 0.0657
Epoch 29/300, resid Loss: 0.0987 | 0.0664
Epoch 30/300, resid Loss: 0.0980 | 0.0654
Epoch 31/300, resid Loss: 0.0978 | 0.0651
Epoch 32/300, resid Loss: 0.0972 | 0.0646
Epoch 33/300, resid Loss: 0.0970 | 0.0640
Epoch 34/300, resid Loss: 0.0962 | 0.0631
Epoch 35/300, resid Loss: 0.0959 | 0.0642
Epoch 36/300, resid Loss: 0.0955 | 0.0624
Epoch 37/300, resid Loss: 0.0951 | 0.0630
Epoch 38/300, resid Loss: 0.0947 | 0.0618
Epoch 39/300, resid Loss: 0.0943 | 0.0619
Epoch 40/300, resid Loss: 0.0938 | 0.0606
Epoch 41/300, resid Loss: 0.0934 | 0.0619
Epoch 42/300, resid Loss: 0.0930 | 0.0599
Epoch 43/300, resid Loss: 0.0926 | 0.0607
Epoch 44/300, resid Loss: 0.0923 | 0.0592
Epoch 45/300, resid Loss: 0.0919 | 0.0597
Epoch 46/300, resid Loss: 0.0915 | 0.0584
Epoch 47/300, resid Loss: 0.0910 | 0.0591
Epoch 48/300, resid Loss: 0.0907 | 0.0573
Epoch 49/300, resid Loss: 0.0904 | 0.0581
Epoch 50/300, resid Loss: 0.0900 | 0.0567
Epoch 51/300, resid Loss: 0.0897 | 0.0571
Epoch 52/300, resid Loss: 0.0894 | 0.0560
Epoch 53/300, resid Loss: 0.0890 | 0.0564
Epoch 54/300, resid Loss: 0.0887 | 0.0552
Epoch 55/300, resid Loss: 0.0884 | 0.0556
Epoch 56/300, resid Loss: 0.0882 | 0.0547
Epoch 57/300, resid Loss: 0.0879 | 0.0549
Epoch 58/300, resid Loss: 0.0876 | 0.0541
Epoch 59/300, resid Loss: 0.0873 | 0.0542
Epoch 60/300, resid Loss: 0.0870 | 0.0534
Epoch 61/300, resid Loss: 0.0867 | 0.0535
Epoch 62/300, resid Loss: 0.0864 | 0.0529
Epoch 63/300, resid Loss: 0.0862 | 0.0529
Epoch 64/300, resid Loss: 0.0859 | 0.0524
Epoch 65/300, resid Loss: 0.0857 | 0.0522
Epoch 66/300, resid Loss: 0.0854 | 0.0518
Epoch 67/300, resid Loss: 0.0852 | 0.0517
Epoch 68/300, resid Loss: 0.0850 | 0.0514
Epoch 69/300, resid Loss: 0.0849 | 0.0513
Epoch 70/300, resid Loss: 0.0847 | 0.0510
Epoch 71/300, resid Loss: 0.0845 | 0.0508
Epoch 72/300, resid Loss: 0.0843 | 0.0506
Epoch 73/300, resid Loss: 0.0841 | 0.0504
Epoch 74/300, resid Loss: 0.0840 | 0.0502
Epoch 75/300, resid Loss: 0.0839 | 0.0501
Epoch 76/300, resid Loss: 0.0838 | 0.0499
Epoch 77/300, resid Loss: 0.0836 | 0.0498
Epoch 78/300, resid Loss: 0.0835 | 0.0496
Epoch 79/300, resid Loss: 0.0834 | 0.0495
Epoch 80/300, resid Loss: 0.0833 | 0.0494
Epoch 81/300, resid Loss: 0.0832 | 0.0493
Epoch 82/300, resid Loss: 0.0831 | 0.0492
Epoch 83/300, resid Loss: 0.0830 | 0.0490
Epoch 84/300, resid Loss: 0.0829 | 0.0489
Epoch 85/300, resid Loss: 0.0829 | 0.0488
Epoch 86/300, resid Loss: 0.0828 | 0.0487
Epoch 87/300, resid Loss: 0.0827 | 0.0486
Epoch 88/300, resid Loss: 0.0826 | 0.0485
Epoch 89/300, resid Loss: 0.0826 | 0.0485
Epoch 90/300, resid Loss: 0.0825 | 0.0484
Epoch 91/300, resid Loss: 0.0824 | 0.0483
Epoch 92/300, resid Loss: 0.0823 | 0.0482
Epoch 93/300, resid Loss: 0.0823 | 0.0481
Epoch 94/300, resid Loss: 0.0822 | 0.0481
Epoch 95/300, resid Loss: 0.0822 | 0.0480
Epoch 96/300, resid Loss: 0.0821 | 0.0479
Epoch 97/300, resid Loss: 0.0821 | 0.0479
Epoch 98/300, resid Loss: 0.0820 | 0.0478
Epoch 99/300, resid Loss: 0.0820 | 0.0477
Epoch 100/300, resid Loss: 0.0819 | 0.0477
Epoch 101/300, resid Loss: 0.0819 | 0.0476
Epoch 102/300, resid Loss: 0.0818 | 0.0475
Epoch 103/300, resid Loss: 0.0818 | 0.0475
Epoch 104/300, resid Loss: 0.0817 | 0.0474
Epoch 105/300, resid Loss: 0.0817 | 0.0474
Epoch 106/300, resid Loss: 0.0816 | 0.0473
Epoch 107/300, resid Loss: 0.0816 | 0.0473
Epoch 108/300, resid Loss: 0.0816 | 0.0472
Epoch 109/300, resid Loss: 0.0815 | 0.0472
Epoch 110/300, resid Loss: 0.0815 | 0.0471
Epoch 111/300, resid Loss: 0.0815 | 0.0471
Epoch 112/300, resid Loss: 0.0814 | 0.0470
Epoch 113/300, resid Loss: 0.0814 | 0.0470
Epoch 114/300, resid Loss: 0.0814 | 0.0470
Epoch 115/300, resid Loss: 0.0813 | 0.0469
Epoch 116/300, resid Loss: 0.0813 | 0.0469
Epoch 117/300, resid Loss: 0.0813 | 0.0469
Epoch 118/300, resid Loss: 0.0812 | 0.0468
Epoch 119/300, resid Loss: 0.0812 | 0.0468
Epoch 120/300, resid Loss: 0.0812 | 0.0468
Epoch 121/300, resid Loss: 0.0812 | 0.0467
Epoch 122/300, resid Loss: 0.0811 | 0.0467
Epoch 123/300, resid Loss: 0.0811 | 0.0467
Epoch 124/300, resid Loss: 0.0811 | 0.0466
Epoch 125/300, resid Loss: 0.0811 | 0.0466
Epoch 126/300, resid Loss: 0.0811 | 0.0466
Epoch 127/300, resid Loss: 0.0810 | 0.0466
Epoch 128/300, resid Loss: 0.0810 | 0.0465
Epoch 129/300, resid Loss: 0.0810 | 0.0465
Epoch 130/300, resid Loss: 0.0810 | 0.0465
Epoch 131/300, resid Loss: 0.0810 | 0.0465
Epoch 132/300, resid Loss: 0.0809 | 0.0464
Epoch 133/300, resid Loss: 0.0809 | 0.0464
Epoch 134/300, resid Loss: 0.0809 | 0.0464
Epoch 135/300, resid Loss: 0.0809 | 0.0464
Epoch 136/300, resid Loss: 0.0809 | 0.0463
Epoch 137/300, resid Loss: 0.0809 | 0.0463
Epoch 138/300, resid Loss: 0.0808 | 0.0463
Epoch 139/300, resid Loss: 0.0808 | 0.0463
Epoch 140/300, resid Loss: 0.0808 | 0.0463
Epoch 141/300, resid Loss: 0.0808 | 0.0463
Epoch 142/300, resid Loss: 0.0808 | 0.0462
Epoch 143/300, resid Loss: 0.0808 | 0.0462
Epoch 144/300, resid Loss: 0.0808 | 0.0462
Epoch 145/300, resid Loss: 0.0807 | 0.0462
Epoch 146/300, resid Loss: 0.0807 | 0.0462
Epoch 147/300, resid Loss: 0.0807 | 0.0462
Epoch 148/300, resid Loss: 0.0807 | 0.0462
Epoch 149/300, resid Loss: 0.0807 | 0.0461
Epoch 150/300, resid Loss: 0.0807 | 0.0461
Epoch 151/300, resid Loss: 0.0807 | 0.0461
Epoch 152/300, resid Loss: 0.0807 | 0.0461
Epoch 153/300, resid Loss: 0.0807 | 0.0461
Epoch 154/300, resid Loss: 0.0807 | 0.0461
Epoch 155/300, resid Loss: 0.0806 | 0.0461
Epoch 156/300, resid Loss: 0.0806 | 0.0461
Epoch 157/300, resid Loss: 0.0806 | 0.0460
Epoch 158/300, resid Loss: 0.0806 | 0.0460
Epoch 159/300, resid Loss: 0.0806 | 0.0460
Epoch 160/300, resid Loss: 0.0806 | 0.0460
Epoch 161/300, resid Loss: 0.0806 | 0.0460
Epoch 162/300, resid Loss: 0.0806 | 0.0460
Epoch 163/300, resid Loss: 0.0806 | 0.0460
Epoch 164/300, resid Loss: 0.0806 | 0.0460
Epoch 165/300, resid Loss: 0.0806 | 0.0460
Epoch 166/300, resid Loss: 0.0806 | 0.0460
Epoch 167/300, resid Loss: 0.0806 | 0.0460
Epoch 168/300, resid Loss: 0.0805 | 0.0460
Epoch 169/300, resid Loss: 0.0805 | 0.0459
Epoch 170/300, resid Loss: 0.0805 | 0.0459
Epoch 171/300, resid Loss: 0.0805 | 0.0459
Epoch 172/300, resid Loss: 0.0805 | 0.0459
Epoch 173/300, resid Loss: 0.0805 | 0.0459
Epoch 174/300, resid Loss: 0.0805 | 0.0459
Epoch 175/300, resid Loss: 0.0805 | 0.0459
Epoch 176/300, resid Loss: 0.0805 | 0.0459
Epoch 177/300, resid Loss: 0.0805 | 0.0459
Epoch 178/300, resid Loss: 0.0805 | 0.0459
Epoch 179/300, resid Loss: 0.0805 | 0.0459
Epoch 180/300, resid Loss: 0.0805 | 0.0459
Epoch 181/300, resid Loss: 0.0805 | 0.0459
Epoch 182/300, resid Loss: 0.0805 | 0.0459
Epoch 183/300, resid Loss: 0.0805 | 0.0459
Epoch 184/300, resid Loss: 0.0805 | 0.0459
Epoch 185/300, resid Loss: 0.0805 | 0.0459
Epoch 186/300, resid Loss: 0.0805 | 0.0459
Epoch 187/300, resid Loss: 0.0805 | 0.0458
Epoch 188/300, resid Loss: 0.0805 | 0.0458
Epoch 189/300, resid Loss: 0.0805 | 0.0458
Epoch 190/300, resid Loss: 0.0805 | 0.0458
Epoch 191/300, resid Loss: 0.0805 | 0.0458
Epoch 192/300, resid Loss: 0.0805 | 0.0458
Epoch 193/300, resid Loss: 0.0804 | 0.0458
Epoch 194/300, resid Loss: 0.0804 | 0.0458
Epoch 195/300, resid Loss: 0.0804 | 0.0458
Epoch 196/300, resid Loss: 0.0804 | 0.0458
Epoch 197/300, resid Loss: 0.0804 | 0.0458
Epoch 198/300, resid Loss: 0.0804 | 0.0458
Epoch 199/300, resid Loss: 0.0804 | 0.0458
Epoch 200/300, resid Loss: 0.0804 | 0.0458
Epoch 201/300, resid Loss: 0.0804 | 0.0458
Epoch 202/300, resid Loss: 0.0804 | 0.0458
Epoch 203/300, resid Loss: 0.0804 | 0.0458
Epoch 204/300, resid Loss: 0.0804 | 0.0458
Epoch 205/300, resid Loss: 0.0804 | 0.0458
Epoch 206/300, resid Loss: 0.0804 | 0.0458
Epoch 207/300, resid Loss: 0.0804 | 0.0458
Epoch 208/300, resid Loss: 0.0804 | 0.0458
Epoch 209/300, resid Loss: 0.0804 | 0.0458
Epoch 210/300, resid Loss: 0.0804 | 0.0458
Epoch 211/300, resid Loss: 0.0804 | 0.0458
Epoch 212/300, resid Loss: 0.0804 | 0.0458
Epoch 213/300, resid Loss: 0.0804 | 0.0458
Epoch 214/300, resid Loss: 0.0804 | 0.0458
Epoch 215/300, resid Loss: 0.0804 | 0.0458
Epoch 216/300, resid Loss: 0.0804 | 0.0458
Epoch 217/300, resid Loss: 0.0804 | 0.0458
Epoch 218/300, resid Loss: 0.0804 | 0.0458
Epoch 219/300, resid Loss: 0.0804 | 0.0458
Epoch 220/300, resid Loss: 0.0804 | 0.0458
Epoch 221/300, resid Loss: 0.0804 | 0.0458
Epoch 222/300, resid Loss: 0.0804 | 0.0458
Epoch 223/300, resid Loss: 0.0804 | 0.0458
Epoch 224/300, resid Loss: 0.0804 | 0.0458
Epoch 225/300, resid Loss: 0.0804 | 0.0458
Epoch 226/300, resid Loss: 0.0804 | 0.0458
Epoch 227/300, resid Loss: 0.0804 | 0.0458
Epoch 228/300, resid Loss: 0.0804 | 0.0458
Epoch 229/300, resid Loss: 0.0804 | 0.0458
Epoch 230/300, resid Loss: 0.0804 | 0.0458
Epoch 231/300, resid Loss: 0.0804 | 0.0458
Epoch 232/300, resid Loss: 0.0804 | 0.0458
Epoch 233/300, resid Loss: 0.0804 | 0.0457
Epoch 234/300, resid Loss: 0.0804 | 0.0457
Epoch 235/300, resid Loss: 0.0804 | 0.0457
Epoch 236/300, resid Loss: 0.0804 | 0.0457
Epoch 237/300, resid Loss: 0.0804 | 0.0457
Epoch 238/300, resid Loss: 0.0804 | 0.0457
Epoch 239/300, resid Loss: 0.0804 | 0.0457
Epoch 240/300, resid Loss: 0.0804 | 0.0457
Epoch 241/300, resid Loss: 0.0804 | 0.0457
Epoch 242/300, resid Loss: 0.0804 | 0.0457
Epoch 243/300, resid Loss: 0.0804 | 0.0457
Epoch 244/300, resid Loss: 0.0804 | 0.0457
Epoch 245/300, resid Loss: 0.0804 | 0.0457
Epoch 246/300, resid Loss: 0.0804 | 0.0457
Epoch 247/300, resid Loss: 0.0804 | 0.0457
Epoch 248/300, resid Loss: 0.0804 | 0.0457
Epoch 249/300, resid Loss: 0.0804 | 0.0457
Epoch 250/300, resid Loss: 0.0804 | 0.0457
Epoch 251/300, resid Loss: 0.0804 | 0.0457
Epoch 252/300, resid Loss: 0.0804 | 0.0457
Epoch 253/300, resid Loss: 0.0804 | 0.0457
Epoch 254/300, resid Loss: 0.0804 | 0.0457
Epoch 255/300, resid Loss: 0.0804 | 0.0457
Epoch 256/300, resid Loss: 0.0804 | 0.0457
Epoch 257/300, resid Loss: 0.0804 | 0.0457
Epoch 258/300, resid Loss: 0.0804 | 0.0457
Epoch 259/300, resid Loss: 0.0804 | 0.0457
Epoch 260/300, resid Loss: 0.0804 | 0.0457
Epoch 261/300, resid Loss: 0.0804 | 0.0457
Epoch 262/300, resid Loss: 0.0804 | 0.0457
Epoch 263/300, resid Loss: 0.0804 | 0.0457
Epoch 264/300, resid Loss: 0.0804 | 0.0457
Epoch 265/300, resid Loss: 0.0804 | 0.0457
Epoch 266/300, resid Loss: 0.0804 | 0.0457
Epoch 267/300, resid Loss: 0.0804 | 0.0457
Epoch 268/300, resid Loss: 0.0804 | 0.0457
Epoch 269/300, resid Loss: 0.0804 | 0.0457
Epoch 270/300, resid Loss: 0.0804 | 0.0457
Epoch 271/300, resid Loss: 0.0804 | 0.0457
Epoch 272/300, resid Loss: 0.0804 | 0.0457
Epoch 273/300, resid Loss: 0.0804 | 0.0457
Epoch 274/300, resid Loss: 0.0804 | 0.0457
Epoch 275/300, resid Loss: 0.0804 | 0.0457
Epoch 276/300, resid Loss: 0.0804 | 0.0457
Epoch 277/300, resid Loss: 0.0804 | 0.0457
Epoch 278/300, resid Loss: 0.0804 | 0.0457
Epoch 279/300, resid Loss: 0.0804 | 0.0457
Epoch 280/300, resid Loss: 0.0804 | 0.0457
Epoch 281/300, resid Loss: 0.0804 | 0.0457
Epoch 282/300, resid Loss: 0.0804 | 0.0457
Epoch 283/300, resid Loss: 0.0804 | 0.0457
Epoch 284/300, resid Loss: 0.0804 | 0.0457
Epoch 285/300, resid Loss: 0.0804 | 0.0457
Epoch 286/300, resid Loss: 0.0804 | 0.0457
Epoch 287/300, resid Loss: 0.0804 | 0.0457
Epoch 288/300, resid Loss: 0.0804 | 0.0457
Epoch 289/300, resid Loss: 0.0804 | 0.0457
Epoch 290/300, resid Loss: 0.0804 | 0.0457
Epoch 291/300, resid Loss: 0.0804 | 0.0457
Epoch 292/300, resid Loss: 0.0804 | 0.0457
Epoch 293/300, resid Loss: 0.0804 | 0.0457
Epoch 294/300, resid Loss: 0.0804 | 0.0457
Epoch 295/300, resid Loss: 0.0804 | 0.0457
Epoch 296/300, resid Loss: 0.0804 | 0.0457
Epoch 297/300, resid Loss: 0.0804 | 0.0457
Epoch 298/300, resid Loss: 0.0804 | 0.0457
Epoch 299/300, resid Loss: 0.0804 | 0.0457
Epoch 300/300, resid Loss: 0.0804 | 0.0457
Runtime (seconds): 3377.8472259044647
9.007966235471077e-05
[165.32031]
[-0.43367004]
[-2.8949304]
[13.41187]
[8.315267]
[12.575461]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 24.150259785586968
RMSE: 4.9142913818359375
MAE: 4.9142913818359375
R-squared: nan
[196.2943]
