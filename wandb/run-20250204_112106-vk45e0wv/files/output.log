[32m[I 2025-02-04 11:21:13,178][0m A new study created in memory with name: no-name-196e100a-a339-4e77-940c-a38110cc77fb[0m
[32m[I 2025-02-04 11:21:40,544][0m Trial 0 finished with value: 0.2913013811490579 and parameters: {'observation_period_num': 149, 'train_rates': 0.6674670087734246, 'learning_rate': 5.002120464539033e-05, 'batch_size': 181, 'step_size': 8, 'gamma': 0.9775532852711282}. Best is trial 0 with value: 0.2913013811490579.[0m
[32m[I 2025-02-04 11:22:10,318][0m Trial 1 finished with value: 0.03779123547045808 and parameters: {'observation_period_num': 11, 'train_rates': 0.8457603633480626, 'learning_rate': 0.0006189286847715342, 'batch_size': 199, 'step_size': 13, 'gamma': 0.9802341699451057}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:22:44,232][0m Trial 2 finished with value: 0.43062836465185694 and parameters: {'observation_period_num': 73, 'train_rates': 0.7895798672236514, 'learning_rate': 8.568925850847262e-06, 'batch_size': 155, 'step_size': 9, 'gamma': 0.7525637148268964}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:23:43,052][0m Trial 3 finished with value: 0.1339590595743791 and parameters: {'observation_period_num': 132, 'train_rates': 0.6504551072435736, 'learning_rate': 0.0004619645845204886, 'batch_size': 75, 'step_size': 14, 'gamma': 0.7651184005320383}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:24:20,591][0m Trial 4 finished with value: 0.14214081417523688 and parameters: {'observation_period_num': 104, 'train_rates': 0.7563673070695924, 'learning_rate': 0.000456488734692385, 'batch_size': 136, 'step_size': 13, 'gamma': 0.945075071437684}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:25:34,884][0m Trial 5 finished with value: 0.08261800495050706 and parameters: {'observation_period_num': 5, 'train_rates': 0.8223891676011575, 'learning_rate': 8.370419441303605e-06, 'batch_size': 72, 'step_size': 14, 'gamma': 0.8169398627502319}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:26:28,787][0m Trial 6 finished with value: 0.2544261109434618 and parameters: {'observation_period_num': 216, 'train_rates': 0.7142554184833849, 'learning_rate': 0.0005140576179003356, 'batch_size': 88, 'step_size': 15, 'gamma': 0.7725177789391033}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:28:10,162][0m Trial 7 finished with value: 0.29719035379821435 and parameters: {'observation_period_num': 250, 'train_rates': 0.6233192674751769, 'learning_rate': 5.4682144615476044e-05, 'batch_size': 40, 'step_size': 13, 'gamma': 0.8930836688913145}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:28:29,017][0m Trial 8 finished with value: 0.9880050027359645 and parameters: {'observation_period_num': 204, 'train_rates': 0.6010105095840003, 'learning_rate': 1.5468804740473122e-06, 'batch_size': 251, 'step_size': 8, 'gamma': 0.7758580450414063}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:28:54,242][0m Trial 9 finished with value: 0.3286770629822908 and parameters: {'observation_period_num': 186, 'train_rates': 0.6424044181952331, 'learning_rate': 0.00029265906300202413, 'batch_size': 188, 'step_size': 9, 'gamma': 0.8500186326084447}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:29:21,524][0m Trial 10 finished with value: 0.09510006755590439 and parameters: {'observation_period_num': 5, 'train_rates': 0.9453448826533621, 'learning_rate': 0.00011647164040941365, 'batch_size': 254, 'step_size': 1, 'gamma': 0.9106528270917505}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:30:17,290][0m Trial 11 finished with value: 0.07714539010876732 and parameters: {'observation_period_num': 9, 'train_rates': 0.8789038477429412, 'learning_rate': 1.1852559087945928e-05, 'batch_size': 102, 'step_size': 11, 'gamma': 0.8332445795558212}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:31:05,965][0m Trial 12 finished with value: 0.14597850761869374 and parameters: {'observation_period_num': 53, 'train_rates': 0.8834746783540293, 'learning_rate': 1.423356172423591e-05, 'batch_size': 117, 'step_size': 11, 'gamma': 0.8391124391945568}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:31:35,059][0m Trial 13 finished with value: 0.6091771408131248 and parameters: {'observation_period_num': 49, 'train_rates': 0.8699069127814312, 'learning_rate': 2.6620168359424612e-06, 'batch_size': 200, 'step_size': 5, 'gamma': 0.9753766829922654}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:32:05,584][0m Trial 14 finished with value: 0.07207736372947693 and parameters: {'observation_period_num': 40, 'train_rates': 0.9802693790950371, 'learning_rate': 0.00012670155324824106, 'batch_size': 207, 'step_size': 11, 'gamma': 0.814201558071121}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:32:34,593][0m Trial 15 finished with value: 0.07385450601577759 and parameters: {'observation_period_num': 82, 'train_rates': 0.9714015135370789, 'learning_rate': 0.0009628490563396227, 'batch_size': 220, 'step_size': 11, 'gamma': 0.8103187128465457}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:33:03,453][0m Trial 16 finished with value: 0.06683506071567535 and parameters: {'observation_period_num': 38, 'train_rates': 0.9342248307091421, 'learning_rate': 0.00014508414363500208, 'batch_size': 219, 'step_size': 6, 'gamma': 0.8822059237595569}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:33:41,626][0m Trial 17 finished with value: 0.056382079089640595 and parameters: {'observation_period_num': 35, 'train_rates': 0.9169131054646293, 'learning_rate': 0.0001881250108378407, 'batch_size': 163, 'step_size': 5, 'gamma': 0.9377082385683835}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:34:15,763][0m Trial 18 finished with value: 0.08365213192000831 and parameters: {'observation_period_num': 100, 'train_rates': 0.8309426940606637, 'learning_rate': 0.0007863330307673408, 'batch_size': 163, 'step_size': 3, 'gamma': 0.9352832887856463}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:34:59,387][0m Trial 19 finished with value: 0.04133736228277861 and parameters: {'observation_period_num': 31, 'train_rates': 0.9175589900596968, 'learning_rate': 0.0002191056045324289, 'batch_size': 136, 'step_size': 6, 'gamma': 0.9467905915316208}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:35:39,390][0m Trial 20 finished with value: 0.23626644664735935 and parameters: {'observation_period_num': 155, 'train_rates': 0.833258015705792, 'learning_rate': 5.5861312029105624e-05, 'batch_size': 133, 'step_size': 6, 'gamma': 0.9864134393343416}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:36:16,948][0m Trial 21 finished with value: 0.045076737173578955 and parameters: {'observation_period_num': 23, 'train_rates': 0.9066905470864798, 'learning_rate': 0.00023271449380889104, 'batch_size': 162, 'step_size': 4, 'gamma': 0.9482849847106402}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:37:00,763][0m Trial 22 finished with value: 0.04068944309320715 and parameters: {'observation_period_num': 23, 'train_rates': 0.9085887755111269, 'learning_rate': 0.0002830074097306211, 'batch_size': 134, 'step_size': 3, 'gamma': 0.9603275757822555}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:37:48,700][0m Trial 23 finished with value: 0.050220107906554116 and parameters: {'observation_period_num': 66, 'train_rates': 0.8569653810383762, 'learning_rate': 0.00036523705592155593, 'batch_size': 120, 'step_size': 2, 'gamma': 0.9640791498619846}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:38:27,401][0m Trial 24 finished with value: 0.07147593611389175 and parameters: {'observation_period_num': 26, 'train_rates': 0.7740002570269143, 'learning_rate': 9.087778939263926e-05, 'batch_size': 139, 'step_size': 3, 'gamma': 0.9238724281047431}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:39:23,274][0m Trial 25 finished with value: 0.09908714843751662 and parameters: {'observation_period_num': 86, 'train_rates': 0.9005782642664196, 'learning_rate': 0.0008425051619634101, 'batch_size': 100, 'step_size': 7, 'gamma': 0.959313170904074}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:43:40,639][0m Trial 26 finished with value: 0.11909987419349713 and parameters: {'observation_period_num': 59, 'train_rates': 0.937799943431249, 'learning_rate': 2.240617010834443e-05, 'batch_size': 22, 'step_size': 1, 'gamma': 0.9139397613976116}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:44:12,330][0m Trial 27 finished with value: 0.051347469224741586 and parameters: {'observation_period_num': 21, 'train_rates': 0.8453683638015133, 'learning_rate': 0.00025516514510474974, 'batch_size': 180, 'step_size': 4, 'gamma': 0.9643158054802254}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:46:02,799][0m Trial 28 finished with value: 0.09363099684317906 and parameters: {'observation_period_num': 121, 'train_rates': 0.9569516775039149, 'learning_rate': 8.408250489014473e-05, 'batch_size': 52, 'step_size': 6, 'gamma': 0.98966282229923}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:46:31,613][0m Trial 29 finished with value: 0.208788809247483 and parameters: {'observation_period_num': 159, 'train_rates': 0.7158344958718912, 'learning_rate': 2.5774178582031694e-05, 'batch_size': 176, 'step_size': 9, 'gamma': 0.9731463595093338}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:46:58,301][0m Trial 30 finished with value: 0.04856742858152154 and parameters: {'observation_period_num': 23, 'train_rates': 0.8072712154144258, 'learning_rate': 0.0005938894454034056, 'batch_size': 236, 'step_size': 7, 'gamma': 0.890863473942555}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:47:38,836][0m Trial 31 finished with value: 0.04038549937516243 and parameters: {'observation_period_num': 22, 'train_rates': 0.9031562866087204, 'learning_rate': 0.00021839442592078671, 'batch_size': 151, 'step_size': 4, 'gamma': 0.9499370427953664}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:48:22,226][0m Trial 32 finished with value: 0.04635720519769576 and parameters: {'observation_period_num': 46, 'train_rates': 0.9150807410531725, 'learning_rate': 0.00020430032669686632, 'batch_size': 145, 'step_size': 3, 'gamma': 0.9543047685989592}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:49:11,458][0m Trial 33 finished with value: 0.05458791714485052 and parameters: {'observation_period_num': 76, 'train_rates': 0.8925479906912618, 'learning_rate': 0.0003813717398171969, 'batch_size': 119, 'step_size': 4, 'gamma': 0.9295301556752815}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:49:48,960][0m Trial 34 finished with value: 0.04286539352546304 and parameters: {'observation_period_num': 20, 'train_rates': 0.8599120242833131, 'learning_rate': 0.0006419975405228565, 'batch_size': 154, 'step_size': 2, 'gamma': 0.9161657259079463}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:50:18,967][0m Trial 35 finished with value: 0.1014073005089393 and parameters: {'observation_period_num': 63, 'train_rates': 0.7987814159455573, 'learning_rate': 7.586754011649963e-05, 'batch_size': 192, 'step_size': 5, 'gamma': 0.9758741247860535}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:51:09,065][0m Trial 36 finished with value: 0.038598878258099295 and parameters: {'observation_period_num': 5, 'train_rates': 0.9508154888490948, 'learning_rate': 0.0003453349230641562, 'batch_size': 127, 'step_size': 7, 'gamma': 0.9429835970861781}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:52:06,353][0m Trial 37 finished with value: 0.040674696266651156 and parameters: {'observation_period_num': 8, 'train_rates': 0.9578264643089693, 'learning_rate': 0.000384105114851934, 'batch_size': 106, 'step_size': 10, 'gamma': 0.8992999162053774}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:53:29,962][0m Trial 38 finished with value: 0.040574174660902754 and parameters: {'observation_period_num': 12, 'train_rates': 0.9649471287034296, 'learning_rate': 0.0005579253479231688, 'batch_size': 72, 'step_size': 13, 'gamma': 0.8999428326374138}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:54:54,537][0m Trial 39 finished with value: 0.14948354068459296 and parameters: {'observation_period_num': 95, 'train_rates': 0.931776671425641, 'learning_rate': 0.0005940912137172876, 'batch_size': 68, 'step_size': 13, 'gamma': 0.8831456309289247}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:56:04,066][0m Trial 40 finished with value: 0.11982103437185287 and parameters: {'observation_period_num': 118, 'train_rates': 0.9899709878888865, 'learning_rate': 0.0005158871242528187, 'batch_size': 86, 'step_size': 15, 'gamma': 0.8585740709910141}. Best is trial 1 with value: 0.03779123547045808.[0m
[32m[I 2025-02-04 11:57:03,550][0m Trial 41 finished with value: 0.035423537760618184 and parameters: {'observation_period_num': 10, 'train_rates': 0.9575520938771839, 'learning_rate': 0.00038654411146070563, 'batch_size': 103, 'step_size': 12, 'gamma': 0.899791418066742}. Best is trial 41 with value: 0.035423537760618184.[0m
[32m[I 2025-02-04 11:58:14,092][0m Trial 42 finished with value: 0.03642177471035236 and parameters: {'observation_period_num': 5, 'train_rates': 0.9626012389315041, 'learning_rate': 0.0001608959179906955, 'batch_size': 87, 'step_size': 12, 'gamma': 0.868108169548403}. Best is trial 41 with value: 0.035423537760618184.[0m
[32m[I 2025-02-04 11:59:23,462][0m Trial 43 finished with value: 0.03482349615078419 and parameters: {'observation_period_num': 6, 'train_rates': 0.9460826217080192, 'learning_rate': 0.00017003328186902613, 'batch_size': 87, 'step_size': 14, 'gamma': 0.8668380923938517}. Best is trial 43 with value: 0.03482349615078419.[0m
[32m[I 2025-02-04 12:00:33,185][0m Trial 44 finished with value: 0.05156420563526501 and parameters: {'observation_period_num': 6, 'train_rates': 0.949244699241746, 'learning_rate': 3.8132154628005035e-05, 'batch_size': 87, 'step_size': 12, 'gamma': 0.8696426287963812}. Best is trial 43 with value: 0.03482349615078419.[0m
[32m[I 2025-02-04 12:02:45,276][0m Trial 45 finished with value: 0.05609016865491867 and parameters: {'observation_period_num': 45, 'train_rates': 0.9848228911385365, 'learning_rate': 0.00016512330454520628, 'batch_size': 45, 'step_size': 14, 'gamma': 0.8648349802675173}. Best is trial 43 with value: 0.03482349615078419.[0m
[32m[I 2025-02-04 12:04:11,721][0m Trial 46 finished with value: 0.05187583226160925 and parameters: {'observation_period_num': 14, 'train_rates': 0.7109505182166665, 'learning_rate': 0.0003446296194691335, 'batch_size': 56, 'step_size': 14, 'gamma': 0.8497141168247997}. Best is trial 43 with value: 0.03482349615078419.[0m
[32m[I 2025-02-04 12:05:15,564][0m Trial 47 finished with value: 0.045857664197683334 and parameters: {'observation_period_num': 33, 'train_rates': 0.9701410322062132, 'learning_rate': 0.0001399472452562667, 'batch_size': 97, 'step_size': 12, 'gamma': 0.8313743290825706}. Best is trial 43 with value: 0.03482349615078419.[0m
[32m[I 2025-02-04 12:06:09,211][0m Trial 48 finished with value: 0.043908699236195024 and parameters: {'observation_period_num': 5, 'train_rates': 0.926695823460959, 'learning_rate': 0.00010030284720238784, 'batch_size': 113, 'step_size': 12, 'gamma': 0.7954614168505965}. Best is trial 43 with value: 0.03482349615078419.[0m
[32m[I 2025-02-04 12:07:08,994][0m Trial 49 finished with value: 0.5136862674385266 and parameters: {'observation_period_num': 237, 'train_rates': 0.7431498398427742, 'learning_rate': 5.553656032091883e-06, 'batch_size': 78, 'step_size': 8, 'gamma': 0.8751075965082263}. Best is trial 43 with value: 0.03482349615078419.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3763 | 0.1754
Epoch 2/300, Loss: 0.1316 | 0.1291
Epoch 3/300, Loss: 0.1190 | 0.1635
Epoch 4/300, Loss: 0.1340 | 0.3094
Epoch 5/300, Loss: 0.1283 | 0.2677
Epoch 6/300, Loss: 0.1122 | 0.0978
Epoch 7/300, Loss: 0.0826 | 0.0967
Epoch 8/300, Loss: 0.0769 | 0.0807
Epoch 9/300, Loss: 0.0732 | 0.0731
Epoch 10/300, Loss: 0.0715 | 0.0700
Epoch 11/300, Loss: 0.0720 | 0.0690
Epoch 12/300, Loss: 0.0747 | 0.0742
Epoch 13/300, Loss: 0.0785 | 0.0944
Epoch 14/300, Loss: 0.0777 | 0.0882
Epoch 15/300, Loss: 0.0719 | 0.0802
Epoch 16/300, Loss: 0.0692 | 0.0749
Epoch 17/300, Loss: 0.0681 | 0.0758
Epoch 18/300, Loss: 0.0695 | 0.0782
Epoch 19/300, Loss: 0.0723 | 0.0797
Epoch 20/300, Loss: 0.0750 | 0.0792
Epoch 21/300, Loss: 0.0760 | 0.0781
Epoch 22/300, Loss: 0.0768 | 0.0831
Epoch 23/300, Loss: 0.0768 | 0.0811
Epoch 24/300, Loss: 0.0760 | 0.0799
Epoch 25/300, Loss: 0.0753 | 0.0736
Epoch 26/300, Loss: 0.0728 | 0.0656
Epoch 27/300, Loss: 0.0650 | 0.0697
Epoch 28/300, Loss: 0.0584 | 0.0766
Epoch 29/300, Loss: 0.0544 | 0.0674
Epoch 30/300, Loss: 0.0512 | 0.0560
Epoch 31/300, Loss: 0.0486 | 0.0546
Epoch 32/300, Loss: 0.0480 | 0.0536
Epoch 33/300, Loss: 0.0475 | 0.0522
Epoch 34/300, Loss: 0.0469 | 0.0521
Epoch 35/300, Loss: 0.0466 | 0.0522
Epoch 36/300, Loss: 0.0464 | 0.0519
Epoch 37/300, Loss: 0.0464 | 0.0508
Epoch 38/300, Loss: 0.0460 | 0.0501
Epoch 39/300, Loss: 0.0458 | 0.0497
Epoch 40/300, Loss: 0.0456 | 0.0495
Epoch 41/300, Loss: 0.0454 | 0.0494
Epoch 42/300, Loss: 0.0451 | 0.0492
Epoch 43/300, Loss: 0.0449 | 0.0492
Epoch 44/300, Loss: 0.0444 | 0.0485
Epoch 45/300, Loss: 0.0434 | 0.0478
Epoch 46/300, Loss: 0.0425 | 0.0473
Epoch 47/300, Loss: 0.0419 | 0.0468
Epoch 48/300, Loss: 0.0414 | 0.0464
Epoch 49/300, Loss: 0.0410 | 0.0461
Epoch 50/300, Loss: 0.0407 | 0.0459
Epoch 51/300, Loss: 0.0405 | 0.0457
Epoch 52/300, Loss: 0.0401 | 0.0455
Epoch 53/300, Loss: 0.0398 | 0.0453
Epoch 54/300, Loss: 0.0396 | 0.0451
Epoch 55/300, Loss: 0.0393 | 0.0449
Epoch 56/300, Loss: 0.0391 | 0.0446
Epoch 57/300, Loss: 0.0389 | 0.0447
Epoch 58/300, Loss: 0.0388 | 0.0445
Epoch 59/300, Loss: 0.0386 | 0.0444
Epoch 60/300, Loss: 0.0384 | 0.0442
Epoch 61/300, Loss: 0.0381 | 0.0441
Epoch 62/300, Loss: 0.0379 | 0.0438
Epoch 63/300, Loss: 0.0377 | 0.0436
Epoch 64/300, Loss: 0.0376 | 0.0438
Epoch 65/300, Loss: 0.0376 | 0.0436
Epoch 66/300, Loss: 0.0374 | 0.0435
Epoch 67/300, Loss: 0.0373 | 0.0433
Epoch 68/300, Loss: 0.0372 | 0.0431
Epoch 69/300, Loss: 0.0371 | 0.0429
Epoch 70/300, Loss: 0.0369 | 0.0427
Epoch 71/300, Loss: 0.0370 | 0.0425
Epoch 72/300, Loss: 0.0373 | 0.0423
Epoch 73/300, Loss: 0.0374 | 0.0421
Epoch 74/300, Loss: 0.0377 | 0.0421
Epoch 75/300, Loss: 0.0380 | 0.0423
Epoch 76/300, Loss: 0.0384 | 0.0424
Epoch 77/300, Loss: 0.0384 | 0.0424
Epoch 78/300, Loss: 0.0380 | 0.0424
Epoch 79/300, Loss: 0.0368 | 0.0416
Epoch 80/300, Loss: 0.0356 | 0.0414
Epoch 81/300, Loss: 0.0350 | 0.0417
Epoch 82/300, Loss: 0.0347 | 0.0415
Epoch 83/300, Loss: 0.0345 | 0.0410
Epoch 84/300, Loss: 0.0344 | 0.0407
Epoch 85/300, Loss: 0.0342 | 0.0403
Epoch 86/300, Loss: 0.0341 | 0.0401
Epoch 87/300, Loss: 0.0339 | 0.0399
Epoch 88/300, Loss: 0.0337 | 0.0398
Epoch 89/300, Loss: 0.0336 | 0.0397
Epoch 90/300, Loss: 0.0335 | 0.0396
Epoch 91/300, Loss: 0.0333 | 0.0394
Epoch 92/300, Loss: 0.0332 | 0.0392
Epoch 93/300, Loss: 0.0332 | 0.0391
Epoch 94/300, Loss: 0.0330 | 0.0390
Epoch 95/300, Loss: 0.0329 | 0.0389
Epoch 96/300, Loss: 0.0328 | 0.0388
Epoch 97/300, Loss: 0.0327 | 0.0387
Epoch 98/300, Loss: 0.0327 | 0.0386
Epoch 99/300, Loss: 0.0326 | 0.0384
Epoch 100/300, Loss: 0.0325 | 0.0383
Epoch 101/300, Loss: 0.0324 | 0.0383
Epoch 102/300, Loss: 0.0323 | 0.0382
Epoch 103/300, Loss: 0.0323 | 0.0381
Epoch 104/300, Loss: 0.0322 | 0.0380
Epoch 105/300, Loss: 0.0321 | 0.0379
Epoch 106/300, Loss: 0.0321 | 0.0378
Epoch 107/300, Loss: 0.0320 | 0.0377
Epoch 108/300, Loss: 0.0320 | 0.0376
Epoch 109/300, Loss: 0.0319 | 0.0375
Epoch 110/300, Loss: 0.0319 | 0.0374
Epoch 111/300, Loss: 0.0318 | 0.0374
Epoch 112/300, Loss: 0.0318 | 0.0373
Epoch 113/300, Loss: 0.0317 | 0.0372
Epoch 114/300, Loss: 0.0317 | 0.0371
Epoch 115/300, Loss: 0.0316 | 0.0371
Epoch 116/300, Loss: 0.0316 | 0.0370
Epoch 117/300, Loss: 0.0315 | 0.0369
Epoch 118/300, Loss: 0.0315 | 0.0368
Epoch 119/300, Loss: 0.0315 | 0.0367
Epoch 120/300, Loss: 0.0315 | 0.0366
Epoch 121/300, Loss: 0.0315 | 0.0366
Epoch 122/300, Loss: 0.0314 | 0.0365
Epoch 123/300, Loss: 0.0314 | 0.0364
Epoch 124/300, Loss: 0.0315 | 0.0363
Epoch 125/300, Loss: 0.0315 | 0.0363
Epoch 126/300, Loss: 0.0315 | 0.0362
Epoch 127/300, Loss: 0.0316 | 0.0362
Epoch 128/300, Loss: 0.0319 | 0.0363
Epoch 129/300, Loss: 0.0320 | 0.0364
Epoch 130/300, Loss: 0.0319 | 0.0363
Epoch 131/300, Loss: 0.0317 | 0.0362
Epoch 132/300, Loss: 0.0315 | 0.0361
Epoch 133/300, Loss: 0.0313 | 0.0360
Epoch 134/300, Loss: 0.0313 | 0.0359
Epoch 135/300, Loss: 0.0311 | 0.0358
Epoch 136/300, Loss: 0.0309 | 0.0357
Epoch 137/300, Loss: 0.0308 | 0.0357
Epoch 138/300, Loss: 0.0307 | 0.0357
Epoch 139/300, Loss: 0.0306 | 0.0356
Epoch 140/300, Loss: 0.0306 | 0.0356
Epoch 141/300, Loss: 0.0306 | 0.0356
Epoch 142/300, Loss: 0.0305 | 0.0355
Epoch 143/300, Loss: 0.0305 | 0.0355
Epoch 144/300, Loss: 0.0304 | 0.0355
Epoch 145/300, Loss: 0.0304 | 0.0355
Epoch 146/300, Loss: 0.0304 | 0.0355
Epoch 147/300, Loss: 0.0304 | 0.0355
Epoch 148/300, Loss: 0.0304 | 0.0355
Epoch 149/300, Loss: 0.0304 | 0.0355
Epoch 150/300, Loss: 0.0304 | 0.0355
Epoch 151/300, Loss: 0.0305 | 0.0355
Epoch 152/300, Loss: 0.0305 | 0.0355
Epoch 153/300, Loss: 0.0305 | 0.0356
Epoch 154/300, Loss: 0.0306 | 0.0356
Epoch 155/300, Loss: 0.0308 | 0.0358
Epoch 156/300, Loss: 0.0310 | 0.0359
Epoch 157/300, Loss: 0.0310 | 0.0360
Epoch 158/300, Loss: 0.0310 | 0.0360
Epoch 159/300, Loss: 0.0310 | 0.0359
Epoch 160/300, Loss: 0.0310 | 0.0359
Epoch 161/300, Loss: 0.0308 | 0.0358
Epoch 162/300, Loss: 0.0308 | 0.0355
Epoch 163/300, Loss: 0.0307 | 0.0353
Epoch 164/300, Loss: 0.0304 | 0.0353
Epoch 165/300, Loss: 0.0303 | 0.0351
Epoch 166/300, Loss: 0.0302 | 0.0350
Epoch 167/300, Loss: 0.0302 | 0.0349
Epoch 168/300, Loss: 0.0302 | 0.0349
Epoch 169/300, Loss: 0.0301 | 0.0348
Epoch 170/300, Loss: 0.0302 | 0.0348
Epoch 171/300, Loss: 0.0303 | 0.0347
Epoch 172/300, Loss: 0.0303 | 0.0347
Epoch 173/300, Loss: 0.0303 | 0.0347
Epoch 174/300, Loss: 0.0303 | 0.0347
Epoch 175/300, Loss: 0.0303 | 0.0347
Epoch 176/300, Loss: 0.0302 | 0.0346
Epoch 177/300, Loss: 0.0301 | 0.0346
Epoch 178/300, Loss: 0.0300 | 0.0346
Epoch 179/300, Loss: 0.0299 | 0.0346
Epoch 180/300, Loss: 0.0298 | 0.0346
Epoch 181/300, Loss: 0.0298 | 0.0345
Epoch 182/300, Loss: 0.0297 | 0.0345
Epoch 183/300, Loss: 0.0297 | 0.0345
Epoch 184/300, Loss: 0.0297 | 0.0345
Epoch 185/300, Loss: 0.0296 | 0.0345
Epoch 186/300, Loss: 0.0296 | 0.0344
Epoch 187/300, Loss: 0.0295 | 0.0344
Epoch 188/300, Loss: 0.0295 | 0.0344
Epoch 189/300, Loss: 0.0295 | 0.0344
Epoch 190/300, Loss: 0.0295 | 0.0344
Epoch 191/300, Loss: 0.0295 | 0.0343
Epoch 192/300, Loss: 0.0294 | 0.0343
Epoch 193/300, Loss: 0.0294 | 0.0343
Epoch 194/300, Loss: 0.0294 | 0.0343
Epoch 195/300, Loss: 0.0294 | 0.0343
Epoch 196/300, Loss: 0.0294 | 0.0343
Epoch 197/300, Loss: 0.0293 | 0.0342
Epoch 198/300, Loss: 0.0293 | 0.0342
Epoch 199/300, Loss: 0.0293 | 0.0342
Epoch 200/300, Loss: 0.0293 | 0.0342
Epoch 201/300, Loss: 0.0293 | 0.0342
Epoch 202/300, Loss: 0.0293 | 0.0342
Epoch 203/300, Loss: 0.0293 | 0.0342
Epoch 204/300, Loss: 0.0292 | 0.0342
Epoch 205/300, Loss: 0.0292 | 0.0341
Epoch 206/300, Loss: 0.0292 | 0.0341
Epoch 207/300, Loss: 0.0292 | 0.0341
Epoch 208/300, Loss: 0.0292 | 0.0341
Epoch 209/300, Loss: 0.0292 | 0.0341
Epoch 210/300, Loss: 0.0292 | 0.0341
Epoch 211/300, Loss: 0.0292 | 0.0341
Epoch 212/300, Loss: 0.0291 | 0.0341
Epoch 213/300, Loss: 0.0291 | 0.0341
Epoch 214/300, Loss: 0.0291 | 0.0340
Epoch 215/300, Loss: 0.0291 | 0.0340
Epoch 216/300, Loss: 0.0291 | 0.0340
Epoch 217/300, Loss: 0.0291 | 0.0340
Epoch 218/300, Loss: 0.0291 | 0.0340
Epoch 219/300, Loss: 0.0291 | 0.0340
Epoch 220/300, Loss: 0.0291 | 0.0340
Epoch 221/300, Loss: 0.0290 | 0.0340
Epoch 222/300, Loss: 0.0290 | 0.0340
Epoch 223/300, Loss: 0.0290 | 0.0340
Epoch 224/300, Loss: 0.0290 | 0.0340
Epoch 225/300, Loss: 0.0290 | 0.0340
Epoch 226/300, Loss: 0.0290 | 0.0339
Epoch 227/300, Loss: 0.0290 | 0.0339
Epoch 228/300, Loss: 0.0290 | 0.0339
Epoch 229/300, Loss: 0.0290 | 0.0339
Epoch 230/300, Loss: 0.0290 | 0.0339
Epoch 231/300, Loss: 0.0290 | 0.0339
Epoch 232/300, Loss: 0.0290 | 0.0339
Epoch 233/300, Loss: 0.0290 | 0.0339
Epoch 234/300, Loss: 0.0289 | 0.0339
Epoch 235/300, Loss: 0.0289 | 0.0339
Epoch 236/300, Loss: 0.0289 | 0.0339
Epoch 237/300, Loss: 0.0289 | 0.0339
Epoch 238/300, Loss: 0.0289 | 0.0339
Epoch 239/300, Loss: 0.0289 | 0.0339
Epoch 240/300, Loss: 0.0289 | 0.0338
Epoch 241/300, Loss: 0.0289 | 0.0338
Epoch 242/300, Loss: 0.0289 | 0.0338
Epoch 243/300, Loss: 0.0289 | 0.0338
Epoch 244/300, Loss: 0.0289 | 0.0338
Epoch 245/300, Loss: 0.0289 | 0.0338
Epoch 246/300, Loss: 0.0289 | 0.0338
Epoch 247/300, Loss: 0.0289 | 0.0338
Epoch 248/300, Loss: 0.0289 | 0.0338
Epoch 249/300, Loss: 0.0289 | 0.0338
Epoch 250/300, Loss: 0.0288 | 0.0338
Epoch 251/300, Loss: 0.0288 | 0.0338
Epoch 252/300, Loss: 0.0288 | 0.0338
Epoch 253/300, Loss: 0.0288 | 0.0338
Epoch 254/300, Loss: 0.0288 | 0.0338
Epoch 255/300, Loss: 0.0288 | 0.0338
Epoch 256/300, Loss: 0.0288 | 0.0338
Epoch 257/300, Loss: 0.0288 | 0.0338
Epoch 258/300, Loss: 0.0288 | 0.0337
Epoch 259/300, Loss: 0.0288 | 0.0337
Epoch 260/300, Loss: 0.0288 | 0.0337
Epoch 261/300, Loss: 0.0288 | 0.0337
Epoch 262/300, Loss: 0.0288 | 0.0337
Epoch 263/300, Loss: 0.0288 | 0.0337
Epoch 264/300, Loss: 0.0288 | 0.0337
Epoch 265/300, Loss: 0.0288 | 0.0337
Epoch 266/300, Loss: 0.0288 | 0.0337
Epoch 267/300, Loss: 0.0288 | 0.0337
Epoch 268/300, Loss: 0.0288 | 0.0337
Epoch 269/300, Loss: 0.0288 | 0.0337
Epoch 270/300, Loss: 0.0288 | 0.0337
Epoch 271/300, Loss: 0.0288 | 0.0337
Epoch 272/300, Loss: 0.0288 | 0.0337
Epoch 273/300, Loss: 0.0288 | 0.0337
Epoch 274/300, Loss: 0.0287 | 0.0337
Epoch 275/300, Loss: 0.0287 | 0.0337
Epoch 276/300, Loss: 0.0287 | 0.0337
Epoch 277/300, Loss: 0.0287 | 0.0337
Epoch 278/300, Loss: 0.0287 | 0.0337
Epoch 279/300, Loss: 0.0287 | 0.0337
Epoch 280/300, Loss: 0.0287 | 0.0337
Epoch 281/300, Loss: 0.0287 | 0.0337
Epoch 282/300, Loss: 0.0287 | 0.0337
Epoch 283/300, Loss: 0.0287 | 0.0337
Epoch 284/300, Loss: 0.0287 | 0.0337
Epoch 285/300, Loss: 0.0287 | 0.0337
Epoch 286/300, Loss: 0.0287 | 0.0337
Epoch 287/300, Loss: 0.0287 | 0.0337
Epoch 288/300, Loss: 0.0287 | 0.0336
Epoch 289/300, Loss: 0.0287 | 0.0336
Epoch 290/300, Loss: 0.0287 | 0.0336
Epoch 291/300, Loss: 0.0287 | 0.0336
Epoch 292/300, Loss: 0.0287 | 0.0336
Epoch 293/300, Loss: 0.0287 | 0.0336
Epoch 294/300, Loss: 0.0287 | 0.0336
Epoch 295/300, Loss: 0.0287 | 0.0336
Epoch 296/300, Loss: 0.0287 | 0.0336
Epoch 297/300, Loss: 0.0287 | 0.0336
Epoch 298/300, Loss: 0.0287 | 0.0336
Epoch 299/300, Loss: 0.0287 | 0.0336
Epoch 300/300, Loss: 0.0287 | 0.0336
Runtime (seconds): 208.6319179534912
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 12.403968947241083
RMSE: 3.5219268798828125
MAE: 3.5219268798828125
R-squared: nan
[111.53193]
