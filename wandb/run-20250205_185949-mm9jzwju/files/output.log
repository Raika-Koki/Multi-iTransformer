ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-05 18:59:54,138][0m A new study created in memory with name: no-name-9b00e175-8d63-46c9-bfa4-6942c0966490[0m
[32m[I 2025-02-05 19:00:26,408][0m Trial 0 finished with value: 0.39128317828450737 and parameters: {'observation_period_num': 238, 'train_rates': 0.6582158561209678, 'learning_rate': 0.0005181992756625456, 'batch_size': 139, 'step_size': 3, 'gamma': 0.9746021291688896}. Best is trial 0 with value: 0.39128317828450737.[0m
[32m[I 2025-02-05 19:03:56,224][0m Trial 1 finished with value: 0.15896566389194616 and parameters: {'observation_period_num': 92, 'train_rates': 0.9015521000989429, 'learning_rate': 0.00032335475549526263, 'batch_size': 26, 'step_size': 14, 'gamma': 0.9482473463428782}. Best is trial 1 with value: 0.15896566389194616.[0m
[32m[I 2025-02-05 19:04:26,783][0m Trial 2 finished with value: 0.2049352079629898 and parameters: {'observation_period_num': 7, 'train_rates': 0.9467700578106477, 'learning_rate': 3.145692672365047e-05, 'batch_size': 210, 'step_size': 4, 'gamma': 0.7558032746586109}. Best is trial 1 with value: 0.15896566389194616.[0m
[32m[I 2025-02-05 19:04:48,316][0m Trial 3 finished with value: 0.34513341455433005 and parameters: {'observation_period_num': 222, 'train_rates': 0.7046003546047346, 'learning_rate': 7.160332021328415e-05, 'batch_size': 251, 'step_size': 15, 'gamma': 0.8922748794780636}. Best is trial 1 with value: 0.15896566389194616.[0m
[32m[I 2025-02-05 19:05:10,356][0m Trial 4 finished with value: 0.3378139378193802 and parameters: {'observation_period_num': 235, 'train_rates': 0.731812949386045, 'learning_rate': 0.00028950392286202214, 'batch_size': 236, 'step_size': 11, 'gamma': 0.851948059443278}. Best is trial 1 with value: 0.15896566389194616.[0m
[32m[I 2025-02-05 19:05:34,908][0m Trial 5 finished with value: 0.35569697727075955 and parameters: {'observation_period_num': 237, 'train_rates': 0.7274244832691297, 'learning_rate': 7.959428874229175e-05, 'batch_size': 217, 'step_size': 12, 'gamma': 0.7555473932841952}. Best is trial 1 with value: 0.15896566389194616.[0m
[32m[I 2025-02-05 19:08:11,408][0m Trial 6 finished with value: 0.4122468427369732 and parameters: {'observation_period_num': 116, 'train_rates': 0.8656714910785193, 'learning_rate': 1.7902558657024416e-06, 'batch_size': 34, 'step_size': 12, 'gamma': 0.817092751079148}. Best is trial 1 with value: 0.15896566389194616.[0m
[32m[I 2025-02-05 19:08:40,777][0m Trial 7 finished with value: 0.6936598039590396 and parameters: {'observation_period_num': 69, 'train_rates': 0.9099870975301065, 'learning_rate': 7.141711349062346e-06, 'batch_size': 209, 'step_size': 2, 'gamma': 0.9170757361031177}. Best is trial 1 with value: 0.15896566389194616.[0m
[32m[I 2025-02-05 19:09:23,079][0m Trial 8 finished with value: 0.2866232899636248 and parameters: {'observation_period_num': 175, 'train_rates': 0.7536388800040601, 'learning_rate': 0.00010436008313987741, 'batch_size': 123, 'step_size': 14, 'gamma': 0.7683713659389423}. Best is trial 1 with value: 0.15896566389194616.[0m
[32m[I 2025-02-05 19:09:58,131][0m Trial 9 finished with value: 1.47257399559021 and parameters: {'observation_period_num': 101, 'train_rates': 0.9702342952255947, 'learning_rate': 1.020432324301358e-06, 'batch_size': 179, 'step_size': 5, 'gamma': 0.8926189790019743}. Best is trial 1 with value: 0.15896566389194616.[0m
[32m[I 2025-02-05 19:12:31,565][0m Trial 10 finished with value: 0.05347963547581559 and parameters: {'observation_period_num': 41, 'train_rates': 0.8527457142693102, 'learning_rate': 0.0009670166087953465, 'batch_size': 35, 'step_size': 8, 'gamma': 0.9874196043861536}. Best is trial 10 with value: 0.05347963547581559.[0m
[32m[I 2025-02-05 19:17:37,993][0m Trial 11 finished with value: 0.04747827329805919 and parameters: {'observation_period_num': 37, 'train_rates': 0.8322061901256025, 'learning_rate': 0.0008242254737266063, 'batch_size': 17, 'step_size': 8, 'gamma': 0.9855637576580226}. Best is trial 11 with value: 0.04747827329805919.[0m
[32m[I 2025-02-05 19:18:57,411][0m Trial 12 finished with value: 0.04237049571890568 and parameters: {'observation_period_num': 21, 'train_rates': 0.8271045602933924, 'learning_rate': 0.00086858230410451, 'batch_size': 68, 'step_size': 8, 'gamma': 0.9787693729839751}. Best is trial 12 with value: 0.04237049571890568.[0m
[32m[I 2025-02-05 19:20:13,590][0m Trial 13 finished with value: 0.03161408840201971 and parameters: {'observation_period_num': 8, 'train_rates': 0.8275989093745978, 'learning_rate': 0.0009012707639866747, 'batch_size': 76, 'step_size': 7, 'gamma': 0.9403209763219007}. Best is trial 13 with value: 0.03161408840201971.[0m
[32m[I 2025-02-05 19:21:19,915][0m Trial 14 finished with value: 0.029166157032965167 and parameters: {'observation_period_num': 8, 'train_rates': 0.8041800595872398, 'learning_rate': 0.00020254165274401564, 'batch_size': 79, 'step_size': 6, 'gamma': 0.9397781959032906}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:22:20,756][0m Trial 15 finished with value: 0.19524630625401773 and parameters: {'observation_period_num': 68, 'train_rates': 0.7851296075567414, 'learning_rate': 0.00018288770677182158, 'batch_size': 84, 'step_size': 6, 'gamma': 0.9343622190173697}. Best is trial 14 with value: 0.029166157032965167.[0m
Early stopping at epoch 84
[32m[I 2025-02-05 19:22:59,979][0m Trial 16 finished with value: 0.6567836787691248 and parameters: {'observation_period_num': 170, 'train_rates': 0.6359443536038123, 'learning_rate': 1.9495607753782358e-05, 'batch_size': 95, 'step_size': 1, 'gamma': 0.8564164789282133}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:23:43,049][0m Trial 17 finished with value: 0.1362901003285608 and parameters: {'observation_period_num': 151, 'train_rates': 0.7907675663538902, 'learning_rate': 0.00018616199768181038, 'batch_size': 123, 'step_size': 6, 'gamma': 0.9471156322779862}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:25:07,680][0m Trial 18 finished with value: 0.2335150328896592 and parameters: {'observation_period_num': 60, 'train_rates': 0.6819577369200567, 'learning_rate': 1.1978975333125754e-05, 'batch_size': 56, 'step_size': 10, 'gamma': 0.9106204545492481}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:25:38,828][0m Trial 19 finished with value: 0.12681344849826567 and parameters: {'observation_period_num': 7, 'train_rates': 0.6105947910305968, 'learning_rate': 0.0003868967561318667, 'batch_size': 160, 'step_size': 6, 'gamma': 0.8175882141664413}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:26:36,134][0m Trial 20 finished with value: 0.045467634448946796 and parameters: {'observation_period_num': 42, 'train_rates': 0.8877821215981504, 'learning_rate': 0.00014539245103693474, 'batch_size': 100, 'step_size': 4, 'gamma': 0.9565603745675271}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:27:59,203][0m Trial 21 finished with value: 0.05787909962236881 and parameters: {'observation_period_num': 23, 'train_rates': 0.8226576895256471, 'learning_rate': 0.0006416540895776387, 'batch_size': 65, 'step_size': 9, 'gamma': 0.9218311664398123}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:29:23,150][0m Trial 22 finished with value: 0.031238643274329844 and parameters: {'observation_period_num': 6, 'train_rates': 0.8121262850966007, 'learning_rate': 0.0004776302457074664, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9604169573047364}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:31:08,064][0m Trial 23 finished with value: 0.16502877970800756 and parameters: {'observation_period_num': 6, 'train_rates': 0.7770353121296106, 'learning_rate': 4.3588985553343034e-05, 'batch_size': 49, 'step_size': 7, 'gamma': 0.9621953593632163}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:32:14,693][0m Trial 24 finished with value: 0.05918995361788906 and parameters: {'observation_period_num': 60, 'train_rates': 0.8127596705168386, 'learning_rate': 0.0002630011989050947, 'batch_size': 79, 'step_size': 7, 'gamma': 0.899328117036895}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:33:04,866][0m Trial 25 finished with value: 0.25307754856698655 and parameters: {'observation_period_num': 87, 'train_rates': 0.7630706357214505, 'learning_rate': 0.0005025439275476073, 'batch_size': 104, 'step_size': 10, 'gamma': 0.9338607169264801}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:34:55,351][0m Trial 26 finished with value: 0.15157618804486714 and parameters: {'observation_period_num': 30, 'train_rates': 0.8646787590377448, 'learning_rate': 0.0004451219165611535, 'batch_size': 50, 'step_size': 5, 'gamma': 0.9347106082432586}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:35:41,118][0m Trial 27 finished with value: 0.05412320823004815 and parameters: {'observation_period_num': 50, 'train_rates': 0.8056327295628966, 'learning_rate': 0.00013433486868013835, 'batch_size': 122, 'step_size': 9, 'gamma': 0.8783949573995764}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:36:45,102][0m Trial 28 finished with value: 0.23005263540851936 and parameters: {'observation_period_num': 134, 'train_rates': 0.7474372464827196, 'learning_rate': 5.654103243217601e-05, 'batch_size': 80, 'step_size': 7, 'gamma': 0.9650232731985007}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:37:26,890][0m Trial 29 finished with value: 0.043665579594271156 and parameters: {'observation_period_num': 20, 'train_rates': 0.841465439892758, 'learning_rate': 0.0005826483164590774, 'batch_size': 143, 'step_size': 4, 'gamma': 0.9663804620648848}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:38:11,947][0m Trial 30 finished with value: 0.1767890185884067 and parameters: {'observation_period_num': 71, 'train_rates': 0.6965515278564884, 'learning_rate': 0.0002517365336090419, 'batch_size': 109, 'step_size': 3, 'gamma': 0.8726447616617553}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:39:29,828][0m Trial 31 finished with value: 0.04024331356254472 and parameters: {'observation_period_num': 21, 'train_rates': 0.8063802607220193, 'learning_rate': 0.0009946522164388995, 'batch_size': 69, 'step_size': 9, 'gamma': 0.9767065038559398}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:40:52,619][0m Trial 32 finished with value: 0.10005326098517368 and parameters: {'observation_period_num': 21, 'train_rates': 0.8771141682427318, 'learning_rate': 0.0005557673005354702, 'batch_size': 68, 'step_size': 9, 'gamma': 0.9503905628716708}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:42:01,118][0m Trial 33 finished with value: 0.031825757875692015 and parameters: {'observation_period_num': 7, 'train_rates': 0.927144348950674, 'learning_rate': 0.0009360604331893587, 'batch_size': 87, 'step_size': 5, 'gamma': 0.9723464817782606}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:43:08,203][0m Trial 34 finished with value: 0.036096266542489715 and parameters: {'observation_period_num': 11, 'train_rates': 0.9338771753241918, 'learning_rate': 0.0003821516588178906, 'batch_size': 90, 'step_size': 5, 'gamma': 0.9388445830588279}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:45:27,544][0m Trial 35 finished with value: 0.04634121908591344 and parameters: {'observation_period_num': 45, 'train_rates': 0.9776676696249604, 'learning_rate': 0.0002542564706046373, 'batch_size': 43, 'step_size': 3, 'gamma': 0.9184298858896613}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:46:09,667][0m Trial 36 finished with value: 0.19547079313885082 and parameters: {'observation_period_num': 203, 'train_rates': 0.9201834710822512, 'learning_rate': 0.0006456233124178624, 'batch_size': 136, 'step_size': 6, 'gamma': 0.9678876762248173}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:47:02,392][0m Trial 37 finished with value: 0.03024227736899868 and parameters: {'observation_period_num': 5, 'train_rates': 0.8941910044475134, 'learning_rate': 0.00038404823870481357, 'batch_size': 110, 'step_size': 7, 'gamma': 0.9005959618547115}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:47:42,260][0m Trial 38 finished with value: 0.0544673671265023 and parameters: {'observation_period_num': 33, 'train_rates': 0.8867959065895691, 'learning_rate': 0.00033638289256573067, 'batch_size': 158, 'step_size': 8, 'gamma': 0.901065562130381}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:48:34,341][0m Trial 39 finished with value: 0.07486358356857023 and parameters: {'observation_period_num': 88, 'train_rates': 0.8499585978191903, 'learning_rate': 9.377463069552193e-05, 'batch_size': 107, 'step_size': 7, 'gamma': 0.9079507939771215}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:51:33,449][0m Trial 40 finished with value: 0.22900097028545613 and parameters: {'observation_period_num': 48, 'train_rates': 0.7263738137797263, 'learning_rate': 4.887677987894048e-06, 'batch_size': 27, 'step_size': 11, 'gamma': 0.8826871501993364}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:52:48,337][0m Trial 41 finished with value: 0.02960646339453286 and parameters: {'observation_period_num': 6, 'train_rates': 0.9090219124157599, 'learning_rate': 0.000638668775927829, 'batch_size': 78, 'step_size': 5, 'gamma': 0.9473202024066575}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:54:31,126][0m Trial 42 finished with value: 0.07940459078358066 and parameters: {'observation_period_num': 30, 'train_rates': 0.9472700464543987, 'learning_rate': 0.00042717561340718357, 'batch_size': 58, 'step_size': 6, 'gamma': 0.9456608378826472}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:55:25,009][0m Trial 43 finished with value: 0.03762614530812803 and parameters: {'observation_period_num': 14, 'train_rates': 0.9044527203714147, 'learning_rate': 0.00019256344367330082, 'batch_size': 115, 'step_size': 4, 'gamma': 0.929412365293525}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:56:45,671][0m Trial 44 finished with value: 0.051367376867989846 and parameters: {'observation_period_num': 31, 'train_rates': 0.9547275399244786, 'learning_rate': 0.0007237859105650604, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9227796191003572}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 19:57:47,057][0m Trial 45 finished with value: 0.07833179887574242 and parameters: {'observation_period_num': 56, 'train_rates': 0.864715573344294, 'learning_rate': 0.0003035648324487043, 'batch_size': 93, 'step_size': 5, 'gamma': 0.9531139311066549}. Best is trial 14 with value: 0.029166157032965167.[0m
[32m[I 2025-02-05 20:00:04,630][0m Trial 46 finished with value: 0.026770296985985802 and parameters: {'observation_period_num': 12, 'train_rates': 0.8963061379821241, 'learning_rate': 0.00013662188894747836, 'batch_size': 41, 'step_size': 8, 'gamma': 0.8514930351652281}. Best is trial 46 with value: 0.026770296985985802.[0m
[32m[I 2025-02-05 20:02:23,897][0m Trial 47 finished with value: 0.22268653265620827 and parameters: {'observation_period_num': 250, 'train_rates': 0.8977197191222104, 'learning_rate': 0.000131571864075643, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8461360378173924}. Best is trial 46 with value: 0.026770296985985802.[0m
[32m[I 2025-02-05 20:07:42,427][0m Trial 48 finished with value: 0.10025052353739738 and parameters: {'observation_period_num': 115, 'train_rates': 0.9874237709500688, 'learning_rate': 2.9609278399019066e-05, 'batch_size': 18, 'step_size': 15, 'gamma': 0.8224179907519458}. Best is trial 46 with value: 0.026770296985985802.[0m
[32m[I 2025-02-05 20:09:24,285][0m Trial 49 finished with value: 0.05758773468551226 and parameters: {'observation_period_num': 18, 'train_rates': 0.9131173277482164, 'learning_rate': 0.00020436414343629803, 'batch_size': 57, 'step_size': 2, 'gamma': 0.8382603980095451}. Best is trial 46 with value: 0.026770296985985802.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-05 20:09:24,297][0m A new study created in memory with name: no-name-0e7d040a-f5ba-43f2-9ab1-a92296037602[0m
[32m[I 2025-02-05 20:09:45,602][0m Trial 0 finished with value: 0.2563398974335012 and parameters: {'observation_period_num': 55, 'train_rates': 0.6371381908170014, 'learning_rate': 5.281663595787244e-05, 'batch_size': 249, 'step_size': 2, 'gamma': 0.9395566892848723}. Best is trial 0 with value: 0.2563398974335012.[0m
[32m[I 2025-02-05 20:10:29,656][0m Trial 1 finished with value: 0.26269348968337314 and parameters: {'observation_period_num': 221, 'train_rates': 0.694073783050197, 'learning_rate': 0.0006041845109940754, 'batch_size': 109, 'step_size': 7, 'gamma': 0.9014328189417851}. Best is trial 0 with value: 0.2563398974335012.[0m
[32m[I 2025-02-05 20:11:05,539][0m Trial 2 finished with value: 0.43019109005656314 and parameters: {'observation_period_num': 210, 'train_rates': 0.6036753892615269, 'learning_rate': 2.963501889828676e-05, 'batch_size': 127, 'step_size': 13, 'gamma': 0.8460870511661549}. Best is trial 0 with value: 0.2563398974335012.[0m
[32m[I 2025-02-05 20:11:32,951][0m Trial 3 finished with value: 0.552993893623352 and parameters: {'observation_period_num': 96, 'train_rates': 0.9351529760229657, 'learning_rate': 1.6942731962208934e-05, 'batch_size': 236, 'step_size': 2, 'gamma': 0.9120428515117488}. Best is trial 0 with value: 0.2563398974335012.[0m
[32m[I 2025-02-05 20:12:09,028][0m Trial 4 finished with value: 0.34492889046669006 and parameters: {'observation_period_num': 236, 'train_rates': 0.9509310290517468, 'learning_rate': 1.602217318140228e-05, 'batch_size': 171, 'step_size': 11, 'gamma': 0.9848416751392023}. Best is trial 0 with value: 0.2563398974335012.[0m
[32m[I 2025-02-05 20:12:37,176][0m Trial 5 finished with value: 0.4035665315130483 and parameters: {'observation_period_num': 148, 'train_rates': 0.9100235052273571, 'learning_rate': 9.37192119676134e-06, 'batch_size': 222, 'step_size': 12, 'gamma': 0.8847147720322847}. Best is trial 0 with value: 0.2563398974335012.[0m
[32m[I 2025-02-05 20:13:10,759][0m Trial 6 finished with value: 0.47214606404304504 and parameters: {'observation_period_num': 31, 'train_rates': 0.9705672424374746, 'learning_rate': 6.145666310385487e-06, 'batch_size': 193, 'step_size': 10, 'gamma': 0.8081795981313619}. Best is trial 0 with value: 0.2563398974335012.[0m
[32m[I 2025-02-05 20:13:47,944][0m Trial 7 finished with value: 0.04015879683482107 and parameters: {'observation_period_num': 9, 'train_rates': 0.9028312830990897, 'learning_rate': 0.0004738163853870661, 'batch_size': 162, 'step_size': 14, 'gamma': 0.9777457784683116}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:14:18,963][0m Trial 8 finished with value: 0.0968410074710846 and parameters: {'observation_period_num': 47, 'train_rates': 0.986544225309323, 'learning_rate': 4.6202703061775406e-05, 'batch_size': 212, 'step_size': 5, 'gamma': 0.9414010032160334}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:16:12,059][0m Trial 9 finished with value: 0.500402096182971 and parameters: {'observation_period_num': 117, 'train_rates': 0.7933092326721434, 'learning_rate': 1.5007425240412903e-06, 'batch_size': 45, 'step_size': 9, 'gamma': 0.8930496972609683}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:17:33,533][0m Trial 10 finished with value: 0.041595366250419014 and parameters: {'observation_period_num': 10, 'train_rates': 0.8379871314853522, 'learning_rate': 0.00097234660529323, 'batch_size': 69, 'step_size': 15, 'gamma': 0.7668551917236928}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:18:59,451][0m Trial 11 finished with value: 0.04124100798485326 and parameters: {'observation_period_num': 11, 'train_rates': 0.8442769249610509, 'learning_rate': 0.0008203957059329791, 'batch_size': 64, 'step_size': 15, 'gamma': 0.7534833445639153}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:19:58,453][0m Trial 12 finished with value: 0.040927499500305754 and parameters: {'observation_period_num': 5, 'train_rates': 0.8441797226907459, 'learning_rate': 0.0002295563370302183, 'batch_size': 93, 'step_size': 15, 'gamma': 0.7684708257355843}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:20:31,753][0m Trial 13 finished with value: 0.21432562265406746 and parameters: {'observation_period_num': 79, 'train_rates': 0.7659235154888786, 'learning_rate': 0.00020258217915479086, 'batch_size': 160, 'step_size': 13, 'gamma': 0.844082134403724}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:21:29,621][0m Trial 14 finished with value: 0.11300690002873502 and parameters: {'observation_period_num': 157, 'train_rates': 0.8612242792661712, 'learning_rate': 0.0001893663624455033, 'batch_size': 95, 'step_size': 15, 'gamma': 0.7996275252078818}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:25:48,843][0m Trial 15 finished with value: 0.12407601395478615 and parameters: {'observation_period_num': 71, 'train_rates': 0.891849101010689, 'learning_rate': 0.00016650392197120213, 'batch_size': 21, 'step_size': 7, 'gamma': 0.9824378950362922}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:26:24,670][0m Trial 16 finished with value: 0.1679162991862442 and parameters: {'observation_period_num': 7, 'train_rates': 0.7489608341156487, 'learning_rate': 0.00037974676404246735, 'batch_size': 149, 'step_size': 13, 'gamma': 0.8546194423144726}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:27:10,709][0m Trial 17 finished with value: 0.15912933723933725 and parameters: {'observation_period_num': 171, 'train_rates': 0.8213395364036843, 'learning_rate': 8.776433433442394e-05, 'batch_size': 117, 'step_size': 5, 'gamma': 0.8140188512452231}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:28:16,314][0m Trial 18 finished with value: 0.10256294606270805 and parameters: {'observation_period_num': 38, 'train_rates': 0.8894891597751325, 'learning_rate': 0.00035251127351714373, 'batch_size': 88, 'step_size': 11, 'gamma': 0.9528927363981013}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:28:44,630][0m Trial 19 finished with value: 0.22797614122350668 and parameters: {'observation_period_num': 109, 'train_rates': 0.7157649872156178, 'learning_rate': 9.956494801223628e-05, 'batch_size': 188, 'step_size': 14, 'gamma': 0.7834796984236283}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:29:27,627][0m Trial 20 finished with value: 0.05514121211636129 and parameters: {'observation_period_num': 72, 'train_rates': 0.8677367514251576, 'learning_rate': 0.0003671568005617961, 'batch_size': 136, 'step_size': 11, 'gamma': 0.8263067498905183}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:30:36,819][0m Trial 21 finished with value: 0.05633698218273667 and parameters: {'observation_period_num': 24, 'train_rates': 0.8191911797947514, 'learning_rate': 0.0009820633645659608, 'batch_size': 78, 'step_size': 15, 'gamma': 0.7501976986158339}. Best is trial 7 with value: 0.04015879683482107.[0m
[32m[I 2025-02-05 20:32:23,147][0m Trial 22 finished with value: 0.03642592726060119 and parameters: {'observation_period_num': 6, 'train_rates': 0.9150405160450185, 'learning_rate': 0.0005726106849879072, 'batch_size': 55, 'step_size': 14, 'gamma': 0.7536121380681258}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:34:41,724][0m Trial 23 finished with value: 0.09498381450079209 and parameters: {'observation_period_num': 55, 'train_rates': 0.9261452201903914, 'learning_rate': 0.0003943669594022982, 'batch_size': 41, 'step_size': 13, 'gamma': 0.7825413327552757}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:36:41,198][0m Trial 24 finished with value: 0.0494510913524059 and parameters: {'observation_period_num': 28, 'train_rates': 0.8885348469166527, 'learning_rate': 0.00011136852529445684, 'batch_size': 48, 'step_size': 14, 'gamma': 0.7730610856065373}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:41:50,976][0m Trial 25 finished with value: 0.046648652762059095 and parameters: {'observation_period_num': 7, 'train_rates': 0.9562484479738174, 'learning_rate': 0.0005256115296302065, 'batch_size': 19, 'step_size': 9, 'gamma': 0.8694973949417254}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:42:47,472][0m Trial 26 finished with value: 0.20721804022692553 and parameters: {'observation_period_num': 48, 'train_rates': 0.7873158820123541, 'learning_rate': 0.00023972194055447236, 'batch_size': 92, 'step_size': 12, 'gamma': 0.922520469306145}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:43:42,374][0m Trial 27 finished with value: 0.8175827326616311 and parameters: {'observation_period_num': 93, 'train_rates': 0.9157806598066557, 'learning_rate': 1.068218264721224e-06, 'batch_size': 109, 'step_size': 14, 'gamma': 0.7607845040283284}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:44:20,611][0m Trial 28 finished with value: 0.04093016552909905 and parameters: {'observation_period_num': 25, 'train_rates': 0.8663722190314829, 'learning_rate': 0.0005444668880609097, 'batch_size': 150, 'step_size': 12, 'gamma': 0.7938203612548417}. Best is trial 22 with value: 0.03642592726060119.[0m
Early stopping at epoch 69
[32m[I 2025-02-05 20:45:20,013][0m Trial 29 finished with value: 0.24052366577997442 and parameters: {'observation_period_num': 61, 'train_rates': 0.8157461275096183, 'learning_rate': 5.789793586175251e-05, 'batch_size': 63, 'step_size': 1, 'gamma': 0.8254971788577392}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:45:55,947][0m Trial 30 finished with value: 0.0988761557491271 and parameters: {'observation_period_num': 41, 'train_rates': 0.9064055401715733, 'learning_rate': 0.00023273686864565876, 'batch_size': 173, 'step_size': 14, 'gamma': 0.9662320204509507}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:46:36,263][0m Trial 31 finished with value: 0.04328727391742957 and parameters: {'observation_period_num': 24, 'train_rates': 0.861092641922791, 'learning_rate': 0.0005834562550813029, 'batch_size': 145, 'step_size': 12, 'gamma': 0.7918476682239313}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:47:20,742][0m Trial 32 finished with value: 0.04676477549148729 and parameters: {'observation_period_num': 24, 'train_rates': 0.8767757090139083, 'learning_rate': 0.0006210031066473853, 'batch_size': 129, 'step_size': 14, 'gamma': 0.7733531517500485}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:48:05,842][0m Trial 33 finished with value: 0.14639051245259388 and parameters: {'observation_period_num': 6, 'train_rates': 0.6580343631997175, 'learning_rate': 0.00012736894985226273, 'batch_size': 108, 'step_size': 13, 'gamma': 0.78904467407063}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:48:43,386][0m Trial 34 finished with value: 0.12105545015579247 and parameters: {'observation_period_num': 190, 'train_rates': 0.9399988866438864, 'learning_rate': 0.0003003249413887701, 'batch_size': 155, 'step_size': 15, 'gamma': 0.8268454276214372}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:49:15,583][0m Trial 35 finished with value: 0.04814013862407703 and parameters: {'observation_period_num': 21, 'train_rates': 0.844454356058076, 'learning_rate': 0.0006420046553033737, 'batch_size': 183, 'step_size': 12, 'gamma': 0.7674519204719448}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:50:05,075][0m Trial 36 finished with value: 0.08524472434493832 and parameters: {'observation_period_num': 59, 'train_rates': 0.9083006741369222, 'learning_rate': 0.0004741649600600424, 'batch_size': 120, 'step_size': 11, 'gamma': 0.9244713684823134}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:50:34,899][0m Trial 37 finished with value: 0.28880321979522705 and parameters: {'observation_period_num': 251, 'train_rates': 0.9447425575501769, 'learning_rate': 7.119318154863522e-05, 'batch_size': 211, 'step_size': 10, 'gamma': 0.8043939199883517}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:51:15,464][0m Trial 38 finished with value: 0.13579782843589783 and parameters: {'observation_period_num': 36, 'train_rates': 0.9874189719372491, 'learning_rate': 3.706820378783162e-05, 'batch_size': 165, 'step_size': 4, 'gamma': 0.8738752809835846}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:51:59,160][0m Trial 39 finished with value: 0.6180545687675476 and parameters: {'observation_period_num': 87, 'train_rates': 0.963097754357177, 'learning_rate': 3.1650373457583804e-06, 'batch_size': 141, 'step_size': 13, 'gamma': 0.903463442947351}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:52:21,160][0m Trial 40 finished with value: 0.2692782915677085 and parameters: {'observation_period_num': 127, 'train_rates': 0.7666418020143625, 'learning_rate': 0.0001410199558012001, 'batch_size': 254, 'step_size': 8, 'gamma': 0.8393490190685361}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:53:49,252][0m Trial 41 finished with value: 0.04193970944513293 and parameters: {'observation_period_num': 16, 'train_rates': 0.8440501521965672, 'learning_rate': 0.0008009129445338855, 'batch_size': 63, 'step_size': 15, 'gamma': 0.7507698214287697}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:56:32,432][0m Trial 42 finished with value: 0.0588435042209877 and parameters: {'observation_period_num': 17, 'train_rates': 0.8357006452198463, 'learning_rate': 2.159996642277075e-05, 'batch_size': 33, 'step_size': 14, 'gamma': 0.7572464223194392}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:58:15,445][0m Trial 43 finished with value: 0.14224180339896766 and parameters: {'observation_period_num': 33, 'train_rates': 0.8594881726906864, 'learning_rate': 0.0008004466803759254, 'batch_size': 53, 'step_size': 15, 'gamma': 0.7798792717301659}. Best is trial 22 with value: 0.03642592726060119.[0m
[32m[I 2025-02-05 20:59:28,720][0m Trial 44 finished with value: 0.036140006197535476 and parameters: {'observation_period_num': 6, 'train_rates': 0.8831338462850308, 'learning_rate': 0.00026296012972942123, 'batch_size': 77, 'step_size': 14, 'gamma': 0.763469451699588}. Best is trial 44 with value: 0.036140006197535476.[0m
[32m[I 2025-02-05 21:00:45,679][0m Trial 45 finished with value: 0.056823035954830035 and parameters: {'observation_period_num': 44, 'train_rates': 0.9317633864005488, 'learning_rate': 0.0002611715078909613, 'batch_size': 76, 'step_size': 12, 'gamma': 0.7996981896012344}. Best is trial 44 with value: 0.036140006197535476.[0m
[32m[I 2025-02-05 21:01:46,613][0m Trial 46 finished with value: 0.03981686077789215 and parameters: {'observation_period_num': 14, 'train_rates': 0.897696332151011, 'learning_rate': 0.00017467383521320502, 'batch_size': 97, 'step_size': 14, 'gamma': 0.7642594172052602}. Best is trial 44 with value: 0.036140006197535476.[0m
[32m[I 2025-02-05 21:02:43,072][0m Trial 47 finished with value: 0.042489112192584624 and parameters: {'observation_period_num': 16, 'train_rates': 0.8946471418604175, 'learning_rate': 0.00016390248617885263, 'batch_size': 102, 'step_size': 13, 'gamma': 0.7663240566162127}. Best is trial 44 with value: 0.036140006197535476.[0m
[32m[I 2025-02-05 21:03:57,694][0m Trial 48 finished with value: 0.03953004347496345 and parameters: {'observation_period_num': 7, 'train_rates': 0.9174018296662215, 'learning_rate': 0.00020064840493635538, 'batch_size': 78, 'step_size': 14, 'gamma': 0.8161240305577286}. Best is trial 44 with value: 0.036140006197535476.[0m
[32m[I 2025-02-05 21:05:15,145][0m Trial 49 finished with value: 0.05260791629552841 and parameters: {'observation_period_num': 50, 'train_rates': 0.9735435167583079, 'learning_rate': 0.0003093840255956464, 'batch_size': 78, 'step_size': 10, 'gamma': 0.8602889154058573}. Best is trial 44 with value: 0.036140006197535476.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-05 21:05:15,155][0m A new study created in memory with name: no-name-b8d6999a-4f17-48a3-a1cf-b1950d4ac2bf[0m
[32m[I 2025-02-05 21:05:56,702][0m Trial 0 finished with value: 0.18198961497169652 and parameters: {'observation_period_num': 57, 'train_rates': 0.6219322900580976, 'learning_rate': 0.000170610254549033, 'batch_size': 108, 'step_size': 12, 'gamma': 0.9360127802236644}. Best is trial 0 with value: 0.18198961497169652.[0m
[32m[I 2025-02-05 21:07:28,406][0m Trial 1 finished with value: 0.09754043062491718 and parameters: {'observation_period_num': 87, 'train_rates': 0.8569382443607012, 'learning_rate': 0.0002184694650659307, 'batch_size': 59, 'step_size': 9, 'gamma': 0.916156729535693}. Best is trial 1 with value: 0.09754043062491718.[0m
[32m[I 2025-02-05 21:07:59,577][0m Trial 2 finished with value: 0.40286643275626766 and parameters: {'observation_period_num': 92, 'train_rates': 0.7465209023075492, 'learning_rate': 2.673485461737017e-05, 'batch_size': 169, 'step_size': 5, 'gamma': 0.8481476429276932}. Best is trial 1 with value: 0.09754043062491718.[0m
[32m[I 2025-02-05 21:08:30,637][0m Trial 3 finished with value: 0.20004532651555154 and parameters: {'observation_period_num': 93, 'train_rates': 0.7291082389946826, 'learning_rate': 0.00013783875928048987, 'batch_size': 167, 'step_size': 11, 'gamma': 0.8497847237318732}. Best is trial 1 with value: 0.09754043062491718.[0m
[32m[I 2025-02-05 21:09:25,895][0m Trial 4 finished with value: 0.22124793182110644 and parameters: {'observation_period_num': 127, 'train_rates': 0.7611028036998715, 'learning_rate': 0.00019301315366524792, 'batch_size': 90, 'step_size': 7, 'gamma': 0.9083111070873074}. Best is trial 1 with value: 0.09754043062491718.[0m
Early stopping at epoch 79
[32m[I 2025-02-05 21:09:48,115][0m Trial 5 finished with value: 1.9265539646148682 and parameters: {'observation_period_num': 58, 'train_rates': 0.9346995498206735, 'learning_rate': 3.3868169343801625e-06, 'batch_size': 243, 'step_size': 2, 'gamma': 0.7674796280179832}. Best is trial 1 with value: 0.09754043062491718.[0m
[32m[I 2025-02-05 21:11:00,996][0m Trial 6 finished with value: 0.5898220288300536 and parameters: {'observation_period_num': 235, 'train_rates': 0.603240289479411, 'learning_rate': 1.9474854324057157e-06, 'batch_size': 56, 'step_size': 5, 'gamma': 0.9887743114491649}. Best is trial 1 with value: 0.09754043062491718.[0m
[32m[I 2025-02-05 21:11:24,461][0m Trial 7 finished with value: 0.8995846810458756 and parameters: {'observation_period_num': 252, 'train_rates': 0.6031488993851168, 'learning_rate': 2.584934794149283e-06, 'batch_size': 185, 'step_size': 9, 'gamma': 0.9642062511637015}. Best is trial 1 with value: 0.09754043062491718.[0m
[32m[I 2025-02-05 21:12:01,598][0m Trial 8 finished with value: 0.3173334245532201 and parameters: {'observation_period_num': 193, 'train_rates': 0.9020490050636409, 'learning_rate': 1.0099716786368512e-05, 'batch_size': 153, 'step_size': 7, 'gamma': 0.9695157850115159}. Best is trial 1 with value: 0.09754043062491718.[0m
[32m[I 2025-02-05 21:12:24,906][0m Trial 9 finished with value: 0.3648323316659246 and parameters: {'observation_period_num': 183, 'train_rates': 0.8484366265120111, 'learning_rate': 3.221945482299755e-05, 'batch_size': 243, 'step_size': 2, 'gamma': 0.9291172482016833}. Best is trial 1 with value: 0.09754043062491718.[0m
[32m[I 2025-02-05 21:17:41,909][0m Trial 10 finished with value: 0.026480828175350787 and parameters: {'observation_period_num': 8, 'train_rates': 0.9786247782972987, 'learning_rate': 0.0006556421409558422, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8010688650695518}. Best is trial 10 with value: 0.026480828175350787.[0m
[32m[I 2025-02-05 21:22:58,403][0m Trial 11 finished with value: 0.032930289815161745 and parameters: {'observation_period_num': 9, 'train_rates': 0.9826812027865391, 'learning_rate': 0.0009051374872083221, 'batch_size': 19, 'step_size': 15, 'gamma': 0.7909262643262811}. Best is trial 10 with value: 0.026480828175350787.[0m
[32m[I 2025-02-05 21:28:51,051][0m Trial 12 finished with value: 0.02316133465824856 and parameters: {'observation_period_num': 5, 'train_rates': 0.9880908885845407, 'learning_rate': 0.0009918461542730392, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7819356330197345}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:34:37,122][0m Trial 13 finished with value: 0.032338082344116025 and parameters: {'observation_period_num': 6, 'train_rates': 0.9674776537458251, 'learning_rate': 0.000758883006018742, 'batch_size': 17, 'step_size': 15, 'gamma': 0.807625190715086}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:36:23,627][0m Trial 14 finished with value: 0.06033432190971715 and parameters: {'observation_period_num': 38, 'train_rates': 0.9136107622227543, 'learning_rate': 0.0005247471528528755, 'batch_size': 54, 'step_size': 12, 'gamma': 0.7579097790650896}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:37:18,108][0m Trial 15 finished with value: 0.051681442493977756 and parameters: {'observation_period_num': 35, 'train_rates': 0.8426559685125842, 'learning_rate': 6.68450905835788e-05, 'batch_size': 106, 'step_size': 14, 'gamma': 0.8185624833616149}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:39:53,904][0m Trial 16 finished with value: 0.16047438979148865 and parameters: {'observation_period_num': 157, 'train_rates': 0.9889821560206449, 'learning_rate': 0.00038472322918702386, 'batch_size': 38, 'step_size': 13, 'gamma': 0.8700146395261524}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:41:04,514][0m Trial 17 finished with value: 0.11624446993618343 and parameters: {'observation_period_num': 123, 'train_rates': 0.9389661535771596, 'learning_rate': 7.516352552605372e-05, 'batch_size': 82, 'step_size': 11, 'gamma': 0.7852289313010402}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:41:30,083][0m Trial 18 finished with value: 0.14465162530148457 and parameters: {'observation_period_num': 30, 'train_rates': 0.6759099956939278, 'learning_rate': 0.00038769521842301597, 'batch_size': 205, 'step_size': 15, 'gamma': 0.8269729662596118}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:42:14,635][0m Trial 19 finished with value: 0.07786426499793662 and parameters: {'observation_period_num': 61, 'train_rates': 0.8829831178152339, 'learning_rate': 0.0009772887553876125, 'batch_size': 134, 'step_size': 13, 'gamma': 0.7505512426149227}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:44:32,468][0m Trial 20 finished with value: 0.05287334917657143 and parameters: {'observation_period_num': 6, 'train_rates': 0.8083019182571085, 'learning_rate': 1.731598650338423e-05, 'batch_size': 38, 'step_size': 11, 'gamma': 0.8732991562893808}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:49:23,319][0m Trial 21 finished with value: 0.0322275675005383 and parameters: {'observation_period_num': 8, 'train_rates': 0.9513129731415609, 'learning_rate': 0.0005226961768087857, 'batch_size': 20, 'step_size': 15, 'gamma': 0.8074710687245923}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:52:15,477][0m Trial 22 finished with value: 0.051483510075895876 and parameters: {'observation_period_num': 28, 'train_rates': 0.9540110697696884, 'learning_rate': 0.00047409302264183993, 'batch_size': 34, 'step_size': 14, 'gamma': 0.7870055476317118}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:53:33,193][0m Trial 23 finished with value: 0.08394450376045356 and parameters: {'observation_period_num': 74, 'train_rates': 0.9285718370198471, 'learning_rate': 0.00034034534647885416, 'batch_size': 76, 'step_size': 13, 'gamma': 0.8356213032193651}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:58:17,964][0m Trial 24 finished with value: 0.03184622652023225 and parameters: {'observation_period_num': 23, 'train_rates': 0.9873918592088625, 'learning_rate': 9.567085318422308e-05, 'batch_size': 21, 'step_size': 15, 'gamma': 0.7986630487542982}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 21:59:54,879][0m Trial 25 finished with value: 0.06110084056854248 and parameters: {'observation_period_num': 43, 'train_rates': 0.9896622634112108, 'learning_rate': 8.171502561000637e-05, 'batch_size': 64, 'step_size': 14, 'gamma': 0.7734492733073979}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:02:11,880][0m Trial 26 finished with value: 0.09549610082387597 and parameters: {'observation_period_num': 19, 'train_rates': 0.8761511269310427, 'learning_rate': 6.312238545327871e-06, 'batch_size': 41, 'step_size': 10, 'gamma': 0.7987005966625882}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:03:01,438][0m Trial 27 finished with value: 0.0694679961392754 and parameters: {'observation_period_num': 47, 'train_rates': 0.9020208554546646, 'learning_rate': 9.329255962870799e-05, 'batch_size': 122, 'step_size': 13, 'gamma': 0.8893745610660212}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:04:22,183][0m Trial 28 finished with value: 0.11406110592473012 and parameters: {'observation_period_num': 75, 'train_rates': 0.963897774970433, 'learning_rate': 4.1240961413471625e-05, 'batch_size': 75, 'step_size': 14, 'gamma': 0.8459571902462624}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:05:16,048][0m Trial 29 finished with value: 0.042342796371034955 and parameters: {'observation_period_num': 24, 'train_rates': 0.921603866811451, 'learning_rate': 0.00021181821166847565, 'batch_size': 109, 'step_size': 12, 'gamma': 0.7749838324953913}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:06:10,247][0m Trial 30 finished with value: 1.7204952137943372 and parameters: {'observation_period_num': 103, 'train_rates': 0.8197668122899436, 'learning_rate': 1.0228162841445167e-06, 'batch_size': 98, 'step_size': 4, 'gamma': 0.8224069767372177}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:11:18,966][0m Trial 31 finished with value: 0.03949331038839315 and parameters: {'observation_period_num': 19, 'train_rates': 0.9588315099377576, 'learning_rate': 0.0006223994186112696, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8034031529082054}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:14:24,615][0m Trial 32 finished with value: 0.09057927884313526 and parameters: {'observation_period_num': 56, 'train_rates': 0.94941585712213, 'learning_rate': 0.0002943985759340865, 'batch_size': 31, 'step_size': 15, 'gamma': 0.811710002623408}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:16:22,660][0m Trial 33 finished with value: 0.02749871441363534 and parameters: {'observation_period_num': 7, 'train_rates': 0.9734083485571051, 'learning_rate': 0.00013960329099138385, 'batch_size': 51, 'step_size': 12, 'gamma': 0.7803814587240187}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:18:21,726][0m Trial 34 finished with value: 0.047888462933210224 and parameters: {'observation_period_num': 42, 'train_rates': 0.978012977450192, 'learning_rate': 0.00013469519709185588, 'batch_size': 50, 'step_size': 12, 'gamma': 0.7602197747433839}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:19:37,061][0m Trial 35 finished with value: 0.14315249719363862 and parameters: {'observation_period_num': 21, 'train_rates': 0.6695139698424231, 'learning_rate': 0.00013425645130334249, 'batch_size': 62, 'step_size': 10, 'gamma': 0.7837586487212009}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:22:44,728][0m Trial 36 finished with value: 0.11803143597498138 and parameters: {'observation_period_num': 73, 'train_rates': 0.8832482198750272, 'learning_rate': 5.1658575596094014e-05, 'batch_size': 29, 'step_size': 14, 'gamma': 0.8356561478458364}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:24:30,099][0m Trial 37 finished with value: 0.21662661142578066 and parameters: {'observation_period_num': 105, 'train_rates': 0.7734663038267877, 'learning_rate': 0.0001863428621537488, 'batch_size': 47, 'step_size': 8, 'gamma': 0.8583513079483469}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:25:54,961][0m Trial 38 finished with value: 0.0637171966756912 and parameters: {'observation_period_num': 51, 'train_rates': 0.9353665548104927, 'learning_rate': 0.0002704613331106695, 'batch_size': 68, 'step_size': 12, 'gamma': 0.769441541651197}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:28:43,267][0m Trial 39 finished with value: 0.23504811155101274 and parameters: {'observation_period_num': 150, 'train_rates': 0.7092687344939794, 'learning_rate': 1.7886926967344837e-05, 'batch_size': 27, 'step_size': 13, 'gamma': 0.7948449892992822}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:30:39,781][0m Trial 40 finished with value: 0.06922670275862537 and parameters: {'observation_period_num': 65, 'train_rates': 0.9074140938152228, 'learning_rate': 0.00014082316627874307, 'batch_size': 48, 'step_size': 10, 'gamma': 0.7750788528966404}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:36:46,447][0m Trial 41 finished with value: 0.03669712909807762 and parameters: {'observation_period_num': 5, 'train_rates': 0.9676912443618024, 'learning_rate': 0.0006538029741607156, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8093767481047943}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:40:32,563][0m Trial 42 finished with value: 0.03830216752572192 and parameters: {'observation_period_num': 16, 'train_rates': 0.9510525050110791, 'learning_rate': 0.0006198403852278962, 'batch_size': 26, 'step_size': 14, 'gamma': 0.8026319495680758}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:46:17,687][0m Trial 43 finished with value: 0.04311151073222429 and parameters: {'observation_period_num': 33, 'train_rates': 0.9758656090045299, 'learning_rate': 0.0002578914091010776, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7809296974490326}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:48:40,265][0m Trial 44 finished with value: 0.04329153423993614 and parameters: {'observation_period_num': 15, 'train_rates': 0.9391361201975519, 'learning_rate': 0.0004587909548222447, 'batch_size': 41, 'step_size': 14, 'gamma': 0.7634207700561035}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:50:16,599][0m Trial 45 finished with value: 0.1729834098368883 and parameters: {'observation_period_num': 220, 'train_rates': 0.9710625937280452, 'learning_rate': 0.0009387782803843619, 'batch_size': 58, 'step_size': 15, 'gamma': 0.8167772485682847}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:53:38,520][0m Trial 46 finished with value: 0.04406806826591492 and parameters: {'observation_period_num': 30, 'train_rates': 0.987183348572339, 'learning_rate': 0.0001109274535115255, 'batch_size': 30, 'step_size': 6, 'gamma': 0.82907701333302}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:54:46,807][0m Trial 47 finished with value: 0.045205076862322655 and parameters: {'observation_period_num': 11, 'train_rates': 0.948385431402514, 'learning_rate': 5.5136356973041996e-05, 'batch_size': 89, 'step_size': 13, 'gamma': 0.7979754359740783}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:55:14,304][0m Trial 48 finished with value: 0.035120092067322715 and parameters: {'observation_period_num': 6, 'train_rates': 0.914332366575927, 'learning_rate': 0.0007285687137545245, 'batch_size': 229, 'step_size': 14, 'gamma': 0.7911552256410816}. Best is trial 12 with value: 0.02316133465824856.[0m
[32m[I 2025-02-05 22:55:49,448][0m Trial 49 finished with value: 0.04814356789496299 and parameters: {'observation_period_num': 37, 'train_rates': 0.8986821902572857, 'learning_rate': 0.00046023021905049644, 'batch_size': 167, 'step_size': 11, 'gamma': 0.7574233691282117}. Best is trial 12 with value: 0.02316133465824856.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-05 22:55:49,459][0m A new study created in memory with name: no-name-3e3a2612-3bad-44de-80f8-77e287420832[0m
[32m[I 2025-02-05 22:56:30,880][0m Trial 0 finished with value: 0.2903654113743428 and parameters: {'observation_period_num': 245, 'train_rates': 0.6847490904298881, 'learning_rate': 0.00024658669884319374, 'batch_size': 113, 'step_size': 4, 'gamma': 0.8956344078643039}. Best is trial 0 with value: 0.2903654113743428.[0m
[32m[I 2025-02-05 22:57:20,498][0m Trial 1 finished with value: 0.12839503299858834 and parameters: {'observation_period_num': 71, 'train_rates': 0.8439280010356136, 'learning_rate': 2.3879437806623792e-05, 'batch_size': 113, 'step_size': 8, 'gamma': 0.8583940572663012}. Best is trial 1 with value: 0.12839503299858834.[0m
[32m[I 2025-02-05 22:57:46,250][0m Trial 2 finished with value: 0.9985056628860552 and parameters: {'observation_period_num': 70, 'train_rates': 0.7316716433138365, 'learning_rate': 2.3218943355096716e-06, 'batch_size': 226, 'step_size': 5, 'gamma': 0.886035088353722}. Best is trial 1 with value: 0.12839503299858834.[0m
[32m[I 2025-02-05 22:58:16,889][0m Trial 3 finished with value: 0.15751911580152467 and parameters: {'observation_period_num': 23, 'train_rates': 0.6652846480553981, 'learning_rate': 0.00013409709626634013, 'batch_size': 171, 'step_size': 4, 'gamma': 0.9416562797907656}. Best is trial 1 with value: 0.12839503299858834.[0m
[32m[I 2025-02-05 22:58:47,723][0m Trial 4 finished with value: 0.5652289445238902 and parameters: {'observation_period_num': 234, 'train_rates': 0.7557508818712458, 'learning_rate': 5.5270971330665244e-05, 'batch_size': 167, 'step_size': 2, 'gamma': 0.8433044460381742}. Best is trial 1 with value: 0.12839503299858834.[0m
Early stopping at epoch 71
[32m[I 2025-02-05 22:59:05,783][0m Trial 5 finished with value: 0.8380774427700594 and parameters: {'observation_period_num': 17, 'train_rates': 0.7053941171318541, 'learning_rate': 1.764343879552874e-06, 'batch_size': 228, 'step_size': 1, 'gamma': 0.8458968073734087}. Best is trial 1 with value: 0.12839503299858834.[0m
[32m[I 2025-02-05 23:01:27,769][0m Trial 6 finished with value: 0.2275928630853978 and parameters: {'observation_period_num': 230, 'train_rates': 0.8084535738132095, 'learning_rate': 1.4408465003297691e-05, 'batch_size': 34, 'step_size': 11, 'gamma': 0.7979385997324175}. Best is trial 1 with value: 0.12839503299858834.[0m
Early stopping at epoch 56
[32m[I 2025-02-05 23:01:46,667][0m Trial 7 finished with value: 0.3060558470297445 and parameters: {'observation_period_num': 54, 'train_rates': 0.6628601708182792, 'learning_rate': 0.0005074728936444953, 'batch_size': 153, 'step_size': 1, 'gamma': 0.7801913012241918}. Best is trial 1 with value: 0.12839503299858834.[0m
[32m[I 2025-02-05 23:02:07,618][0m Trial 8 finished with value: 0.8189050491255615 and parameters: {'observation_period_num': 171, 'train_rates': 0.6567221862253896, 'learning_rate': 7.87501713885348e-06, 'batch_size': 235, 'step_size': 2, 'gamma': 0.8514697952512913}. Best is trial 1 with value: 0.12839503299858834.[0m
Early stopping at epoch 80
[32m[I 2025-02-05 23:02:46,780][0m Trial 9 finished with value: 0.25157120172887937 and parameters: {'observation_period_num': 114, 'train_rates': 0.793621825936092, 'learning_rate': 0.00034122908947376945, 'batch_size': 109, 'step_size': 1, 'gamma': 0.803614955597262}. Best is trial 1 with value: 0.12839503299858834.[0m
[32m[I 2025-02-05 23:04:49,364][0m Trial 10 finished with value: 0.18324894378007936 and parameters: {'observation_period_num': 129, 'train_rates': 0.9145785665373598, 'learning_rate': 3.730469276918332e-05, 'batch_size': 46, 'step_size': 15, 'gamma': 0.9886393234867926}. Best is trial 1 with value: 0.12839503299858834.[0m
[32m[I 2025-02-05 23:05:22,966][0m Trial 11 finished with value: 0.0515017069509772 and parameters: {'observation_period_num': 23, 'train_rates': 0.8961119757682932, 'learning_rate': 0.00012163412360024715, 'batch_size': 182, 'step_size': 8, 'gamma': 0.9476916835911702}. Best is trial 11 with value: 0.0515017069509772.[0m
[32m[I 2025-02-05 23:06:31,914][0m Trial 12 finished with value: 0.07028982412884943 and parameters: {'observation_period_num': 74, 'train_rates': 0.911106916945834, 'learning_rate': 9.684821244006832e-05, 'batch_size': 82, 'step_size': 9, 'gamma': 0.9245068271623866}. Best is trial 11 with value: 0.0515017069509772.[0m
[32m[I 2025-02-05 23:07:55,097][0m Trial 13 finished with value: 0.09911159425973892 and parameters: {'observation_period_num': 98, 'train_rates': 0.9752806498357695, 'learning_rate': 0.00010022715977730761, 'batch_size': 72, 'step_size': 9, 'gamma': 0.9360790944943685}. Best is trial 11 with value: 0.0515017069509772.[0m
[32m[I 2025-02-05 23:08:28,347][0m Trial 14 finished with value: 0.031249795805293912 and parameters: {'observation_period_num': 13, 'train_rates': 0.8937300973762938, 'learning_rate': 0.0008543100658951358, 'batch_size': 185, 'step_size': 11, 'gamma': 0.9388352246079599}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:08:57,423][0m Trial 15 finished with value: 0.03650469818840856 and parameters: {'observation_period_num': 6, 'train_rates': 0.8831371661624678, 'learning_rate': 0.0007263904407791241, 'batch_size': 201, 'step_size': 13, 'gamma': 0.9798063193805552}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:09:31,218][0m Trial 16 finished with value: 0.04679380729794502 and parameters: {'observation_period_num': 7, 'train_rates': 0.9676365820188713, 'learning_rate': 0.0009796084591895896, 'batch_size': 198, 'step_size': 14, 'gamma': 0.9836573527901437}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:10:00,887][0m Trial 17 finished with value: 0.0582896564155817 and parameters: {'observation_period_num': 44, 'train_rates': 0.8517337708440539, 'learning_rate': 0.0008239292312292364, 'batch_size': 204, 'step_size': 12, 'gamma': 0.9658808716233611}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:10:23,844][0m Trial 18 finished with value: 0.16267174071845272 and parameters: {'observation_period_num': 179, 'train_rates': 0.869279698930761, 'learning_rate': 0.0005660659140225824, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9050019295596209}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:11:05,620][0m Trial 19 finished with value: 0.15326766285793914 and parameters: {'observation_period_num': 160, 'train_rates': 0.9419225444876023, 'learning_rate': 0.0002765861167965773, 'batch_size': 141, 'step_size': 13, 'gamma': 0.9589245029110689}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:11:30,764][0m Trial 20 finished with value: 0.20245318741606022 and parameters: {'observation_period_num': 45, 'train_rates': 0.6207658242819528, 'learning_rate': 0.00021300323671969695, 'batch_size': 195, 'step_size': 10, 'gamma': 0.9159835681913923}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:12:04,107][0m Trial 21 finished with value: 0.038965173065662384 and parameters: {'observation_period_num': 5, 'train_rates': 0.9824660755138417, 'learning_rate': 0.0009236451497992157, 'batch_size': 206, 'step_size': 15, 'gamma': 0.9855255806864787}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:12:35,777][0m Trial 22 finished with value: 0.033100854605436325 and parameters: {'observation_period_num': 8, 'train_rates': 0.9880336115099434, 'learning_rate': 0.0005904634639643983, 'batch_size': 215, 'step_size': 15, 'gamma': 0.9709812200037191}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:13:02,558][0m Trial 23 finished with value: 0.08200904726982117 and parameters: {'observation_period_num': 43, 'train_rates': 0.9341687498344672, 'learning_rate': 0.000465737448245506, 'batch_size': 240, 'step_size': 13, 'gamma': 0.9627291773303227}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:13:30,166][0m Trial 24 finished with value: 0.05546570943869712 and parameters: {'observation_period_num': 31, 'train_rates': 0.8765943011151452, 'learning_rate': 0.0005095203333919245, 'batch_size': 217, 'step_size': 14, 'gamma': 0.9703463917473005}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:13:59,955][0m Trial 25 finished with value: 0.0941206704679333 and parameters: {'observation_period_num': 91, 'train_rates': 0.8038579835084194, 'learning_rate': 0.0001759446922707848, 'batch_size': 188, 'step_size': 11, 'gamma': 0.9292191431046166}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:14:39,120][0m Trial 26 finished with value: 0.08611906147221239 and parameters: {'observation_period_num': 59, 'train_rates': 0.9435972737331155, 'learning_rate': 5.6653545397991975e-05, 'batch_size': 158, 'step_size': 13, 'gamma': 0.8758771014430132}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:15:02,360][0m Trial 27 finished with value: 0.23233009351597184 and parameters: {'observation_period_num': 32, 'train_rates': 0.8334081408603509, 'learning_rate': 6.015835237484271e-06, 'batch_size': 248, 'step_size': 15, 'gamma': 0.9510586047915588}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:15:44,566][0m Trial 28 finished with value: 0.15472884663080766 and parameters: {'observation_period_num': 202, 'train_rates': 0.8928606229937538, 'learning_rate': 0.00033693992127468227, 'batch_size': 136, 'step_size': 11, 'gamma': 0.9714742005190907}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:16:14,040][0m Trial 29 finished with value: 0.11839722841978073 and parameters: {'observation_period_num': 136, 'train_rates': 0.954274071289026, 'learning_rate': 0.0006243577355668135, 'batch_size': 215, 'step_size': 7, 'gamma': 0.9062445776500495}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:16:51,387][0m Trial 30 finished with value: 0.047631166875362396 and parameters: {'observation_period_num': 6, 'train_rates': 0.9872368944232599, 'learning_rate': 0.00023964537042633362, 'batch_size': 173, 'step_size': 14, 'gamma': 0.8984518531925247}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:17:23,078][0m Trial 31 finished with value: 0.0335986390709877 and parameters: {'observation_period_num': 14, 'train_rates': 0.9896430233103577, 'learning_rate': 0.0009413917801910023, 'batch_size': 213, 'step_size': 15, 'gamma': 0.9837318836039334}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:17:52,018][0m Trial 32 finished with value: 0.05027786410245739 and parameters: {'observation_period_num': 33, 'train_rates': 0.9167259472941538, 'learning_rate': 0.000765919252137215, 'batch_size': 216, 'step_size': 12, 'gamma': 0.9810203021547014}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:18:22,153][0m Trial 33 finished with value: 0.1654596790527643 and parameters: {'observation_period_num': 16, 'train_rates': 0.7708482371445379, 'learning_rate': 0.00038269596986747074, 'batch_size': 185, 'step_size': 14, 'gamma': 0.9518923935240841}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:18:48,623][0m Trial 34 finished with value: 0.11933326483143811 and parameters: {'observation_period_num': 64, 'train_rates': 0.8334966785035948, 'learning_rate': 0.0009329699750316602, 'batch_size': 222, 'step_size': 15, 'gamma': 0.9683821106145515}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:19:19,685][0m Trial 35 finished with value: 0.1415737420320511 and parameters: {'observation_period_num': 85, 'train_rates': 0.9567794151193229, 'learning_rate': 0.00017071544475840068, 'batch_size': 208, 'step_size': 13, 'gamma': 0.9203130091829115}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:19:45,640][0m Trial 36 finished with value: 0.047170656391986476 and parameters: {'observation_period_num': 21, 'train_rates': 0.8763373595507906, 'learning_rate': 0.0005880573879560563, 'batch_size': 235, 'step_size': 14, 'gamma': 0.942111298052325}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:20:21,399][0m Trial 37 finished with value: 0.08885403437183258 and parameters: {'observation_period_num': 46, 'train_rates': 0.9355769365559414, 'learning_rate': 0.0003542794162147478, 'batch_size': 172, 'step_size': 6, 'gamma': 0.9754977542441424}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:21:02,427][0m Trial 38 finished with value: 1.1439402327133021 and parameters: {'observation_period_num': 77, 'train_rates': 0.729654565090297, 'learning_rate': 1.0838716151789422e-06, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8860969072197469}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:21:43,684][0m Trial 39 finished with value: 0.04959837719798088 and parameters: {'observation_period_num': 31, 'train_rates': 0.9638167347915394, 'learning_rate': 0.0006427262355603373, 'batch_size': 156, 'step_size': 12, 'gamma': 0.989236875477319}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:22:10,522][0m Trial 40 finished with value: 0.35533429206102746 and parameters: {'observation_period_num': 18, 'train_rates': 0.851856802437804, 'learning_rate': 4.797329936787516e-06, 'batch_size': 227, 'step_size': 15, 'gamma': 0.8306281085596241}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:22:43,493][0m Trial 41 finished with value: 0.03224077448248863 and parameters: {'observation_period_num': 5, 'train_rates': 0.9779081048223992, 'learning_rate': 0.0009660069608734583, 'batch_size': 204, 'step_size': 15, 'gamma': 0.9574120586533322}. Best is trial 14 with value: 0.031249795805293912.[0m
[32m[I 2025-02-05 23:23:18,105][0m Trial 42 finished with value: 0.02747483178973198 and parameters: {'observation_period_num': 6, 'train_rates': 0.9889479038177518, 'learning_rate': 0.0009997479022770277, 'batch_size': 194, 'step_size': 14, 'gamma': 0.9563824894567773}. Best is trial 42 with value: 0.02747483178973198.[0m
[32m[I 2025-02-05 23:23:54,354][0m Trial 43 finished with value: 0.03658222407102585 and parameters: {'observation_period_num': 22, 'train_rates': 0.9895713690829019, 'learning_rate': 0.0004306437652945332, 'batch_size': 182, 'step_size': 15, 'gamma': 0.9361083960700907}. Best is trial 42 with value: 0.02747483178973198.[0m
[32m[I 2025-02-05 23:24:26,461][0m Trial 44 finished with value: 0.1366124219660248 and parameters: {'observation_period_num': 55, 'train_rates': 0.9229496403459817, 'learning_rate': 2.0585601011182022e-05, 'batch_size': 195, 'step_size': 14, 'gamma': 0.95568640927701}. Best is trial 42 with value: 0.02747483178973198.[0m
[32m[I 2025-02-05 23:24:53,292][0m Trial 45 finished with value: 0.05837656930088997 and parameters: {'observation_period_num': 35, 'train_rates': 0.9602132300145435, 'learning_rate': 0.0009498501869243216, 'batch_size': 239, 'step_size': 15, 'gamma': 0.9439289625167434}. Best is trial 42 with value: 0.02747483178973198.[0m
[32m[I 2025-02-05 23:25:30,481][0m Trial 46 finished with value: 0.19908127188682556 and parameters: {'observation_period_num': 250, 'train_rates': 0.9752016476558327, 'learning_rate': 0.00027496463212974047, 'batch_size': 162, 'step_size': 4, 'gamma': 0.7668482507357881}. Best is trial 42 with value: 0.02747483178973198.[0m
[32m[I 2025-02-05 23:26:13,913][0m Trial 47 finished with value: 0.03402433544397354 and parameters: {'observation_period_num': 16, 'train_rates': 0.9889112777231028, 'learning_rate': 0.0006751387792388356, 'batch_size': 146, 'step_size': 14, 'gamma': 0.9604749876204496}. Best is trial 42 with value: 0.02747483178973198.[0m
[32m[I 2025-02-05 23:26:50,501][0m Trial 48 finished with value: 0.04273092746734619 and parameters: {'observation_period_num': 14, 'train_rates': 0.9493053534195911, 'learning_rate': 0.0004359379103231942, 'batch_size': 178, 'step_size': 13, 'gamma': 0.930437429929015}. Best is trial 42 with value: 0.02747483178973198.[0m
[32m[I 2025-02-05 23:27:18,877][0m Trial 49 finished with value: 0.06198202023061655 and parameters: {'observation_period_num': 27, 'train_rates': 0.8994524605923649, 'learning_rate': 6.52968206485486e-05, 'batch_size': 230, 'step_size': 10, 'gamma': 0.9178792218775121}. Best is trial 42 with value: 0.02747483178973198.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-05 23:27:18,888][0m A new study created in memory with name: no-name-32376155-691e-4a91-bf92-efd3b17810ed[0m
[32m[I 2025-02-05 23:29:03,445][0m Trial 0 finished with value: 0.16696326739195758 and parameters: {'observation_period_num': 25, 'train_rates': 0.728542805256716, 'learning_rate': 0.0002918475980107751, 'batch_size': 47, 'step_size': 2, 'gamma': 0.7771883403108124}. Best is trial 0 with value: 0.16696326739195758.[0m
Early stopping at epoch 92
[32m[I 2025-02-05 23:29:31,646][0m Trial 1 finished with value: 0.9457958592539248 and parameters: {'observation_period_num': 141, 'train_rates': 0.7957745964895127, 'learning_rate': 1.7415673795021625e-06, 'batch_size': 183, 'step_size': 1, 'gamma': 0.8963075219188149}. Best is trial 0 with value: 0.16696326739195758.[0m
[32m[I 2025-02-05 23:30:11,942][0m Trial 2 finished with value: 0.723275964993697 and parameters: {'observation_period_num': 108, 'train_rates': 0.8860159298364745, 'learning_rate': 2.060492198337601e-06, 'batch_size': 140, 'step_size': 13, 'gamma': 0.8208441636007784}. Best is trial 0 with value: 0.16696326739195758.[0m
[32m[I 2025-02-05 23:31:03,322][0m Trial 3 finished with value: 0.302588131866957 and parameters: {'observation_period_num': 40, 'train_rates': 0.7718294197613612, 'learning_rate': 1.0933955784829732e-05, 'batch_size': 105, 'step_size': 13, 'gamma': 0.7681829419728703}. Best is trial 0 with value: 0.16696326739195758.[0m
Early stopping at epoch 63
[32m[I 2025-02-05 23:31:33,716][0m Trial 4 finished with value: 0.15267531783696411 and parameters: {'observation_period_num': 123, 'train_rates': 0.8471630069529567, 'learning_rate': 0.0005342036810643399, 'batch_size': 118, 'step_size': 1, 'gamma': 0.8104947505877229}. Best is trial 4 with value: 0.15267531783696411.[0m
[32m[I 2025-02-05 23:31:55,392][0m Trial 5 finished with value: 0.2828922973110758 and parameters: {'observation_period_num': 194, 'train_rates': 0.7586058182215182, 'learning_rate': 0.0008868677535774402, 'batch_size': 253, 'step_size': 4, 'gamma': 0.8076581371460432}. Best is trial 4 with value: 0.15267531783696411.[0m
[32m[I 2025-02-05 23:32:27,546][0m Trial 6 finished with value: 0.2917645768083017 and parameters: {'observation_period_num': 164, 'train_rates': 0.6876667607289525, 'learning_rate': 0.0004453641220070837, 'batch_size': 150, 'step_size': 11, 'gamma': 0.7726757121491958}. Best is trial 4 with value: 0.15267531783696411.[0m
[32m[I 2025-02-05 23:33:52,656][0m Trial 7 finished with value: 0.27089942156313757 and parameters: {'observation_period_num': 185, 'train_rates': 0.6845698034530093, 'learning_rate': 7.060846656244471e-05, 'batch_size': 52, 'step_size': 6, 'gamma': 0.8569034726406765}. Best is trial 4 with value: 0.15267531783696411.[0m
[32m[I 2025-02-05 23:35:50,783][0m Trial 8 finished with value: 0.328554208559251 and parameters: {'observation_period_num': 6, 'train_rates': 0.7099055524297612, 'learning_rate': 1.2343733506516016e-06, 'batch_size': 41, 'step_size': 11, 'gamma': 0.8648687349237892}. Best is trial 4 with value: 0.15267531783696411.[0m
[32m[I 2025-02-05 23:36:54,889][0m Trial 9 finished with value: 0.1869218945503235 and parameters: {'observation_period_num': 230, 'train_rates': 0.97862336303593, 'learning_rate': 0.00011140210276973238, 'batch_size': 89, 'step_size': 6, 'gamma': 0.9767593439021953}. Best is trial 4 with value: 0.15267531783696411.[0m
[32m[I 2025-02-05 23:37:22,969][0m Trial 10 finished with value: 0.19787134014326951 and parameters: {'observation_period_num': 87, 'train_rates': 0.848292652263689, 'learning_rate': 1.664271207460716e-05, 'batch_size': 204, 'step_size': 8, 'gamma': 0.9273760446549598}. Best is trial 4 with value: 0.15267531783696411.[0m
Early stopping at epoch 78
[32m[I 2025-02-05 23:40:25,617][0m Trial 11 finished with value: 0.07079364327674216 and parameters: {'observation_period_num': 71, 'train_rates': 0.90228627864077, 'learning_rate': 0.00027484457741920056, 'batch_size': 24, 'step_size': 1, 'gamma': 0.8146017789256724}. Best is trial 11 with value: 0.07079364327674216.[0m
[32m[I 2025-02-05 23:45:38,518][0m Trial 12 finished with value: 0.06350061431834378 and parameters: {'observation_period_num': 72, 'train_rates': 0.9328915657642596, 'learning_rate': 0.0001784721868029254, 'batch_size': 18, 'step_size': 3, 'gamma': 0.811704904013427}. Best is trial 12 with value: 0.06350061431834378.[0m
[32m[I 2025-02-05 23:50:29,660][0m Trial 13 finished with value: 0.05798223180075487 and parameters: {'observation_period_num': 59, 'train_rates': 0.975276469236855, 'learning_rate': 0.0001314973041692545, 'batch_size': 20, 'step_size': 4, 'gamma': 0.8373469592562887}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-05 23:51:54,824][0m Trial 14 finished with value: 0.08206361532211304 and parameters: {'observation_period_num': 59, 'train_rates': 0.9831382689530622, 'learning_rate': 0.00010682833759316397, 'batch_size': 72, 'step_size': 4, 'gamma': 0.8390713162466077}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-05 23:57:48,060][0m Trial 15 finished with value: 0.06756058072124548 and parameters: {'observation_period_num': 89, 'train_rates': 0.9300396535334726, 'learning_rate': 3.0191624128400667e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8994806145569625}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-05 23:58:49,081][0m Trial 16 finished with value: 0.18373113339271563 and parameters: {'observation_period_num': 52, 'train_rates': 0.6047663305521858, 'learning_rate': 5.5047663356042395e-05, 'batch_size': 72, 'step_size': 7, 'gamma': 0.9065211791477099}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:04:38,323][0m Trial 17 finished with value: 0.0768902762369676 and parameters: {'observation_period_num': 97, 'train_rates': 0.9387054552660927, 'learning_rate': 0.000178911522805051, 'batch_size': 16, 'step_size': 3, 'gamma': 0.7933428398230797}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:06:01,332][0m Trial 18 finished with value: 0.15381761757951035 and parameters: {'observation_period_num': 23, 'train_rates': 0.9481372549513137, 'learning_rate': 8.640979328668755e-06, 'batch_size': 72, 'step_size': 9, 'gamma': 0.8413379678685099}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:06:36,042][0m Trial 19 finished with value: 0.2924210050785511 and parameters: {'observation_period_num': 138, 'train_rates': 0.8606817307185796, 'learning_rate': 3.838277911073755e-05, 'batch_size': 164, 'step_size': 5, 'gamma': 0.7563740714107898}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:07:23,537][0m Trial 20 finished with value: 0.0665525446358176 and parameters: {'observation_period_num': 77, 'train_rates': 0.8099952178748356, 'learning_rate': 0.0001397666937657542, 'batch_size': 113, 'step_size': 15, 'gamma': 0.9580054400840852}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:07:50,608][0m Trial 21 finished with value: 0.08262560932667225 and parameters: {'observation_period_num': 74, 'train_rates': 0.8130403416785215, 'learning_rate': 0.00014975814084554196, 'batch_size': 227, 'step_size': 15, 'gamma': 0.987389369231001}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:10:15,789][0m Trial 22 finished with value: 0.14481082630644942 and parameters: {'observation_period_num': 115, 'train_rates': 0.9054054444094553, 'learning_rate': 0.0002331481173295732, 'batch_size': 38, 'step_size': 9, 'gamma': 0.9526195991649066}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:11:09,299][0m Trial 23 finished with value: 0.13189536333084106 and parameters: {'observation_period_num': 41, 'train_rates': 0.9638935338378892, 'learning_rate': 0.0009907869683853361, 'batch_size': 113, 'step_size': 15, 'gamma': 0.8397263880916834}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:12:24,530][0m Trial 24 finished with value: 0.24105673032123004 and parameters: {'observation_period_num': 76, 'train_rates': 0.6381804907871688, 'learning_rate': 7.548154429890252e-05, 'batch_size': 60, 'step_size': 3, 'gamma': 0.8853780264569177}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:13:21,931][0m Trial 25 finished with value: 0.0826940944089609 and parameters: {'observation_period_num': 64, 'train_rates': 0.8236888551441311, 'learning_rate': 2.554719416379861e-05, 'batch_size': 95, 'step_size': 7, 'gamma': 0.9327870091593973}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:13:54,637][0m Trial 26 finished with value: 0.07481984917236412 and parameters: {'observation_period_num': 100, 'train_rates': 0.8794107936354527, 'learning_rate': 0.0004494994733795631, 'batch_size': 183, 'step_size': 11, 'gamma': 0.8689036058807602}. Best is trial 13 with value: 0.05798223180075487.[0m
[32m[I 2025-02-06 00:16:46,190][0m Trial 27 finished with value: 0.027881711839952254 and parameters: {'observation_period_num': 7, 'train_rates': 0.9255758667521732, 'learning_rate': 0.00012144616340285131, 'batch_size': 34, 'step_size': 3, 'gamma': 0.9601827262900791}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:19:12,297][0m Trial 28 finished with value: 0.06774628116953664 and parameters: {'observation_period_num': 19, 'train_rates': 0.9164307695822049, 'learning_rate': 5.8271386051569874e-05, 'batch_size': 39, 'step_size': 3, 'gamma': 0.7917642985205278}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:22:45,469][0m Trial 29 finished with value: 0.06036915679772695 and parameters: {'observation_period_num': 37, 'train_rates': 0.9898680817782273, 'learning_rate': 0.00022583789277251181, 'batch_size': 28, 'step_size': 2, 'gamma': 0.7944209514893305}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:24:32,416][0m Trial 30 finished with value: 0.04774214987590061 and parameters: {'observation_period_num': 11, 'train_rates': 0.958230193300804, 'learning_rate': 0.0003154182059792523, 'batch_size': 56, 'step_size': 2, 'gamma': 0.7889194240291059}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:27:19,679][0m Trial 31 finished with value: 0.05929632424416705 and parameters: {'observation_period_num': 34, 'train_rates': 0.9601847135641283, 'learning_rate': 0.00032028757442420427, 'batch_size': 35, 'step_size': 2, 'gamma': 0.7906759603022637}. Best is trial 27 with value: 0.027881711839952254.[0m
Early stopping at epoch 52
[32m[I 2025-02-06 00:28:16,975][0m Trial 32 finished with value: 0.0784006469672726 and parameters: {'observation_period_num': 10, 'train_rates': 0.9579497089730928, 'learning_rate': 0.00036744046721344867, 'batch_size': 56, 'step_size': 1, 'gamma': 0.7564838656067631}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:31:10,825][0m Trial 33 finished with value: 0.04306805975221355 and parameters: {'observation_period_num': 33, 'train_rates': 0.9603022802029443, 'learning_rate': 0.0006744714916347875, 'batch_size': 34, 'step_size': 2, 'gamma': 0.828305096117592}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:32:43,365][0m Trial 34 finished with value: 0.2538348616690965 and parameters: {'observation_period_num': 20, 'train_rates': 0.8865473386372668, 'learning_rate': 4.84323919260147e-06, 'batch_size': 61, 'step_size': 5, 'gamma': 0.83081143613186}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:33:54,524][0m Trial 35 finished with value: 0.035585765835157664 and parameters: {'observation_period_num': 6, 'train_rates': 0.9273482322700448, 'learning_rate': 0.0006677650208244041, 'batch_size': 85, 'step_size': 2, 'gamma': 0.8802583279990267}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:34:59,904][0m Trial 36 finished with value: 0.036678174727906784 and parameters: {'observation_period_num': 5, 'train_rates': 0.9188591838760218, 'learning_rate': 0.0006644307584780005, 'batch_size': 91, 'step_size': 2, 'gamma': 0.8837894262148915}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:35:43,874][0m Trial 37 finished with value: 0.06420349121470464 and parameters: {'observation_period_num': 29, 'train_rates': 0.8785617220278324, 'learning_rate': 0.0006097875238279197, 'batch_size': 133, 'step_size': 1, 'gamma': 0.8852367673298899}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:36:43,722][0m Trial 38 finished with value: 0.16214737320156605 and parameters: {'observation_period_num': 5, 'train_rates': 0.7743438852847483, 'learning_rate': 0.0006878581160215174, 'batch_size': 91, 'step_size': 2, 'gamma': 0.918733801941752}. Best is trial 27 with value: 0.027881711839952254.[0m
Early stopping at epoch 93
[32m[I 2025-02-06 00:37:52,828][0m Trial 39 finished with value: 0.05916242720937826 and parameters: {'observation_period_num': 38, 'train_rates': 0.915758820761647, 'learning_rate': 0.0007442797330247292, 'batch_size': 79, 'step_size': 1, 'gamma': 0.855135280836664}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:38:37,879][0m Trial 40 finished with value: 0.046556404619654525 and parameters: {'observation_period_num': 49, 'train_rates': 0.8642952637311685, 'learning_rate': 0.0004875298312236359, 'batch_size': 127, 'step_size': 5, 'gamma': 0.8803703188264846}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:39:26,332][0m Trial 41 finished with value: 0.05502706548304257 and parameters: {'observation_period_num': 46, 'train_rates': 0.8967924987968462, 'learning_rate': 0.0005011528813516506, 'batch_size': 122, 'step_size': 5, 'gamma': 0.8859102746315235}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:40:06,621][0m Trial 42 finished with value: 0.04202048107981682 and parameters: {'observation_period_num': 23, 'train_rates': 0.8611521636600936, 'learning_rate': 0.0009541067098146655, 'batch_size': 148, 'step_size': 3, 'gamma': 0.8771048294723487}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:40:45,045][0m Trial 43 finished with value: 0.04284126361788705 and parameters: {'observation_period_num': 24, 'train_rates': 0.9267029460500207, 'learning_rate': 0.0008741737896949542, 'batch_size': 167, 'step_size': 3, 'gamma': 0.8510925178241642}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:41:24,255][0m Trial 44 finished with value: 0.03933939946295038 and parameters: {'observation_period_num': 18, 'train_rates': 0.8317871322854498, 'learning_rate': 0.0007637143161689289, 'batch_size': 151, 'step_size': 3, 'gamma': 0.853515458152094}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:42:00,874][0m Trial 45 finished with value: 0.17635378013561148 and parameters: {'observation_period_num': 252, 'train_rates': 0.833049791057559, 'learning_rate': 0.0009536299436423747, 'batch_size': 146, 'step_size': 4, 'gamma': 0.9189073335546347}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:42:37,303][0m Trial 46 finished with value: 0.03810871339839837 and parameters: {'observation_period_num': 5, 'train_rates': 0.8417276818763524, 'learning_rate': 0.0003676144084008695, 'batch_size': 156, 'step_size': 6, 'gamma': 0.8630838218310835}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:43:11,928][0m Trial 47 finished with value: 0.1725075434024338 and parameters: {'observation_period_num': 12, 'train_rates': 0.7917387927181777, 'learning_rate': 0.0003578911720290599, 'batch_size': 162, 'step_size': 6, 'gamma': 0.8644840417044763}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:43:41,160][0m Trial 48 finished with value: 0.2572389636514646 and parameters: {'observation_period_num': 173, 'train_rates': 0.7330910827709306, 'learning_rate': 0.0002475196784485272, 'batch_size': 184, 'step_size': 4, 'gamma': 0.8937145739649806}. Best is trial 27 with value: 0.027881711839952254.[0m
[32m[I 2025-02-06 00:44:33,870][0m Trial 49 finished with value: 0.1714890444929191 and parameters: {'observation_period_num': 207, 'train_rates': 0.8366100961507519, 'learning_rate': 0.00041556203595922254, 'batch_size': 102, 'step_size': 7, 'gamma': 0.9577747000735732}. Best is trial 27 with value: 0.027881711839952254.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-06 00:44:33,882][0m A new study created in memory with name: no-name-4f26f186-1c3e-4036-8bb1-15121c6882b7[0m
[32m[I 2025-02-06 00:46:35,816][0m Trial 0 finished with value: 0.40637226680099775 and parameters: {'observation_period_num': 154, 'train_rates': 0.8870796699963175, 'learning_rate': 1.459346106458826e-06, 'batch_size': 44, 'step_size': 13, 'gamma': 0.8760366343318154}. Best is trial 0 with value: 0.40637226680099775.[0m
[32m[I 2025-02-06 00:46:57,858][0m Trial 1 finished with value: 0.5012883411513435 and parameters: {'observation_period_num': 197, 'train_rates': 0.7975925778611662, 'learning_rate': 1.440791089111894e-05, 'batch_size': 248, 'step_size': 12, 'gamma': 0.8085379631355164}. Best is trial 0 with value: 0.40637226680099775.[0m
[32m[I 2025-02-06 00:47:29,888][0m Trial 2 finished with value: 0.2635884020098034 and parameters: {'observation_period_num': 166, 'train_rates': 0.8275208302355709, 'learning_rate': 0.00016158620284333107, 'batch_size': 173, 'step_size': 3, 'gamma': 0.7563352676238482}. Best is trial 2 with value: 0.2635884020098034.[0m
[32m[I 2025-02-06 00:47:54,513][0m Trial 3 finished with value: 0.3532027304172516 and parameters: {'observation_period_num': 150, 'train_rates': 0.9562693615167214, 'learning_rate': 2.2913608464931542e-05, 'batch_size': 255, 'step_size': 9, 'gamma': 0.8977753409920464}. Best is trial 2 with value: 0.2635884020098034.[0m
[32m[I 2025-02-06 00:48:16,791][0m Trial 4 finished with value: 0.48230000189792965 and parameters: {'observation_period_num': 220, 'train_rates': 0.7630570999109556, 'learning_rate': 1.0811083976245864e-05, 'batch_size': 250, 'step_size': 14, 'gamma': 0.9731270057040187}. Best is trial 2 with value: 0.2635884020098034.[0m
[32m[I 2025-02-06 00:48:47,510][0m Trial 5 finished with value: 0.2995516342612413 and parameters: {'observation_period_num': 184, 'train_rates': 0.7373609980679711, 'learning_rate': 7.682083205137111e-05, 'batch_size': 157, 'step_size': 13, 'gamma': 0.837345408126222}. Best is trial 2 with value: 0.2635884020098034.[0m
[32m[I 2025-02-06 00:49:15,861][0m Trial 6 finished with value: 0.2535321020686919 and parameters: {'observation_period_num': 60, 'train_rates': 0.7747468657323967, 'learning_rate': 4.371109019928107e-05, 'batch_size': 203, 'step_size': 12, 'gamma': 0.7507641672486296}. Best is trial 6 with value: 0.2535321020686919.[0m
[32m[I 2025-02-06 00:50:01,107][0m Trial 7 finished with value: 0.1570250689983368 and parameters: {'observation_period_num': 137, 'train_rates': 0.9638574655881138, 'learning_rate': 0.00024704303711999654, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8843136674262386}. Best is trial 7 with value: 0.1570250689983368.[0m
[32m[I 2025-02-06 00:50:31,596][0m Trial 8 finished with value: 0.1289673932828009 and parameters: {'observation_period_num': 57, 'train_rates': 0.8343877280911879, 'learning_rate': 3.490934996434796e-05, 'batch_size': 194, 'step_size': 14, 'gamma': 0.7779414630492556}. Best is trial 8 with value: 0.1289673932828009.[0m
[32m[I 2025-02-06 00:51:26,587][0m Trial 9 finished with value: 0.8505189556101854 and parameters: {'observation_period_num': 130, 'train_rates': 0.7970847116686456, 'learning_rate': 6.053373367817679e-06, 'batch_size': 96, 'step_size': 4, 'gamma': 0.7549627461778334}. Best is trial 8 with value: 0.1289673932828009.[0m
[32m[I 2025-02-06 00:52:17,046][0m Trial 10 finished with value: 0.11283964557307107 and parameters: {'observation_period_num': 7, 'train_rates': 0.6175818658756671, 'learning_rate': 0.0009785372282966496, 'batch_size': 92, 'step_size': 6, 'gamma': 0.9455112342371688}. Best is trial 10 with value: 0.11283964557307107.[0m
[32m[I 2025-02-06 00:53:11,127][0m Trial 11 finished with value: 0.12986546111451747 and parameters: {'observation_period_num': 6, 'train_rates': 0.6117004161364603, 'learning_rate': 0.0008144573546466209, 'batch_size': 85, 'step_size': 6, 'gamma': 0.9543664636383555}. Best is trial 10 with value: 0.11283964557307107.[0m
[32m[I 2025-02-06 00:57:02,249][0m Trial 12 finished with value: 0.19913584227507242 and parameters: {'observation_period_num': 68, 'train_rates': 0.6412644735464035, 'learning_rate': 0.000724652071989393, 'batch_size': 19, 'step_size': 6, 'gamma': 0.9354188775898648}. Best is trial 10 with value: 0.11283964557307107.[0m
Early stopping at epoch 58
[32m[I 2025-02-06 00:57:30,772][0m Trial 13 finished with value: 0.8643531026421013 and parameters: {'observation_period_num': 21, 'train_rates': 0.6899692162917852, 'learning_rate': 1.2205840583865956e-06, 'batch_size': 103, 'step_size': 1, 'gamma': 0.8336482140735498}. Best is trial 10 with value: 0.11283964557307107.[0m
[32m[I 2025-02-06 00:58:00,808][0m Trial 14 finished with value: 0.44000046292758915 and parameters: {'observation_period_num': 79, 'train_rates': 0.8482752094993768, 'learning_rate': 4.310788837551412e-06, 'batch_size': 201, 'step_size': 7, 'gamma': 0.9132042967165328}. Best is trial 10 with value: 0.11283964557307107.[0m
[32m[I 2025-02-06 00:58:39,239][0m Trial 15 finished with value: 0.15384858689634254 and parameters: {'observation_period_num': 43, 'train_rates': 0.6934716487010907, 'learning_rate': 0.00033892873647026753, 'batch_size': 128, 'step_size': 15, 'gamma': 0.8052323758308657}. Best is trial 10 with value: 0.11283964557307107.[0m
[32m[I 2025-02-06 00:59:56,144][0m Trial 16 finished with value: 0.08090815724401207 and parameters: {'observation_period_num': 94, 'train_rates': 0.900143572126328, 'learning_rate': 6.63907840455503e-05, 'batch_size': 72, 'step_size': 10, 'gamma': 0.9269235745081383}. Best is trial 16 with value: 0.08090815724401207.[0m
[32m[I 2025-02-06 01:01:29,994][0m Trial 17 finished with value: 0.1514947820495773 and parameters: {'observation_period_num': 104, 'train_rates': 0.9044477283630464, 'learning_rate': 0.00012309047748439863, 'batch_size': 60, 'step_size': 10, 'gamma': 0.9848106757639181}. Best is trial 16 with value: 0.08090815724401207.[0m
[32m[I 2025-02-06 01:03:00,243][0m Trial 18 finished with value: 0.1423761508482344 and parameters: {'observation_period_num': 97, 'train_rates': 0.9109248194750725, 'learning_rate': 0.00041342808159350135, 'batch_size': 63, 'step_size': 8, 'gamma': 0.927901576004443}. Best is trial 16 with value: 0.08090815724401207.[0m
[32m[I 2025-02-06 01:05:16,817][0m Trial 19 finished with value: 0.15194114872317607 and parameters: {'observation_period_num': 36, 'train_rates': 0.6868141598110971, 'learning_rate': 6.389895293586462e-05, 'batch_size': 34, 'step_size': 4, 'gamma': 0.9555400684121395}. Best is trial 16 with value: 0.08090815724401207.[0m
[32m[I 2025-02-06 01:06:34,192][0m Trial 20 finished with value: 0.1878652274608612 and parameters: {'observation_period_num': 244, 'train_rates': 0.9842407066704904, 'learning_rate': 0.0001030915815560575, 'batch_size': 76, 'step_size': 10, 'gamma': 0.8492664530781616}. Best is trial 16 with value: 0.08090815724401207.[0m
[32m[I 2025-02-06 01:07:23,630][0m Trial 21 finished with value: 0.10616380068959232 and parameters: {'observation_period_num': 104, 'train_rates': 0.8535264932829157, 'learning_rate': 3.733777728839142e-05, 'batch_size': 112, 'step_size': 15, 'gamma': 0.9193198294087476}. Best is trial 16 with value: 0.08090815724401207.[0m
[32m[I 2025-02-06 01:08:13,098][0m Trial 22 finished with value: 0.18862719025165253 and parameters: {'observation_period_num': 107, 'train_rates': 0.8727608983075332, 'learning_rate': 1.986919254933622e-05, 'batch_size': 116, 'step_size': 6, 'gamma': 0.9130823324620938}. Best is trial 16 with value: 0.08090815724401207.[0m
[32m[I 2025-02-06 01:09:08,566][0m Trial 23 finished with value: 0.2952737130023338 and parameters: {'observation_period_num': 91, 'train_rates': 0.9356791910427438, 'learning_rate': 8.461943748847497e-06, 'batch_size': 109, 'step_size': 11, 'gamma': 0.9392794206802787}. Best is trial 16 with value: 0.08090815724401207.[0m
[32m[I 2025-02-06 01:09:49,007][0m Trial 24 finished with value: 0.5907363221447736 and parameters: {'observation_period_num': 120, 'train_rates': 0.8681585404267254, 'learning_rate': 3.7319538968218016e-06, 'batch_size': 149, 'step_size': 8, 'gamma': 0.9014724115297007}. Best is trial 16 with value: 0.08090815724401207.[0m
[32m[I 2025-02-06 01:11:06,343][0m Trial 25 finished with value: 0.04807706794235855 and parameters: {'observation_period_num': 7, 'train_rates': 0.924108035570424, 'learning_rate': 4.98857452554627e-05, 'batch_size': 79, 'step_size': 5, 'gamma': 0.9534123969360789}. Best is trial 25 with value: 0.04807706794235855.[0m
[32m[I 2025-02-06 01:12:34,856][0m Trial 26 finished with value: 0.07035747928415785 and parameters: {'observation_period_num': 35, 'train_rates': 0.9311003012126162, 'learning_rate': 5.125653188354658e-05, 'batch_size': 67, 'step_size': 1, 'gamma': 0.9645034700663947}. Best is trial 25 with value: 0.04807706794235855.[0m
[32m[I 2025-02-06 01:14:10,954][0m Trial 27 finished with value: 0.06492958058986593 and parameters: {'observation_period_num': 39, 'train_rates': 0.9300723457605605, 'learning_rate': 5.840922840303434e-05, 'batch_size': 63, 'step_size': 1, 'gamma': 0.968811345949367}. Best is trial 25 with value: 0.04807706794235855.[0m
[32m[I 2025-02-06 01:16:13,144][0m Trial 28 finished with value: 0.049524714797735216 and parameters: {'observation_period_num': 31, 'train_rates': 0.9455261791473606, 'learning_rate': 0.00015586782334701529, 'batch_size': 48, 'step_size': 1, 'gamma': 0.9677432521426634}. Best is trial 25 with value: 0.04807706794235855.[0m
[32m[I 2025-02-06 01:18:25,804][0m Trial 29 finished with value: 0.03910077401601216 and parameters: {'observation_period_num': 21, 'train_rates': 0.9769115491322056, 'learning_rate': 0.0001903625759313093, 'batch_size': 45, 'step_size': 2, 'gamma': 0.9890218273438722}. Best is trial 29 with value: 0.03910077401601216.[0m
[32m[I 2025-02-06 01:21:24,709][0m Trial 30 finished with value: 0.03761809123189826 and parameters: {'observation_period_num': 21, 'train_rates': 0.9870445986409888, 'learning_rate': 0.0002117174982429822, 'batch_size': 34, 'step_size': 3, 'gamma': 0.9884566337373585}. Best is trial 30 with value: 0.03761809123189826.[0m
[32m[I 2025-02-06 01:23:46,866][0m Trial 31 finished with value: 0.049335695803165436 and parameters: {'observation_period_num': 21, 'train_rates': 0.9863869043142699, 'learning_rate': 0.0001878809676164571, 'batch_size': 43, 'step_size': 3, 'gamma': 0.9855060852975576}. Best is trial 30 with value: 0.03761809123189826.[0m
[32m[I 2025-02-06 01:28:59,801][0m Trial 32 finished with value: 0.03671347004744936 and parameters: {'observation_period_num': 19, 'train_rates': 0.9887115691252297, 'learning_rate': 0.0002679495504551159, 'batch_size': 19, 'step_size': 3, 'gamma': 0.984387538375902}. Best is trial 32 with value: 0.03671347004744936.[0m
[32m[I 2025-02-06 01:35:13,534][0m Trial 33 finished with value: 0.0361125137243006 and parameters: {'observation_period_num': 18, 'train_rates': 0.9848453067038433, 'learning_rate': 0.0004657100769993278, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9834035982917502}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 01:41:20,064][0m Trial 34 finished with value: 0.10430400909447088 and parameters: {'observation_period_num': 52, 'train_rates': 0.9718485912463195, 'learning_rate': 0.0005554049974588084, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9856728399865272}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 01:44:29,776][0m Trial 35 finished with value: 0.04458097416238907 and parameters: {'observation_period_num': 23, 'train_rates': 0.9602999537537189, 'learning_rate': 0.00027151443854139985, 'batch_size': 31, 'step_size': 2, 'gamma': 0.9887470534557425}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 01:47:48,743][0m Trial 36 finished with value: 0.12381698191165924 and parameters: {'observation_period_num': 73, 'train_rates': 0.9884039758111435, 'learning_rate': 0.0004913355514015922, 'batch_size': 30, 'step_size': 2, 'gamma': 0.9764953043125878}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 01:49:49,090][0m Trial 37 finished with value: 0.07368960121975225 and parameters: {'observation_period_num': 47, 'train_rates': 0.9591199174723926, 'learning_rate': 0.00020223039949514838, 'batch_size': 49, 'step_size': 4, 'gamma': 0.9608568951450216}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 01:53:50,123][0m Trial 38 finished with value: 0.043820846052979354 and parameters: {'observation_period_num': 19, 'train_rates': 0.8859748941822111, 'learning_rate': 0.00011079438358793299, 'batch_size': 23, 'step_size': 2, 'gamma': 0.9760552289291291}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 01:56:12,410][0m Trial 39 finished with value: 0.15522743147962234 and parameters: {'observation_period_num': 172, 'train_rates': 0.9695863037747706, 'learning_rate': 0.00032062308309446655, 'batch_size': 40, 'step_size': 5, 'gamma': 0.8701519920107296}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 01:58:08,792][0m Trial 40 finished with value: 0.12412865372265086 and parameters: {'observation_period_num': 81, 'train_rates': 0.9469557916773321, 'learning_rate': 0.0005982720159450618, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9434569176343732}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 02:02:02,151][0m Trial 41 finished with value: 0.04725039861303695 and parameters: {'observation_period_num': 15, 'train_rates': 0.8882187961741647, 'learning_rate': 9.967043166903344e-05, 'batch_size': 24, 'step_size': 4, 'gamma': 0.9748839929319708}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 02:08:14,334][0m Trial 42 finished with value: 0.06165574843946256 and parameters: {'observation_period_num': 27, 'train_rates': 0.9870856334422715, 'learning_rate': 0.00014411337364532152, 'batch_size': 16, 'step_size': 2, 'gamma': 0.989616409854684}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 02:11:10,881][0m Trial 43 finished with value: 0.09009716092121034 and parameters: {'observation_period_num': 63, 'train_rates': 0.9204021787348792, 'learning_rate': 0.00023086693257044956, 'batch_size': 32, 'step_size': 2, 'gamma': 0.972761921988284}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 02:15:04,864][0m Trial 44 finished with value: 0.04575475273457797 and parameters: {'observation_period_num': 13, 'train_rates': 0.9484781885569742, 'learning_rate': 0.0004073673986943565, 'batch_size': 25, 'step_size': 3, 'gamma': 0.9502620245776714}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 02:15:34,201][0m Trial 45 finished with value: 0.12411905825138092 and parameters: {'observation_period_num': 53, 'train_rates': 0.9681137106246707, 'learning_rate': 8.906024578426672e-05, 'batch_size': 227, 'step_size': 5, 'gamma': 0.9750560326609669}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 02:17:10,444][0m Trial 46 finished with value: 0.16042198848060774 and parameters: {'observation_period_num': 205, 'train_rates': 0.8171889256963482, 'learning_rate': 0.00028400379575078987, 'batch_size': 53, 'step_size': 2, 'gamma': 0.9787729287036909}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 02:19:51,209][0m Trial 47 finished with value: 0.03971135653555393 and parameters: {'observation_period_num': 5, 'train_rates': 0.9728737767724637, 'learning_rate': 2.6027551998855967e-05, 'batch_size': 38, 'step_size': 4, 'gamma': 0.9591044474347453}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 02:22:25,387][0m Trial 48 finished with value: 0.04975308105349541 and parameters: {'observation_period_num': 30, 'train_rates': 0.9734097529661606, 'learning_rate': 2.499696103082396e-05, 'batch_size': 39, 'step_size': 4, 'gamma': 0.9603078855704533}. Best is trial 33 with value: 0.0361125137243006.[0m
[32m[I 2025-02-06 02:23:35,422][0m Trial 49 finished with value: 0.15387937702514506 and parameters: {'observation_period_num': 5, 'train_rates': 0.9451406273382577, 'learning_rate': 1.148374867725432e-05, 'batch_size': 87, 'step_size': 3, 'gamma': 0.8821365478186801}. Best is trial 33 with value: 0.0361125137243006.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 12, 'train_rates': 0.8963061379821241, 'learning_rate': 0.00013662188894747836, 'batch_size': 41, 'step_size': 8, 'gamma': 0.8514930351652281}
Epoch 1/300, trend Loss: 0.2907 | 0.2200
Epoch 2/300, trend Loss: 0.1443 | 0.1423
Epoch 3/300, trend Loss: 0.1226 | 0.1091
Epoch 4/300, trend Loss: 0.1130 | 0.0933
Epoch 5/300, trend Loss: 0.1091 | 0.0858
Epoch 6/300, trend Loss: 0.1071 | 0.0792
Epoch 7/300, trend Loss: 0.1044 | 0.0741
Epoch 8/300, trend Loss: 0.1016 | 0.0701
Epoch 9/300, trend Loss: 0.0986 | 0.0751
Epoch 10/300, trend Loss: 0.0954 | 0.0696
Epoch 11/300, trend Loss: 0.0941 | 0.0671
Epoch 12/300, trend Loss: 0.0932 | 0.0655
Epoch 13/300, trend Loss: 0.0921 | 0.0655
Epoch 14/300, trend Loss: 0.0895 | 0.0628
Epoch 15/300, trend Loss: 0.0910 | 0.0604
Epoch 16/300, trend Loss: 0.0901 | 0.0565
Epoch 17/300, trend Loss: 0.0877 | 0.0537
Epoch 18/300, trend Loss: 0.0836 | 0.0535
Epoch 19/300, trend Loss: 0.0805 | 0.0537
Epoch 20/300, trend Loss: 0.0788 | 0.0541
Epoch 21/300, trend Loss: 0.0806 | 0.0747
Epoch 22/300, trend Loss: 0.0806 | 0.0625
Epoch 23/300, trend Loss: 0.0791 | 0.0588
Epoch 24/300, trend Loss: 0.0778 | 0.0545
Epoch 25/300, trend Loss: 0.0768 | 0.0522
Epoch 26/300, trend Loss: 0.0759 | 0.0495
Epoch 27/300, trend Loss: 0.0751 | 0.0475
Epoch 28/300, trend Loss: 0.0745 | 0.0460
Epoch 29/300, trend Loss: 0.0739 | 0.0450
Epoch 30/300, trend Loss: 0.0733 | 0.0440
Epoch 31/300, trend Loss: 0.0729 | 0.0432
Epoch 32/300, trend Loss: 0.0724 | 0.0424
Epoch 33/300, trend Loss: 0.0720 | 0.0420
Epoch 34/300, trend Loss: 0.0716 | 0.0414
Epoch 35/300, trend Loss: 0.0712 | 0.0409
Epoch 36/300, trend Loss: 0.0709 | 0.0405
Epoch 37/300, trend Loss: 0.0706 | 0.0401
Epoch 38/300, trend Loss: 0.0702 | 0.0396
Epoch 39/300, trend Loss: 0.0698 | 0.0393
Epoch 40/300, trend Loss: 0.0695 | 0.0389
Epoch 41/300, trend Loss: 0.0692 | 0.0388
Epoch 42/300, trend Loss: 0.0689 | 0.0385
Epoch 43/300, trend Loss: 0.0685 | 0.0382
Epoch 44/300, trend Loss: 0.0682 | 0.0379
Epoch 45/300, trend Loss: 0.0679 | 0.0381
Epoch 46/300, trend Loss: 0.0677 | 0.0378
Epoch 47/300, trend Loss: 0.0675 | 0.0376
Epoch 48/300, trend Loss: 0.0673 | 0.0374
Epoch 49/300, trend Loss: 0.0671 | 0.0373
Epoch 50/300, trend Loss: 0.0670 | 0.0371
Epoch 51/300, trend Loss: 0.0669 | 0.0369
Epoch 52/300, trend Loss: 0.0668 | 0.0368
Epoch 53/300, trend Loss: 0.0668 | 0.0361
Epoch 54/300, trend Loss: 0.0669 | 0.0358
Epoch 55/300, trend Loss: 0.0668 | 0.0357
Epoch 56/300, trend Loss: 0.0668 | 0.0356
Epoch 57/300, trend Loss: 0.0669 | 0.0345
Epoch 58/300, trend Loss: 0.0669 | 0.0343
Epoch 59/300, trend Loss: 0.0667 | 0.0342
Epoch 60/300, trend Loss: 0.0666 | 0.0343
Epoch 61/300, trend Loss: 0.0667 | 0.0335
Epoch 62/300, trend Loss: 0.0666 | 0.0334
Epoch 63/300, trend Loss: 0.0664 | 0.0334
Epoch 64/300, trend Loss: 0.0662 | 0.0334
Epoch 65/300, trend Loss: 0.0662 | 0.0333
Epoch 66/300, trend Loss: 0.0661 | 0.0332
Epoch 67/300, trend Loss: 0.0659 | 0.0332
Epoch 68/300, trend Loss: 0.0657 | 0.0331
Epoch 69/300, trend Loss: 0.0656 | 0.0334
Epoch 70/300, trend Loss: 0.0655 | 0.0333
Epoch 71/300, trend Loss: 0.0652 | 0.0332
Epoch 72/300, trend Loss: 0.0650 | 0.0331
Epoch 73/300, trend Loss: 0.0648 | 0.0334
Epoch 74/300, trend Loss: 0.0647 | 0.0334
Epoch 75/300, trend Loss: 0.0646 | 0.0333
Epoch 76/300, trend Loss: 0.0645 | 0.0333
Epoch 77/300, trend Loss: 0.0644 | 0.0335
Epoch 78/300, trend Loss: 0.0643 | 0.0336
Epoch 79/300, trend Loss: 0.0643 | 0.0335
Epoch 80/300, trend Loss: 0.0642 | 0.0334
Epoch 81/300, trend Loss: 0.0642 | 0.0337
Epoch 82/300, trend Loss: 0.0641 | 0.0336
Epoch 83/300, trend Loss: 0.0640 | 0.0335
Epoch 84/300, trend Loss: 0.0640 | 0.0334
Epoch 85/300, trend Loss: 0.0639 | 0.0337
Epoch 86/300, trend Loss: 0.0639 | 0.0336
Epoch 87/300, trend Loss: 0.0638 | 0.0335
Epoch 88/300, trend Loss: 0.0638 | 0.0334
Epoch 89/300, trend Loss: 0.0637 | 0.0337
Epoch 90/300, trend Loss: 0.0637 | 0.0336
Epoch 91/300, trend Loss: 0.0636 | 0.0336
Epoch 92/300, trend Loss: 0.0636 | 0.0336
Epoch 93/300, trend Loss: 0.0635 | 0.0338
Epoch 94/300, trend Loss: 0.0635 | 0.0337
Epoch 95/300, trend Loss: 0.0634 | 0.0337
Epoch 96/300, trend Loss: 0.0634 | 0.0337
Epoch 97/300, trend Loss: 0.0634 | 0.0336
Epoch 98/300, trend Loss: 0.0633 | 0.0336
Epoch 99/300, trend Loss: 0.0633 | 0.0335
Epoch 100/300, trend Loss: 0.0633 | 0.0334
Epoch 101/300, trend Loss: 0.0633 | 0.0333
Epoch 102/300, trend Loss: 0.0633 | 0.0333
Epoch 103/300, trend Loss: 0.0633 | 0.0332
Epoch 104/300, trend Loss: 0.0632 | 0.0331
Epoch 105/300, trend Loss: 0.0632 | 0.0331
Epoch 106/300, trend Loss: 0.0631 | 0.0330
Epoch 107/300, trend Loss: 0.0630 | 0.0329
Epoch 108/300, trend Loss: 0.0630 | 0.0329
Epoch 109/300, trend Loss: 0.0629 | 0.0328
Epoch 110/300, trend Loss: 0.0629 | 0.0328
Epoch 111/300, trend Loss: 0.0628 | 0.0328
Epoch 112/300, trend Loss: 0.0628 | 0.0328
Epoch 113/300, trend Loss: 0.0628 | 0.0328
Epoch 114/300, trend Loss: 0.0627 | 0.0327
Epoch 115/300, trend Loss: 0.0627 | 0.0327
Epoch 116/300, trend Loss: 0.0627 | 0.0327
Epoch 117/300, trend Loss: 0.0626 | 0.0327
Epoch 118/300, trend Loss: 0.0626 | 0.0327
Epoch 119/300, trend Loss: 0.0626 | 0.0327
Epoch 120/300, trend Loss: 0.0626 | 0.0327
Epoch 121/300, trend Loss: 0.0626 | 0.0327
Epoch 122/300, trend Loss: 0.0625 | 0.0327
Epoch 123/300, trend Loss: 0.0625 | 0.0327
Epoch 124/300, trend Loss: 0.0625 | 0.0326
Epoch 125/300, trend Loss: 0.0625 | 0.0327
Epoch 126/300, trend Loss: 0.0625 | 0.0326
Epoch 127/300, trend Loss: 0.0625 | 0.0326
Epoch 128/300, trend Loss: 0.0625 | 0.0326
Epoch 129/300, trend Loss: 0.0624 | 0.0326
Epoch 130/300, trend Loss: 0.0624 | 0.0326
Epoch 131/300, trend Loss: 0.0624 | 0.0326
Epoch 132/300, trend Loss: 0.0624 | 0.0326
Epoch 133/300, trend Loss: 0.0624 | 0.0326
Epoch 134/300, trend Loss: 0.0624 | 0.0326
Epoch 135/300, trend Loss: 0.0624 | 0.0326
Epoch 136/300, trend Loss: 0.0624 | 0.0326
Epoch 137/300, trend Loss: 0.0624 | 0.0326
Epoch 138/300, trend Loss: 0.0624 | 0.0326
Epoch 139/300, trend Loss: 0.0624 | 0.0326
Epoch 140/300, trend Loss: 0.0624 | 0.0325
Epoch 141/300, trend Loss: 0.0623 | 0.0325
Epoch 142/300, trend Loss: 0.0623 | 0.0325
Epoch 143/300, trend Loss: 0.0623 | 0.0325
Epoch 144/300, trend Loss: 0.0623 | 0.0325
Epoch 145/300, trend Loss: 0.0623 | 0.0325
Epoch 146/300, trend Loss: 0.0623 | 0.0325
Epoch 147/300, trend Loss: 0.0623 | 0.0325
Epoch 148/300, trend Loss: 0.0623 | 0.0325
Epoch 149/300, trend Loss: 0.0623 | 0.0325
Epoch 150/300, trend Loss: 0.0623 | 0.0325
Epoch 151/300, trend Loss: 0.0623 | 0.0325
Epoch 152/300, trend Loss: 0.0623 | 0.0325
Epoch 153/300, trend Loss: 0.0623 | 0.0325
Epoch 154/300, trend Loss: 0.0623 | 0.0325
Epoch 155/300, trend Loss: 0.0623 | 0.0325
Epoch 156/300, trend Loss: 0.0623 | 0.0325
Epoch 157/300, trend Loss: 0.0623 | 0.0325
Epoch 158/300, trend Loss: 0.0623 | 0.0325
Epoch 159/300, trend Loss: 0.0623 | 0.0325
Epoch 160/300, trend Loss: 0.0623 | 0.0325
Epoch 161/300, trend Loss: 0.0623 | 0.0325
Epoch 162/300, trend Loss: 0.0622 | 0.0325
Epoch 163/300, trend Loss: 0.0622 | 0.0325
Epoch 164/300, trend Loss: 0.0622 | 0.0325
Epoch 165/300, trend Loss: 0.0622 | 0.0325
Epoch 166/300, trend Loss: 0.0622 | 0.0325
Epoch 167/300, trend Loss: 0.0622 | 0.0325
Epoch 168/300, trend Loss: 0.0622 | 0.0325
Epoch 169/300, trend Loss: 0.0622 | 0.0325
Epoch 170/300, trend Loss: 0.0622 | 0.0325
Epoch 171/300, trend Loss: 0.0622 | 0.0325
Epoch 172/300, trend Loss: 0.0622 | 0.0325
Epoch 173/300, trend Loss: 0.0622 | 0.0325
Epoch 174/300, trend Loss: 0.0622 | 0.0325
Epoch 175/300, trend Loss: 0.0622 | 0.0325
Epoch 176/300, trend Loss: 0.0622 | 0.0324
Epoch 177/300, trend Loss: 0.0622 | 0.0324
Epoch 178/300, trend Loss: 0.0622 | 0.0324
Epoch 179/300, trend Loss: 0.0622 | 0.0324
Epoch 180/300, trend Loss: 0.0622 | 0.0324
Epoch 181/300, trend Loss: 0.0622 | 0.0324
Epoch 182/300, trend Loss: 0.0622 | 0.0324
Epoch 183/300, trend Loss: 0.0622 | 0.0324
Epoch 184/300, trend Loss: 0.0622 | 0.0324
Epoch 185/300, trend Loss: 0.0622 | 0.0324
Epoch 186/300, trend Loss: 0.0622 | 0.0324
Epoch 187/300, trend Loss: 0.0622 | 0.0324
Epoch 188/300, trend Loss: 0.0622 | 0.0324
Epoch 189/300, trend Loss: 0.0622 | 0.0324
Epoch 190/300, trend Loss: 0.0622 | 0.0324
Epoch 191/300, trend Loss: 0.0622 | 0.0324
Epoch 192/300, trend Loss: 0.0622 | 0.0324
Epoch 193/300, trend Loss: 0.0622 | 0.0324
Epoch 194/300, trend Loss: 0.0622 | 0.0324
Epoch 195/300, trend Loss: 0.0622 | 0.0324
Epoch 196/300, trend Loss: 0.0622 | 0.0324
Epoch 197/300, trend Loss: 0.0622 | 0.0324
Epoch 198/300, trend Loss: 0.0622 | 0.0324
Epoch 199/300, trend Loss: 0.0622 | 0.0324
Epoch 200/300, trend Loss: 0.0622 | 0.0324
Epoch 201/300, trend Loss: 0.0622 | 0.0324
Epoch 202/300, trend Loss: 0.0622 | 0.0324
Epoch 203/300, trend Loss: 0.0622 | 0.0324
Epoch 204/300, trend Loss: 0.0622 | 0.0324
Epoch 205/300, trend Loss: 0.0622 | 0.0324
Epoch 206/300, trend Loss: 0.0622 | 0.0324
Epoch 207/300, trend Loss: 0.0622 | 0.0324
Epoch 208/300, trend Loss: 0.0622 | 0.0324
Epoch 209/300, trend Loss: 0.0622 | 0.0324
Epoch 210/300, trend Loss: 0.0622 | 0.0324
Epoch 211/300, trend Loss: 0.0622 | 0.0324
Epoch 212/300, trend Loss: 0.0622 | 0.0324
Epoch 213/300, trend Loss: 0.0622 | 0.0324
Epoch 214/300, trend Loss: 0.0622 | 0.0324
Epoch 215/300, trend Loss: 0.0622 | 0.0324
Epoch 216/300, trend Loss: 0.0622 | 0.0324
Epoch 217/300, trend Loss: 0.0622 | 0.0324
Epoch 218/300, trend Loss: 0.0622 | 0.0324
Epoch 219/300, trend Loss: 0.0622 | 0.0324
Epoch 220/300, trend Loss: 0.0622 | 0.0324
Epoch 221/300, trend Loss: 0.0622 | 0.0324
Epoch 222/300, trend Loss: 0.0622 | 0.0324
Epoch 223/300, trend Loss: 0.0622 | 0.0324
Epoch 224/300, trend Loss: 0.0622 | 0.0324
Epoch 225/300, trend Loss: 0.0622 | 0.0324
Epoch 226/300, trend Loss: 0.0622 | 0.0324
Epoch 227/300, trend Loss: 0.0622 | 0.0324
Epoch 228/300, trend Loss: 0.0622 | 0.0324
Epoch 229/300, trend Loss: 0.0622 | 0.0324
Epoch 230/300, trend Loss: 0.0622 | 0.0324
Epoch 231/300, trend Loss: 0.0622 | 0.0324
Epoch 232/300, trend Loss: 0.0622 | 0.0324
Epoch 233/300, trend Loss: 0.0622 | 0.0324
Epoch 234/300, trend Loss: 0.0622 | 0.0324
Epoch 235/300, trend Loss: 0.0622 | 0.0324
Epoch 236/300, trend Loss: 0.0622 | 0.0324
Epoch 237/300, trend Loss: 0.0622 | 0.0324
Epoch 238/300, trend Loss: 0.0622 | 0.0324
Epoch 239/300, trend Loss: 0.0622 | 0.0324
Epoch 240/300, trend Loss: 0.0622 | 0.0324
Epoch 241/300, trend Loss: 0.0622 | 0.0324
Epoch 242/300, trend Loss: 0.0622 | 0.0324
Epoch 243/300, trend Loss: 0.0622 | 0.0324
Epoch 244/300, trend Loss: 0.0622 | 0.0324
Epoch 245/300, trend Loss: 0.0622 | 0.0324
Epoch 246/300, trend Loss: 0.0622 | 0.0324
Epoch 247/300, trend Loss: 0.0622 | 0.0324
Epoch 248/300, trend Loss: 0.0622 | 0.0324
Epoch 249/300, trend Loss: 0.0622 | 0.0324
Epoch 250/300, trend Loss: 0.0622 | 0.0324
Epoch 251/300, trend Loss: 0.0622 | 0.0324
Epoch 252/300, trend Loss: 0.0622 | 0.0324
Epoch 253/300, trend Loss: 0.0622 | 0.0324
Epoch 254/300, trend Loss: 0.0622 | 0.0324
Epoch 255/300, trend Loss: 0.0622 | 0.0324
Epoch 256/300, trend Loss: 0.0622 | 0.0324
Epoch 257/300, trend Loss: 0.0622 | 0.0324
Epoch 258/300, trend Loss: 0.0622 | 0.0324
Epoch 259/300, trend Loss: 0.0622 | 0.0324
Epoch 260/300, trend Loss: 0.0622 | 0.0324
Epoch 261/300, trend Loss: 0.0622 | 0.0324
Epoch 262/300, trend Loss: 0.0622 | 0.0324
Epoch 263/300, trend Loss: 0.0622 | 0.0324
Epoch 264/300, trend Loss: 0.0622 | 0.0324
Epoch 265/300, trend Loss: 0.0622 | 0.0324
Epoch 266/300, trend Loss: 0.0622 | 0.0324
Epoch 267/300, trend Loss: 0.0622 | 0.0324
Epoch 268/300, trend Loss: 0.0622 | 0.0324
Epoch 269/300, trend Loss: 0.0622 | 0.0324
Epoch 270/300, trend Loss: 0.0622 | 0.0324
Epoch 271/300, trend Loss: 0.0622 | 0.0324
Epoch 272/300, trend Loss: 0.0622 | 0.0324
Epoch 273/300, trend Loss: 0.0622 | 0.0324
Epoch 274/300, trend Loss: 0.0622 | 0.0324
Epoch 275/300, trend Loss: 0.0622 | 0.0324
Epoch 276/300, trend Loss: 0.0622 | 0.0324
Epoch 277/300, trend Loss: 0.0622 | 0.0324
Epoch 278/300, trend Loss: 0.0622 | 0.0324
Epoch 279/300, trend Loss: 0.0622 | 0.0324
Epoch 280/300, trend Loss: 0.0622 | 0.0324
Epoch 281/300, trend Loss: 0.0622 | 0.0324
Epoch 282/300, trend Loss: 0.0622 | 0.0324
Epoch 283/300, trend Loss: 0.0622 | 0.0324
Epoch 284/300, trend Loss: 0.0622 | 0.0324
Epoch 285/300, trend Loss: 0.0622 | 0.0324
Epoch 286/300, trend Loss: 0.0622 | 0.0324
Epoch 287/300, trend Loss: 0.0622 | 0.0324
Epoch 288/300, trend Loss: 0.0622 | 0.0324
Epoch 289/300, trend Loss: 0.0622 | 0.0324
Epoch 290/300, trend Loss: 0.0622 | 0.0324
Epoch 291/300, trend Loss: 0.0622 | 0.0324
Epoch 292/300, trend Loss: 0.0622 | 0.0324
Epoch 293/300, trend Loss: 0.0622 | 0.0324
Epoch 294/300, trend Loss: 0.0622 | 0.0324
Epoch 295/300, trend Loss: 0.0622 | 0.0324
Epoch 296/300, trend Loss: 0.0622 | 0.0324
Epoch 297/300, trend Loss: 0.0622 | 0.0324
Epoch 298/300, trend Loss: 0.0622 | 0.0324
Epoch 299/300, trend Loss: 0.0622 | 0.0324
Epoch 300/300, trend Loss: 0.0622 | 0.0324
Training seasonal_0 component with params: {'observation_period_num': 6, 'train_rates': 0.8831338462850308, 'learning_rate': 0.00026296012972942123, 'batch_size': 77, 'step_size': 14, 'gamma': 0.763469451699588}
Epoch 1/300, seasonal_0 Loss: 0.2015 | 0.1329
Epoch 2/300, seasonal_0 Loss: 0.1385 | 0.1022
Epoch 3/300, seasonal_0 Loss: 0.1263 | 0.0943
Epoch 4/300, seasonal_0 Loss: 0.1195 | 0.0978
Epoch 5/300, seasonal_0 Loss: 0.1180 | 0.0922
Epoch 6/300, seasonal_0 Loss: 0.1142 | 0.0880
Epoch 7/300, seasonal_0 Loss: 0.1105 | 0.0816
Epoch 8/300, seasonal_0 Loss: 0.1074 | 0.0733
Epoch 9/300, seasonal_0 Loss: 0.1060 | 0.0718
Epoch 10/300, seasonal_0 Loss: 0.1044 | 0.0708
Epoch 11/300, seasonal_0 Loss: 0.1031 | 0.0684
Epoch 12/300, seasonal_0 Loss: 0.1010 | 0.0649
Epoch 13/300, seasonal_0 Loss: 0.0989 | 0.0610
Epoch 14/300, seasonal_0 Loss: 0.0970 | 0.0578
Epoch 15/300, seasonal_0 Loss: 0.0948 | 0.0573
Epoch 16/300, seasonal_0 Loss: 0.0934 | 0.0599
Epoch 17/300, seasonal_0 Loss: 0.0913 | 0.0661
Epoch 18/300, seasonal_0 Loss: 0.0927 | 0.0710
Epoch 19/300, seasonal_0 Loss: 0.0958 | 0.0724
Epoch 20/300, seasonal_0 Loss: 0.0978 | 0.0626
Epoch 21/300, seasonal_0 Loss: 0.0969 | 0.0587
Epoch 22/300, seasonal_0 Loss: 0.1003 | 0.0571
Epoch 23/300, seasonal_0 Loss: 0.0964 | 0.0595
Epoch 24/300, seasonal_0 Loss: 0.0949 | 0.0679
Epoch 25/300, seasonal_0 Loss: 0.0924 | 0.0646
Epoch 26/300, seasonal_0 Loss: 0.0890 | 0.0598
Epoch 27/300, seasonal_0 Loss: 0.0856 | 0.0564
Epoch 28/300, seasonal_0 Loss: 0.0835 | 0.0550
Epoch 29/300, seasonal_0 Loss: 0.0825 | 0.0528
Epoch 30/300, seasonal_0 Loss: 0.0817 | 0.0529
Epoch 31/300, seasonal_0 Loss: 0.0812 | 0.0529
Epoch 32/300, seasonal_0 Loss: 0.0812 | 0.0526
Epoch 33/300, seasonal_0 Loss: 0.0814 | 0.0524
Epoch 34/300, seasonal_0 Loss: 0.0816 | 0.0522
Epoch 35/300, seasonal_0 Loss: 0.0818 | 0.0523
Epoch 36/300, seasonal_0 Loss: 0.0825 | 0.0574
Epoch 37/300, seasonal_0 Loss: 0.0833 | 0.0587
Epoch 38/300, seasonal_0 Loss: 0.0823 | 0.0588
Epoch 39/300, seasonal_0 Loss: 0.0809 | 0.0568
Epoch 40/300, seasonal_0 Loss: 0.0794 | 0.0543
Epoch 41/300, seasonal_0 Loss: 0.0781 | 0.0520
Epoch 42/300, seasonal_0 Loss: 0.0771 | 0.0502
Epoch 43/300, seasonal_0 Loss: 0.0764 | 0.0489
Epoch 44/300, seasonal_0 Loss: 0.0760 | 0.0472
Epoch 45/300, seasonal_0 Loss: 0.0758 | 0.0466
Epoch 46/300, seasonal_0 Loss: 0.0760 | 0.0462
Epoch 47/300, seasonal_0 Loss: 0.0763 | 0.0459
Epoch 48/300, seasonal_0 Loss: 0.0764 | 0.0456
Epoch 49/300, seasonal_0 Loss: 0.0761 | 0.0455
Epoch 50/300, seasonal_0 Loss: 0.0756 | 0.0454
Epoch 51/300, seasonal_0 Loss: 0.0751 | 0.0459
Epoch 52/300, seasonal_0 Loss: 0.0746 | 0.0456
Epoch 53/300, seasonal_0 Loss: 0.0743 | 0.0453
Epoch 54/300, seasonal_0 Loss: 0.0740 | 0.0451
Epoch 55/300, seasonal_0 Loss: 0.0739 | 0.0449
Epoch 56/300, seasonal_0 Loss: 0.0737 | 0.0447
Epoch 57/300, seasonal_0 Loss: 0.0735 | 0.0445
Epoch 58/300, seasonal_0 Loss: 0.0734 | 0.0444
Epoch 59/300, seasonal_0 Loss: 0.0732 | 0.0443
Epoch 60/300, seasonal_0 Loss: 0.0731 | 0.0442
Epoch 61/300, seasonal_0 Loss: 0.0730 | 0.0440
Epoch 62/300, seasonal_0 Loss: 0.0729 | 0.0439
Epoch 63/300, seasonal_0 Loss: 0.0727 | 0.0438
Epoch 64/300, seasonal_0 Loss: 0.0726 | 0.0436
Epoch 65/300, seasonal_0 Loss: 0.0725 | 0.0435
Epoch 66/300, seasonal_0 Loss: 0.0724 | 0.0434
Epoch 67/300, seasonal_0 Loss: 0.0723 | 0.0433
Epoch 68/300, seasonal_0 Loss: 0.0722 | 0.0432
Epoch 69/300, seasonal_0 Loss: 0.0720 | 0.0431
Epoch 70/300, seasonal_0 Loss: 0.0719 | 0.0429
Epoch 71/300, seasonal_0 Loss: 0.0718 | 0.0428
Epoch 72/300, seasonal_0 Loss: 0.0717 | 0.0427
Epoch 73/300, seasonal_0 Loss: 0.0716 | 0.0426
Epoch 74/300, seasonal_0 Loss: 0.0715 | 0.0425
Epoch 75/300, seasonal_0 Loss: 0.0714 | 0.0424
Epoch 76/300, seasonal_0 Loss: 0.0713 | 0.0423
Epoch 77/300, seasonal_0 Loss: 0.0712 | 0.0422
Epoch 78/300, seasonal_0 Loss: 0.0711 | 0.0421
Epoch 79/300, seasonal_0 Loss: 0.0710 | 0.0420
Epoch 80/300, seasonal_0 Loss: 0.0709 | 0.0420
Epoch 81/300, seasonal_0 Loss: 0.0708 | 0.0419
Epoch 82/300, seasonal_0 Loss: 0.0708 | 0.0418
Epoch 83/300, seasonal_0 Loss: 0.0707 | 0.0417
Epoch 84/300, seasonal_0 Loss: 0.0706 | 0.0416
Epoch 85/300, seasonal_0 Loss: 0.0705 | 0.0416
Epoch 86/300, seasonal_0 Loss: 0.0704 | 0.0416
Epoch 87/300, seasonal_0 Loss: 0.0704 | 0.0415
Epoch 88/300, seasonal_0 Loss: 0.0703 | 0.0414
Epoch 89/300, seasonal_0 Loss: 0.0702 | 0.0413
Epoch 90/300, seasonal_0 Loss: 0.0701 | 0.0412
Epoch 91/300, seasonal_0 Loss: 0.0701 | 0.0412
Epoch 92/300, seasonal_0 Loss: 0.0700 | 0.0412
Epoch 93/300, seasonal_0 Loss: 0.0700 | 0.0412
Epoch 94/300, seasonal_0 Loss: 0.0699 | 0.0411
Epoch 95/300, seasonal_0 Loss: 0.0699 | 0.0410
Epoch 96/300, seasonal_0 Loss: 0.0698 | 0.0410
Epoch 97/300, seasonal_0 Loss: 0.0698 | 0.0409
Epoch 98/300, seasonal_0 Loss: 0.0698 | 0.0409
Epoch 99/300, seasonal_0 Loss: 0.0697 | 0.0408
Epoch 100/300, seasonal_0 Loss: 0.0697 | 0.0407
Epoch 101/300, seasonal_0 Loss: 0.0697 | 0.0406
Epoch 102/300, seasonal_0 Loss: 0.0697 | 0.0406
Epoch 103/300, seasonal_0 Loss: 0.0697 | 0.0405
Epoch 104/300, seasonal_0 Loss: 0.0696 | 0.0404
Epoch 105/300, seasonal_0 Loss: 0.0696 | 0.0403
Epoch 106/300, seasonal_0 Loss: 0.0697 | 0.0402
Epoch 107/300, seasonal_0 Loss: 0.0697 | 0.0402
Epoch 108/300, seasonal_0 Loss: 0.0696 | 0.0402
Epoch 109/300, seasonal_0 Loss: 0.0695 | 0.0402
Epoch 110/300, seasonal_0 Loss: 0.0694 | 0.0402
Epoch 111/300, seasonal_0 Loss: 0.0694 | 0.0402
Epoch 112/300, seasonal_0 Loss: 0.0693 | 0.0402
Epoch 113/300, seasonal_0 Loss: 0.0693 | 0.0403
Epoch 114/300, seasonal_0 Loss: 0.0693 | 0.0403
Epoch 115/300, seasonal_0 Loss: 0.0693 | 0.0403
Epoch 116/300, seasonal_0 Loss: 0.0693 | 0.0402
Epoch 117/300, seasonal_0 Loss: 0.0692 | 0.0401
Epoch 118/300, seasonal_0 Loss: 0.0692 | 0.0400
Epoch 119/300, seasonal_0 Loss: 0.0691 | 0.0400
Epoch 120/300, seasonal_0 Loss: 0.0691 | 0.0399
Epoch 121/300, seasonal_0 Loss: 0.0691 | 0.0398
Epoch 122/300, seasonal_0 Loss: 0.0690 | 0.0398
Epoch 123/300, seasonal_0 Loss: 0.0690 | 0.0398
Epoch 124/300, seasonal_0 Loss: 0.0690 | 0.0398
Epoch 125/300, seasonal_0 Loss: 0.0690 | 0.0398
Epoch 126/300, seasonal_0 Loss: 0.0690 | 0.0398
Epoch 127/300, seasonal_0 Loss: 0.0690 | 0.0398
Epoch 128/300, seasonal_0 Loss: 0.0689 | 0.0398
Epoch 129/300, seasonal_0 Loss: 0.0689 | 0.0398
Epoch 130/300, seasonal_0 Loss: 0.0689 | 0.0398
Epoch 131/300, seasonal_0 Loss: 0.0689 | 0.0397
Epoch 132/300, seasonal_0 Loss: 0.0689 | 0.0397
Epoch 133/300, seasonal_0 Loss: 0.0689 | 0.0397
Epoch 134/300, seasonal_0 Loss: 0.0689 | 0.0397
Epoch 135/300, seasonal_0 Loss: 0.0689 | 0.0397
Epoch 136/300, seasonal_0 Loss: 0.0689 | 0.0397
Epoch 137/300, seasonal_0 Loss: 0.0688 | 0.0397
Epoch 138/300, seasonal_0 Loss: 0.0688 | 0.0397
Epoch 139/300, seasonal_0 Loss: 0.0688 | 0.0397
Epoch 140/300, seasonal_0 Loss: 0.0688 | 0.0397
Epoch 141/300, seasonal_0 Loss: 0.0688 | 0.0397
Epoch 142/300, seasonal_0 Loss: 0.0688 | 0.0397
Epoch 143/300, seasonal_0 Loss: 0.0688 | 0.0396
Epoch 144/300, seasonal_0 Loss: 0.0688 | 0.0396
Epoch 145/300, seasonal_0 Loss: 0.0688 | 0.0396
Epoch 146/300, seasonal_0 Loss: 0.0688 | 0.0396
Epoch 147/300, seasonal_0 Loss: 0.0688 | 0.0396
Epoch 148/300, seasonal_0 Loss: 0.0688 | 0.0396
Epoch 149/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 150/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 151/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 152/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 153/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 154/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 155/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 156/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 157/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 158/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 159/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 160/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 161/300, seasonal_0 Loss: 0.0687 | 0.0396
Epoch 162/300, seasonal_0 Loss: 0.0687 | 0.0395
Epoch 163/300, seasonal_0 Loss: 0.0687 | 0.0395
Epoch 164/300, seasonal_0 Loss: 0.0687 | 0.0395
Epoch 165/300, seasonal_0 Loss: 0.0687 | 0.0395
Epoch 166/300, seasonal_0 Loss: 0.0687 | 0.0395
Epoch 167/300, seasonal_0 Loss: 0.0687 | 0.0395
Epoch 168/300, seasonal_0 Loss: 0.0687 | 0.0395
Epoch 169/300, seasonal_0 Loss: 0.0687 | 0.0395
Epoch 170/300, seasonal_0 Loss: 0.0687 | 0.0395
Epoch 171/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 172/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 173/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 174/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 175/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 176/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 177/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 178/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 179/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 180/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 181/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 182/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 183/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 184/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 185/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 186/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 187/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 188/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 189/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 190/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 191/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 192/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 193/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 194/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 195/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 196/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 197/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 198/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 199/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 200/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 201/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 202/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 203/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 204/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 205/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 206/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 207/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 208/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 209/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 210/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 211/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 212/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 213/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 214/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 215/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 216/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 217/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 218/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 219/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 220/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 221/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 222/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 223/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 224/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 225/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 226/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 227/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 228/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 229/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 230/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 231/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 232/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 233/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 234/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 235/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 236/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 237/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 238/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 239/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 240/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 241/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 242/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 243/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 244/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 245/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 246/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 247/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 248/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 249/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 250/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 251/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 252/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 253/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 254/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 255/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 256/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 257/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 258/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 259/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 260/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 261/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 262/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 263/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 264/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 265/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 266/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 267/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 268/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 269/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 270/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 271/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 272/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 273/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 274/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 275/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 276/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 277/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 278/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 279/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 280/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 281/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 282/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 283/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 284/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 285/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 286/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 287/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 288/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 289/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 290/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 291/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 292/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 293/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 294/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 295/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 296/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 297/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 298/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 299/300, seasonal_0 Loss: 0.0686 | 0.0395
Epoch 300/300, seasonal_0 Loss: 0.0686 | 0.0395
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.9880908885845407, 'learning_rate': 0.0009918461542730392, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7819356330197345}
Epoch 1/300, seasonal_1 Loss: 0.1623 | 0.0726
Epoch 2/300, seasonal_1 Loss: 0.1005 | 0.0703
Epoch 3/300, seasonal_1 Loss: 0.0951 | 0.0578
Epoch 4/300, seasonal_1 Loss: 0.0903 | 0.0558
Epoch 5/300, seasonal_1 Loss: 0.0874 | 0.0738
Epoch 6/300, seasonal_1 Loss: 0.0816 | 0.0472
Epoch 7/300, seasonal_1 Loss: 0.0790 | 0.0508
Epoch 8/300, seasonal_1 Loss: 0.0796 | 0.0460
Epoch 9/300, seasonal_1 Loss: 0.0745 | 0.0434
Epoch 10/300, seasonal_1 Loss: 0.0722 | 0.0407
Epoch 11/300, seasonal_1 Loss: 0.0738 | 0.0401
Epoch 12/300, seasonal_1 Loss: 0.0711 | 0.0412
Epoch 13/300, seasonal_1 Loss: 0.0688 | 0.0404
Epoch 14/300, seasonal_1 Loss: 0.0706 | 0.0397
Epoch 15/300, seasonal_1 Loss: 0.0718 | 0.0417
Epoch 16/300, seasonal_1 Loss: 0.0689 | 0.0412
Epoch 17/300, seasonal_1 Loss: 0.0645 | 0.0365
Epoch 18/300, seasonal_1 Loss: 0.0635 | 0.0347
Epoch 19/300, seasonal_1 Loss: 0.0625 | 0.0296
Epoch 20/300, seasonal_1 Loss: 0.0621 | 0.0343
Epoch 21/300, seasonal_1 Loss: 0.0623 | 0.0338
Epoch 22/300, seasonal_1 Loss: 0.0610 | 0.0262
Epoch 23/300, seasonal_1 Loss: 0.0603 | 0.0280
Epoch 24/300, seasonal_1 Loss: 0.0582 | 0.0271
Epoch 25/300, seasonal_1 Loss: 0.0571 | 0.0303
Epoch 26/300, seasonal_1 Loss: 0.0579 | 0.0259
Epoch 27/300, seasonal_1 Loss: 0.0564 | 0.0274
Epoch 28/300, seasonal_1 Loss: 0.0570 | 0.0261
Epoch 29/300, seasonal_1 Loss: 0.0564 | 0.0274
Epoch 30/300, seasonal_1 Loss: 0.0575 | 0.0271
Epoch 31/300, seasonal_1 Loss: 0.0571 | 0.0335
Epoch 32/300, seasonal_1 Loss: 0.0541 | 0.0251
Epoch 33/300, seasonal_1 Loss: 0.0529 | 0.0261
Epoch 34/300, seasonal_1 Loss: 0.0532 | 0.0259
Epoch 35/300, seasonal_1 Loss: 0.0523 | 0.0235
Epoch 36/300, seasonal_1 Loss: 0.0549 | 0.0278
Epoch 37/300, seasonal_1 Loss: 0.0533 | 0.0267
Epoch 38/300, seasonal_1 Loss: 0.0512 | 0.0263
Epoch 39/300, seasonal_1 Loss: 0.0507 | 0.0204
Epoch 40/300, seasonal_1 Loss: 0.0493 | 0.0213
Epoch 41/300, seasonal_1 Loss: 0.0491 | 0.0205
Epoch 42/300, seasonal_1 Loss: 0.0486 | 0.0219
Epoch 43/300, seasonal_1 Loss: 0.0480 | 0.0226
Epoch 44/300, seasonal_1 Loss: 0.0479 | 0.0286
Epoch 45/300, seasonal_1 Loss: 0.0490 | 0.0249
Epoch 46/300, seasonal_1 Loss: 0.0476 | 0.0233
Epoch 47/300, seasonal_1 Loss: 0.0471 | 0.0244
Epoch 48/300, seasonal_1 Loss: 0.0465 | 0.0258
Epoch 49/300, seasonal_1 Loss: 0.0465 | 0.0263
Epoch 50/300, seasonal_1 Loss: 0.0494 | 0.0296
Epoch 51/300, seasonal_1 Loss: 0.0470 | 0.0287
Epoch 52/300, seasonal_1 Loss: 0.0536 | 0.0456
Epoch 53/300, seasonal_1 Loss: 0.0546 | 0.0303
Epoch 54/300, seasonal_1 Loss: 0.0496 | 0.0256
Epoch 55/300, seasonal_1 Loss: 0.0468 | 0.0251
Epoch 56/300, seasonal_1 Loss: 0.0458 | 0.0247
Epoch 57/300, seasonal_1 Loss: 0.0453 | 0.0245
Epoch 58/300, seasonal_1 Loss: 0.0448 | 0.0248
Epoch 59/300, seasonal_1 Loss: 0.0446 | 0.0250
Epoch 60/300, seasonal_1 Loss: 0.0445 | 0.0246
Epoch 61/300, seasonal_1 Loss: 0.0439 | 0.0247
Epoch 62/300, seasonal_1 Loss: 0.0434 | 0.0259
Epoch 63/300, seasonal_1 Loss: 0.0434 | 0.0264
Epoch 64/300, seasonal_1 Loss: 0.0448 | 0.0264
Epoch 65/300, seasonal_1 Loss: 0.0439 | 0.0254
Epoch 66/300, seasonal_1 Loss: 0.0434 | 0.0262
Epoch 67/300, seasonal_1 Loss: 0.0427 | 0.0264
Epoch 68/300, seasonal_1 Loss: 0.0422 | 0.0284
Epoch 69/300, seasonal_1 Loss: 0.0430 | 0.0253
Epoch 70/300, seasonal_1 Loss: 0.0398 | 0.0256
Epoch 71/300, seasonal_1 Loss: 0.0391 | 0.0262
Epoch 72/300, seasonal_1 Loss: 0.0368 | 0.0279
Epoch 73/300, seasonal_1 Loss: 0.0362 | 0.0264
Epoch 74/300, seasonal_1 Loss: 0.0416 | 0.0332
Epoch 75/300, seasonal_1 Loss: 0.0346 | 0.0266
Epoch 76/300, seasonal_1 Loss: 0.0331 | 0.0271
Epoch 77/300, seasonal_1 Loss: 0.0325 | 0.0281
Epoch 78/300, seasonal_1 Loss: 0.0321 | 0.0278
Epoch 79/300, seasonal_1 Loss: 0.0321 | 0.0293
Epoch 80/300, seasonal_1 Loss: 0.0312 | 0.0303
Epoch 81/300, seasonal_1 Loss: 0.0308 | 0.0299
Epoch 82/300, seasonal_1 Loss: 0.0305 | 0.0301
Epoch 83/300, seasonal_1 Loss: 0.0303 | 0.0314
Epoch 84/300, seasonal_1 Loss: 0.0300 | 0.0310
Epoch 85/300, seasonal_1 Loss: 0.0297 | 0.0310
Epoch 86/300, seasonal_1 Loss: 0.0295 | 0.0315
Epoch 87/300, seasonal_1 Loss: 0.0293 | 0.0320
Epoch 88/300, seasonal_1 Loss: 0.0291 | 0.0326
Epoch 89/300, seasonal_1 Loss: 0.0289 | 0.0323
Epoch 90/300, seasonal_1 Loss: 0.0288 | 0.0334
Epoch 91/300, seasonal_1 Loss: 0.0285 | 0.0325
Epoch 92/300, seasonal_1 Loss: 0.0284 | 0.0328
Epoch 93/300, seasonal_1 Loss: 0.0281 | 0.0335
Epoch 94/300, seasonal_1 Loss: 0.0280 | 0.0329
Epoch 95/300, seasonal_1 Loss: 0.0279 | 0.0338
Epoch 96/300, seasonal_1 Loss: 0.0276 | 0.0331
Epoch 97/300, seasonal_1 Loss: 0.0275 | 0.0347
Epoch 98/300, seasonal_1 Loss: 0.0274 | 0.0342
Epoch 99/300, seasonal_1 Loss: 0.0273 | 0.0346
Epoch 100/300, seasonal_1 Loss: 0.0271 | 0.0357
Epoch 101/300, seasonal_1 Loss: 0.0270 | 0.0355
Epoch 102/300, seasonal_1 Loss: 0.0268 | 0.0373
Epoch 103/300, seasonal_1 Loss: 0.0267 | 0.0360
Epoch 104/300, seasonal_1 Loss: 0.0266 | 0.0388
Epoch 105/300, seasonal_1 Loss: 0.0265 | 0.0371
Epoch 106/300, seasonal_1 Loss: 0.0263 | 0.0406
Epoch 107/300, seasonal_1 Loss: 0.0262 | 0.0376
Epoch 108/300, seasonal_1 Loss: 0.0260 | 0.0387
Epoch 109/300, seasonal_1 Loss: 0.0259 | 0.0370
Epoch 110/300, seasonal_1 Loss: 0.0258 | 0.0380
Epoch 111/300, seasonal_1 Loss: 0.0257 | 0.0370
Epoch 112/300, seasonal_1 Loss: 0.0256 | 0.0386
Epoch 113/300, seasonal_1 Loss: 0.0255 | 0.0372
Epoch 114/300, seasonal_1 Loss: 0.0253 | 0.0387
Epoch 115/300, seasonal_1 Loss: 0.0252 | 0.0379
Epoch 116/300, seasonal_1 Loss: 0.0251 | 0.0390
Epoch 117/300, seasonal_1 Loss: 0.0250 | 0.0394
Epoch 118/300, seasonal_1 Loss: 0.0249 | 0.0396
Epoch 119/300, seasonal_1 Loss: 0.0248 | 0.0403
Epoch 120/300, seasonal_1 Loss: 0.0247 | 0.0411
Epoch 121/300, seasonal_1 Loss: 0.0246 | 0.0403
Epoch 122/300, seasonal_1 Loss: 0.0246 | 0.0407
Epoch 123/300, seasonal_1 Loss: 0.0245 | 0.0406
Epoch 124/300, seasonal_1 Loss: 0.0244 | 0.0409
Epoch 125/300, seasonal_1 Loss: 0.0243 | 0.0408
Epoch 126/300, seasonal_1 Loss: 0.0243 | 0.0410
Epoch 127/300, seasonal_1 Loss: 0.0242 | 0.0411
Epoch 128/300, seasonal_1 Loss: 0.0241 | 0.0411
Epoch 129/300, seasonal_1 Loss: 0.0240 | 0.0410
Epoch 130/300, seasonal_1 Loss: 0.0240 | 0.0412
Epoch 131/300, seasonal_1 Loss: 0.0239 | 0.0414
Epoch 132/300, seasonal_1 Loss: 0.0239 | 0.0415
Epoch 133/300, seasonal_1 Loss: 0.0238 | 0.0416
Epoch 134/300, seasonal_1 Loss: 0.0238 | 0.0419
Epoch 135/300, seasonal_1 Loss: 0.0237 | 0.0421
Epoch 136/300, seasonal_1 Loss: 0.0237 | 0.0421
Epoch 137/300, seasonal_1 Loss: 0.0236 | 0.0422
Epoch 138/300, seasonal_1 Loss: 0.0236 | 0.0425
Epoch 139/300, seasonal_1 Loss: 0.0235 | 0.0426
Epoch 140/300, seasonal_1 Loss: 0.0235 | 0.0427
Epoch 141/300, seasonal_1 Loss: 0.0234 | 0.0428
Epoch 142/300, seasonal_1 Loss: 0.0234 | 0.0430
Epoch 143/300, seasonal_1 Loss: 0.0234 | 0.0431
Epoch 144/300, seasonal_1 Loss: 0.0233 | 0.0429
Epoch 145/300, seasonal_1 Loss: 0.0233 | 0.0431
Epoch 146/300, seasonal_1 Loss: 0.0233 | 0.0432
Epoch 147/300, seasonal_1 Loss: 0.0232 | 0.0433
Epoch 148/300, seasonal_1 Loss: 0.0232 | 0.0433
Epoch 149/300, seasonal_1 Loss: 0.0232 | 0.0434
Epoch 150/300, seasonal_1 Loss: 0.0231 | 0.0433
Epoch 151/300, seasonal_1 Loss: 0.0231 | 0.0435
Epoch 152/300, seasonal_1 Loss: 0.0231 | 0.0435
Epoch 153/300, seasonal_1 Loss: 0.0230 | 0.0436
Epoch 154/300, seasonal_1 Loss: 0.0230 | 0.0436
Epoch 155/300, seasonal_1 Loss: 0.0230 | 0.0437
Epoch 156/300, seasonal_1 Loss: 0.0230 | 0.0437
Epoch 157/300, seasonal_1 Loss: 0.0229 | 0.0437
Epoch 158/300, seasonal_1 Loss: 0.0229 | 0.0438
Epoch 159/300, seasonal_1 Loss: 0.0229 | 0.0440
Epoch 160/300, seasonal_1 Loss: 0.0229 | 0.0440
Epoch 161/300, seasonal_1 Loss: 0.0229 | 0.0440
Epoch 162/300, seasonal_1 Loss: 0.0228 | 0.0440
Epoch 163/300, seasonal_1 Loss: 0.0228 | 0.0440
Epoch 164/300, seasonal_1 Loss: 0.0228 | 0.0440
Epoch 165/300, seasonal_1 Loss: 0.0228 | 0.0439
Epoch 166/300, seasonal_1 Loss: 0.0228 | 0.0440
Epoch 167/300, seasonal_1 Loss: 0.0227 | 0.0439
Epoch 168/300, seasonal_1 Loss: 0.0227 | 0.0438
Epoch 169/300, seasonal_1 Loss: 0.0227 | 0.0437
Epoch 170/300, seasonal_1 Loss: 0.0227 | 0.0436
Epoch 171/300, seasonal_1 Loss: 0.0227 | 0.0435
Epoch 172/300, seasonal_1 Loss: 0.0227 | 0.0434
Epoch 173/300, seasonal_1 Loss: 0.0226 | 0.0433
Epoch 174/300, seasonal_1 Loss: 0.0226 | 0.0432
Epoch 175/300, seasonal_1 Loss: 0.0226 | 0.0431
Epoch 176/300, seasonal_1 Loss: 0.0226 | 0.0430
Epoch 177/300, seasonal_1 Loss: 0.0226 | 0.0429
Epoch 178/300, seasonal_1 Loss: 0.0226 | 0.0428
Epoch 179/300, seasonal_1 Loss: 0.0225 | 0.0427
Epoch 180/300, seasonal_1 Loss: 0.0225 | 0.0427
Epoch 181/300, seasonal_1 Loss: 0.0225 | 0.0426
Epoch 182/300, seasonal_1 Loss: 0.0225 | 0.0426
Epoch 183/300, seasonal_1 Loss: 0.0225 | 0.0425
Epoch 184/300, seasonal_1 Loss: 0.0225 | 0.0425
Epoch 185/300, seasonal_1 Loss: 0.0224 | 0.0425
Epoch 186/300, seasonal_1 Loss: 0.0224 | 0.0425
Epoch 187/300, seasonal_1 Loss: 0.0224 | 0.0425
Epoch 188/300, seasonal_1 Loss: 0.0224 | 0.0425
Epoch 189/300, seasonal_1 Loss: 0.0224 | 0.0425
Epoch 190/300, seasonal_1 Loss: 0.0224 | 0.0426
Epoch 191/300, seasonal_1 Loss: 0.0224 | 0.0426
Epoch 192/300, seasonal_1 Loss: 0.0224 | 0.0426
Epoch 193/300, seasonal_1 Loss: 0.0223 | 0.0426
Epoch 194/300, seasonal_1 Loss: 0.0223 | 0.0427
Epoch 195/300, seasonal_1 Loss: 0.0223 | 0.0427
Epoch 196/300, seasonal_1 Loss: 0.0223 | 0.0427
Epoch 197/300, seasonal_1 Loss: 0.0223 | 0.0428
Epoch 198/300, seasonal_1 Loss: 0.0223 | 0.0428
Epoch 199/300, seasonal_1 Loss: 0.0223 | 0.0428
Epoch 200/300, seasonal_1 Loss: 0.0223 | 0.0429
Epoch 201/300, seasonal_1 Loss: 0.0223 | 0.0429
Epoch 202/300, seasonal_1 Loss: 0.0223 | 0.0429
Epoch 203/300, seasonal_1 Loss: 0.0222 | 0.0430
Epoch 204/300, seasonal_1 Loss: 0.0222 | 0.0430
Epoch 205/300, seasonal_1 Loss: 0.0222 | 0.0430
Epoch 206/300, seasonal_1 Loss: 0.0222 | 0.0430
Epoch 207/300, seasonal_1 Loss: 0.0222 | 0.0431
Epoch 208/300, seasonal_1 Loss: 0.0222 | 0.0431
Epoch 209/300, seasonal_1 Loss: 0.0222 | 0.0431
Epoch 210/300, seasonal_1 Loss: 0.0222 | 0.0431
Epoch 211/300, seasonal_1 Loss: 0.0222 | 0.0431
Epoch 212/300, seasonal_1 Loss: 0.0222 | 0.0432
Epoch 213/300, seasonal_1 Loss: 0.0222 | 0.0432
Epoch 214/300, seasonal_1 Loss: 0.0222 | 0.0432
Epoch 215/300, seasonal_1 Loss: 0.0222 | 0.0432
Epoch 216/300, seasonal_1 Loss: 0.0222 | 0.0432
Epoch 217/300, seasonal_1 Loss: 0.0222 | 0.0433
Epoch 218/300, seasonal_1 Loss: 0.0222 | 0.0433
Epoch 219/300, seasonal_1 Loss: 0.0221 | 0.0433
Epoch 220/300, seasonal_1 Loss: 0.0221 | 0.0433
Epoch 221/300, seasonal_1 Loss: 0.0221 | 0.0433
Epoch 222/300, seasonal_1 Loss: 0.0221 | 0.0433
Epoch 223/300, seasonal_1 Loss: 0.0221 | 0.0434
Epoch 224/300, seasonal_1 Loss: 0.0221 | 0.0434
Epoch 225/300, seasonal_1 Loss: 0.0221 | 0.0434
Epoch 226/300, seasonal_1 Loss: 0.0221 | 0.0434
Epoch 227/300, seasonal_1 Loss: 0.0221 | 0.0434
Epoch 228/300, seasonal_1 Loss: 0.0221 | 0.0434
Epoch 229/300, seasonal_1 Loss: 0.0221 | 0.0434
Epoch 230/300, seasonal_1 Loss: 0.0221 | 0.0434
Epoch 231/300, seasonal_1 Loss: 0.0221 | 0.0434
Epoch 232/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 233/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 234/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 235/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 236/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 237/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 238/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 239/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 240/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 241/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 242/300, seasonal_1 Loss: 0.0221 | 0.0435
Epoch 243/300, seasonal_1 Loss: 0.0221 | 0.0436
Epoch 244/300, seasonal_1 Loss: 0.0221 | 0.0436
Epoch 245/300, seasonal_1 Loss: 0.0221 | 0.0436
Epoch 246/300, seasonal_1 Loss: 0.0221 | 0.0436
Epoch 247/300, seasonal_1 Loss: 0.0221 | 0.0436
Epoch 248/300, seasonal_1 Loss: 0.0221 | 0.0436
Epoch 249/300, seasonal_1 Loss: 0.0221 | 0.0436
Epoch 250/300, seasonal_1 Loss: 0.0220 | 0.0436
Epoch 251/300, seasonal_1 Loss: 0.0220 | 0.0436
Epoch 252/300, seasonal_1 Loss: 0.0220 | 0.0436
Epoch 253/300, seasonal_1 Loss: 0.0220 | 0.0436
Epoch 254/300, seasonal_1 Loss: 0.0220 | 0.0436
Epoch 255/300, seasonal_1 Loss: 0.0220 | 0.0436
Epoch 256/300, seasonal_1 Loss: 0.0220 | 0.0436
Epoch 257/300, seasonal_1 Loss: 0.0220 | 0.0436
Epoch 258/300, seasonal_1 Loss: 0.0220 | 0.0436
Epoch 259/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 260/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 261/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 262/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 263/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 264/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 265/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 266/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 267/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 268/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 269/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 270/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 271/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 272/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 273/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 274/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 275/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 276/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 277/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 278/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 279/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 280/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 281/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 282/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 283/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 284/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 285/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 286/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 287/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 288/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 289/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 290/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 291/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 292/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 293/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 294/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 295/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 296/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 297/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 298/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 299/300, seasonal_1 Loss: 0.0220 | 0.0437
Epoch 300/300, seasonal_1 Loss: 0.0220 | 0.0438
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.9889479038177518, 'learning_rate': 0.0009997479022770277, 'batch_size': 194, 'step_size': 14, 'gamma': 0.9563824894567773}
Epoch 1/300, seasonal_2 Loss: 0.5938 | 0.2281
Epoch 2/300, seasonal_2 Loss: 0.1618 | 0.1043
Epoch 3/300, seasonal_2 Loss: 0.1392 | 0.0859
Epoch 4/300, seasonal_2 Loss: 0.1344 | 0.0910
Epoch 5/300, seasonal_2 Loss: 0.1445 | 0.1163
Epoch 6/300, seasonal_2 Loss: 0.1664 | 0.1152
Epoch 7/300, seasonal_2 Loss: 0.1261 | 0.0984
Epoch 8/300, seasonal_2 Loss: 0.1413 | 0.1239
Epoch 9/300, seasonal_2 Loss: 0.1650 | 0.1118
Epoch 10/300, seasonal_2 Loss: 0.1277 | 0.0867
Epoch 11/300, seasonal_2 Loss: 0.1189 | 0.0796
Epoch 12/300, seasonal_2 Loss: 0.1048 | 0.0888
Epoch 13/300, seasonal_2 Loss: 0.1126 | 0.0921
Epoch 14/300, seasonal_2 Loss: 0.1355 | 0.0850
Epoch 15/300, seasonal_2 Loss: 0.1784 | 0.1596
Epoch 16/300, seasonal_2 Loss: 0.1568 | 0.1791
Epoch 17/300, seasonal_2 Loss: 0.1289 | 0.1772
Epoch 18/300, seasonal_2 Loss: 0.1048 | 0.0716
Epoch 19/300, seasonal_2 Loss: 0.0910 | 0.0757
Epoch 20/300, seasonal_2 Loss: 0.0859 | 0.0753
Epoch 21/300, seasonal_2 Loss: 0.0855 | 0.0608
Epoch 22/300, seasonal_2 Loss: 0.0818 | 0.0579
Epoch 23/300, seasonal_2 Loss: 0.0797 | 0.0569
Epoch 24/300, seasonal_2 Loss: 0.0785 | 0.0563
Epoch 25/300, seasonal_2 Loss: 0.0776 | 0.0573
Epoch 26/300, seasonal_2 Loss: 0.0765 | 0.0552
Epoch 27/300, seasonal_2 Loss: 0.0759 | 0.0534
Epoch 28/300, seasonal_2 Loss: 0.0757 | 0.0529
Epoch 29/300, seasonal_2 Loss: 0.0753 | 0.0523
Epoch 30/300, seasonal_2 Loss: 0.0741 | 0.0506
Epoch 31/300, seasonal_2 Loss: 0.0742 | 0.0515
Epoch 32/300, seasonal_2 Loss: 0.0797 | 0.0566
Epoch 33/300, seasonal_2 Loss: 0.0832 | 0.0550
Epoch 34/300, seasonal_2 Loss: 0.0765 | 0.0574
Epoch 35/300, seasonal_2 Loss: 0.0746 | 0.0532
Epoch 36/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 37/300, seasonal_2 Loss: 0.0761 | 0.0547
Epoch 38/300, seasonal_2 Loss: 0.0764 | 0.0536
Epoch 39/300, seasonal_2 Loss: 0.0754 | 0.0559
Epoch 40/300, seasonal_2 Loss: 0.0763 | 0.0532
Epoch 41/300, seasonal_2 Loss: 0.0747 | 0.0542
Epoch 42/300, seasonal_2 Loss: 0.0741 | 0.0524
Epoch 43/300, seasonal_2 Loss: 0.0738 | 0.0513
Epoch 44/300, seasonal_2 Loss: 0.0715 | 0.0549
Epoch 45/300, seasonal_2 Loss: 0.0723 | 0.0537
Epoch 46/300, seasonal_2 Loss: 0.0736 | 0.0581
Epoch 47/300, seasonal_2 Loss: 0.0784 | 0.0671
Epoch 48/300, seasonal_2 Loss: 0.0805 | 0.0724
Epoch 49/300, seasonal_2 Loss: 0.0841 | 0.0757
Epoch 50/300, seasonal_2 Loss: 0.0921 | 0.0876
Epoch 51/300, seasonal_2 Loss: 0.0858 | 0.0756
Epoch 52/300, seasonal_2 Loss: 0.0951 | 0.0916
Epoch 53/300, seasonal_2 Loss: 0.0972 | 0.0637
Epoch 54/300, seasonal_2 Loss: 0.0905 | 0.0690
Epoch 55/300, seasonal_2 Loss: 0.0905 | 0.0589
Epoch 56/300, seasonal_2 Loss: 0.0915 | 0.0790
Epoch 57/300, seasonal_2 Loss: 0.0941 | 0.0780
Epoch 58/300, seasonal_2 Loss: 0.0937 | 0.0574
Epoch 59/300, seasonal_2 Loss: 0.0916 | 0.0769
Epoch 60/300, seasonal_2 Loss: 0.1020 | 0.0586
Epoch 61/300, seasonal_2 Loss: 0.0844 | 0.0584
Epoch 62/300, seasonal_2 Loss: 0.0846 | 0.0576
Epoch 63/300, seasonal_2 Loss: 0.0794 | 0.0516
Epoch 64/300, seasonal_2 Loss: 0.0739 | 0.0511
Epoch 65/300, seasonal_2 Loss: 0.0707 | 0.0504
Epoch 66/300, seasonal_2 Loss: 0.0719 | 0.0550
Epoch 67/300, seasonal_2 Loss: 0.0722 | 0.0562
Epoch 68/300, seasonal_2 Loss: 0.0702 | 0.0539
Epoch 69/300, seasonal_2 Loss: 0.0689 | 0.0534
Epoch 70/300, seasonal_2 Loss: 0.0675 | 0.0534
Epoch 71/300, seasonal_2 Loss: 0.0680 | 0.0517
Epoch 72/300, seasonal_2 Loss: 0.0692 | 0.0528
Epoch 73/300, seasonal_2 Loss: 0.0707 | 0.0509
Epoch 74/300, seasonal_2 Loss: 0.0712 | 0.0499
Epoch 75/300, seasonal_2 Loss: 0.0688 | 0.0523
Epoch 76/300, seasonal_2 Loss: 0.0681 | 0.0495
Epoch 77/300, seasonal_2 Loss: 0.0677 | 0.0477
Epoch 78/300, seasonal_2 Loss: 0.0698 | 0.0521
Epoch 79/300, seasonal_2 Loss: 0.0700 | 0.0511
Epoch 80/300, seasonal_2 Loss: 0.0676 | 0.0527
Epoch 81/300, seasonal_2 Loss: 0.0667 | 0.0549
Epoch 82/300, seasonal_2 Loss: 0.0652 | 0.0518
Epoch 83/300, seasonal_2 Loss: 0.0652 | 0.0473
Epoch 84/300, seasonal_2 Loss: 0.0661 | 0.0503
Epoch 85/300, seasonal_2 Loss: 0.0671 | 0.0478
Epoch 86/300, seasonal_2 Loss: 0.0667 | 0.0484
Epoch 87/300, seasonal_2 Loss: 0.0656 | 0.0495
Epoch 88/300, seasonal_2 Loss: 0.0657 | 0.0461
Epoch 89/300, seasonal_2 Loss: 0.0669 | 0.0475
Epoch 90/300, seasonal_2 Loss: 0.0645 | 0.0477
Epoch 91/300, seasonal_2 Loss: 0.0648 | 0.0480
Epoch 92/300, seasonal_2 Loss: 0.0659 | 0.0450
Epoch 93/300, seasonal_2 Loss: 0.0664 | 0.0498
Epoch 94/300, seasonal_2 Loss: 0.0653 | 0.0480
Epoch 95/300, seasonal_2 Loss: 0.0636 | 0.0510
Epoch 96/300, seasonal_2 Loss: 0.0625 | 0.0438
Epoch 97/300, seasonal_2 Loss: 0.0616 | 0.0556
Epoch 98/300, seasonal_2 Loss: 0.0629 | 0.0440
Epoch 99/300, seasonal_2 Loss: 0.0653 | 0.0478
Epoch 100/300, seasonal_2 Loss: 0.0666 | 0.0484
Epoch 101/300, seasonal_2 Loss: 0.0662 | 0.0495
Epoch 102/300, seasonal_2 Loss: 0.0640 | 0.0449
Epoch 103/300, seasonal_2 Loss: 0.0641 | 0.0448
Epoch 104/300, seasonal_2 Loss: 0.0641 | 0.0435
Epoch 105/300, seasonal_2 Loss: 0.0622 | 0.0443
Epoch 106/300, seasonal_2 Loss: 0.0617 | 0.0466
Epoch 107/300, seasonal_2 Loss: 0.0625 | 0.0479
Epoch 108/300, seasonal_2 Loss: 0.0630 | 0.0496
Epoch 109/300, seasonal_2 Loss: 0.0629 | 0.0503
Epoch 110/300, seasonal_2 Loss: 0.0620 | 0.0477
Epoch 111/300, seasonal_2 Loss: 0.0629 | 0.0484
Epoch 112/300, seasonal_2 Loss: 0.0621 | 0.0491
Epoch 113/300, seasonal_2 Loss: 0.0661 | 0.0468
Epoch 114/300, seasonal_2 Loss: 0.0671 | 0.0495
Epoch 115/300, seasonal_2 Loss: 0.0670 | 0.0438
Epoch 116/300, seasonal_2 Loss: 0.0613 | 0.0416
Epoch 117/300, seasonal_2 Loss: 0.0611 | 0.0415
Epoch 118/300, seasonal_2 Loss: 0.0624 | 0.0461
Epoch 119/300, seasonal_2 Loss: 0.0631 | 0.0425
Epoch 120/300, seasonal_2 Loss: 0.0609 | 0.0482
Epoch 121/300, seasonal_2 Loss: 0.0607 | 0.0437
Epoch 122/300, seasonal_2 Loss: 0.0599 | 0.0404
Epoch 123/300, seasonal_2 Loss: 0.0589 | 0.0431
Epoch 124/300, seasonal_2 Loss: 0.0587 | 0.0442
Epoch 125/300, seasonal_2 Loss: 0.0590 | 0.0412
Epoch 126/300, seasonal_2 Loss: 0.0593 | 0.0415
Epoch 127/300, seasonal_2 Loss: 0.0589 | 0.0401
Epoch 128/300, seasonal_2 Loss: 0.0580 | 0.0386
Epoch 129/300, seasonal_2 Loss: 0.0581 | 0.0362
Epoch 130/300, seasonal_2 Loss: 0.0580 | 0.0376
Epoch 131/300, seasonal_2 Loss: 0.0590 | 0.0411
Epoch 132/300, seasonal_2 Loss: 0.0603 | 0.0485
Epoch 133/300, seasonal_2 Loss: 0.0633 | 0.0395
Epoch 134/300, seasonal_2 Loss: 0.0655 | 0.0431
Epoch 135/300, seasonal_2 Loss: 0.0616 | 0.0449
Epoch 136/300, seasonal_2 Loss: 0.0581 | 0.0390
Epoch 137/300, seasonal_2 Loss: 0.0579 | 0.0435
Epoch 138/300, seasonal_2 Loss: 0.0588 | 0.0388
Epoch 139/300, seasonal_2 Loss: 0.0588 | 0.0406
Epoch 140/300, seasonal_2 Loss: 0.0584 | 0.0423
Epoch 141/300, seasonal_2 Loss: 0.0581 | 0.0371
Epoch 142/300, seasonal_2 Loss: 0.0560 | 0.0330
Epoch 143/300, seasonal_2 Loss: 0.0550 | 0.0333
Epoch 144/300, seasonal_2 Loss: 0.0557 | 0.0309
Epoch 145/300, seasonal_2 Loss: 0.0566 | 0.0326
Epoch 146/300, seasonal_2 Loss: 0.0567 | 0.0303
Epoch 147/300, seasonal_2 Loss: 0.0551 | 0.0302
Epoch 148/300, seasonal_2 Loss: 0.0552 | 0.0277
Epoch 149/300, seasonal_2 Loss: 0.0547 | 0.0273
Epoch 150/300, seasonal_2 Loss: 0.0546 | 0.0332
Epoch 151/300, seasonal_2 Loss: 0.0566 | 0.0324
Epoch 152/300, seasonal_2 Loss: 0.0568 | 0.0333
Epoch 153/300, seasonal_2 Loss: 0.0601 | 0.0395
Epoch 154/300, seasonal_2 Loss: 0.0577 | 0.0305
Epoch 155/300, seasonal_2 Loss: 0.0558 | 0.0341
Epoch 156/300, seasonal_2 Loss: 0.0548 | 0.0375
Epoch 157/300, seasonal_2 Loss: 0.0537 | 0.0350
Epoch 158/300, seasonal_2 Loss: 0.0528 | 0.0319
Epoch 159/300, seasonal_2 Loss: 0.0530 | 0.0263
Epoch 160/300, seasonal_2 Loss: 0.0524 | 0.0285
Epoch 161/300, seasonal_2 Loss: 0.0554 | 0.0320
Epoch 162/300, seasonal_2 Loss: 0.0540 | 0.0323
Epoch 163/300, seasonal_2 Loss: 0.0532 | 0.0359
Epoch 164/300, seasonal_2 Loss: 0.0533 | 0.0349
Epoch 165/300, seasonal_2 Loss: 0.0527 | 0.0279
Epoch 166/300, seasonal_2 Loss: 0.0528 | 0.0274
Epoch 167/300, seasonal_2 Loss: 0.0523 | 0.0266
Epoch 168/300, seasonal_2 Loss: 0.0529 | 0.0313
Epoch 169/300, seasonal_2 Loss: 0.0557 | 0.0450
Epoch 170/300, seasonal_2 Loss: 0.0567 | 0.0403
Epoch 171/300, seasonal_2 Loss: 0.0588 | 0.0267
Epoch 172/300, seasonal_2 Loss: 0.0568 | 0.0336
Epoch 173/300, seasonal_2 Loss: 0.0575 | 0.0329
Epoch 174/300, seasonal_2 Loss: 0.0572 | 0.0372
Epoch 175/300, seasonal_2 Loss: 0.0553 | 0.0282
Epoch 176/300, seasonal_2 Loss: 0.0562 | 0.0278
Epoch 177/300, seasonal_2 Loss: 0.0587 | 0.0329
Epoch 178/300, seasonal_2 Loss: 0.0578 | 0.0342
Epoch 179/300, seasonal_2 Loss: 0.0555 | 0.0270
Epoch 180/300, seasonal_2 Loss: 0.0530 | 0.0273
Epoch 181/300, seasonal_2 Loss: 0.0540 | 0.0264
Epoch 182/300, seasonal_2 Loss: 0.0541 | 0.0276
Epoch 183/300, seasonal_2 Loss: 0.0530 | 0.0288
Epoch 184/300, seasonal_2 Loss: 0.0517 | 0.0306
Epoch 185/300, seasonal_2 Loss: 0.0525 | 0.0238
Epoch 186/300, seasonal_2 Loss: 0.0528 | 0.0264
Epoch 187/300, seasonal_2 Loss: 0.0545 | 0.0323
Epoch 188/300, seasonal_2 Loss: 0.0519 | 0.0231
Epoch 189/300, seasonal_2 Loss: 0.0500 | 0.0243
Epoch 190/300, seasonal_2 Loss: 0.0502 | 0.0239
Epoch 191/300, seasonal_2 Loss: 0.0502 | 0.0250
Epoch 192/300, seasonal_2 Loss: 0.0496 | 0.0240
Epoch 193/300, seasonal_2 Loss: 0.0492 | 0.0240
Epoch 194/300, seasonal_2 Loss: 0.0489 | 0.0240
Epoch 195/300, seasonal_2 Loss: 0.0490 | 0.0229
Epoch 196/300, seasonal_2 Loss: 0.0490 | 0.0233
Epoch 197/300, seasonal_2 Loss: 0.0490 | 0.0241
Epoch 198/300, seasonal_2 Loss: 0.0490 | 0.0246
Epoch 199/300, seasonal_2 Loss: 0.0489 | 0.0256
Epoch 200/300, seasonal_2 Loss: 0.0488 | 0.0264
Epoch 201/300, seasonal_2 Loss: 0.0489 | 0.0276
Epoch 202/300, seasonal_2 Loss: 0.0490 | 0.0269
Epoch 203/300, seasonal_2 Loss: 0.0487 | 0.0283
Epoch 204/300, seasonal_2 Loss: 0.0489 | 0.0267
Epoch 205/300, seasonal_2 Loss: 0.0486 | 0.0266
Epoch 206/300, seasonal_2 Loss: 0.0486 | 0.0249
Epoch 207/300, seasonal_2 Loss: 0.0485 | 0.0261
Epoch 208/300, seasonal_2 Loss: 0.0489 | 0.0245
Epoch 209/300, seasonal_2 Loss: 0.0491 | 0.0248
Epoch 210/300, seasonal_2 Loss: 0.0495 | 0.0258
Epoch 211/300, seasonal_2 Loss: 0.0494 | 0.0251
Epoch 212/300, seasonal_2 Loss: 0.0495 | 0.0247
Epoch 213/300, seasonal_2 Loss: 0.0493 | 0.0263
Epoch 214/300, seasonal_2 Loss: 0.0492 | 0.0250
Epoch 215/300, seasonal_2 Loss: 0.0488 | 0.0250
Epoch 216/300, seasonal_2 Loss: 0.0485 | 0.0245
Epoch 217/300, seasonal_2 Loss: 0.0483 | 0.0266
Epoch 218/300, seasonal_2 Loss: 0.0480 | 0.0232
Epoch 219/300, seasonal_2 Loss: 0.0476 | 0.0233
Epoch 220/300, seasonal_2 Loss: 0.0474 | 0.0235
Epoch 221/300, seasonal_2 Loss: 0.0476 | 0.0255
Epoch 222/300, seasonal_2 Loss: 0.0477 | 0.0240
Epoch 223/300, seasonal_2 Loss: 0.0475 | 0.0238
Epoch 224/300, seasonal_2 Loss: 0.0472 | 0.0233
Epoch 225/300, seasonal_2 Loss: 0.0474 | 0.0248
Epoch 226/300, seasonal_2 Loss: 0.0477 | 0.0246
Epoch 227/300, seasonal_2 Loss: 0.0474 | 0.0260
Epoch 228/300, seasonal_2 Loss: 0.0476 | 0.0254
Epoch 229/300, seasonal_2 Loss: 0.0474 | 0.0255
Epoch 230/300, seasonal_2 Loss: 0.0477 | 0.0282
Epoch 231/300, seasonal_2 Loss: 0.0476 | 0.0267
Epoch 232/300, seasonal_2 Loss: 0.0479 | 0.0253
Epoch 233/300, seasonal_2 Loss: 0.0479 | 0.0264
Epoch 234/300, seasonal_2 Loss: 0.0479 | 0.0278
Epoch 235/300, seasonal_2 Loss: 0.0486 | 0.0249
Epoch 236/300, seasonal_2 Loss: 0.0480 | 0.0257
Epoch 237/300, seasonal_2 Loss: 0.0480 | 0.0246
Epoch 238/300, seasonal_2 Loss: 0.0477 | 0.0230
Epoch 239/300, seasonal_2 Loss: 0.0477 | 0.0267
Epoch 240/300, seasonal_2 Loss: 0.0493 | 0.0283
Epoch 241/300, seasonal_2 Loss: 0.0488 | 0.0282
Epoch 242/300, seasonal_2 Loss: 0.0483 | 0.0239
Epoch 243/300, seasonal_2 Loss: 0.0469 | 0.0243
Epoch 244/300, seasonal_2 Loss: 0.0464 | 0.0226
Epoch 245/300, seasonal_2 Loss: 0.0458 | 0.0246
Epoch 246/300, seasonal_2 Loss: 0.0458 | 0.0238
Epoch 247/300, seasonal_2 Loss: 0.0462 | 0.0264
Epoch 248/300, seasonal_2 Loss: 0.0460 | 0.0263
Epoch 249/300, seasonal_2 Loss: 0.0458 | 0.0245
Epoch 250/300, seasonal_2 Loss: 0.0453 | 0.0247
Epoch 251/300, seasonal_2 Loss: 0.0450 | 0.0247
Epoch 252/300, seasonal_2 Loss: 0.0450 | 0.0237
Epoch 253/300, seasonal_2 Loss: 0.0450 | 0.0241
Epoch 254/300, seasonal_2 Loss: 0.0449 | 0.0243
Epoch 255/300, seasonal_2 Loss: 0.0449 | 0.0229
Epoch 256/300, seasonal_2 Loss: 0.0450 | 0.0238
Epoch 257/300, seasonal_2 Loss: 0.0453 | 0.0240
Epoch 258/300, seasonal_2 Loss: 0.0450 | 0.0236
Epoch 259/300, seasonal_2 Loss: 0.0449 | 0.0242
Epoch 260/300, seasonal_2 Loss: 0.0448 | 0.0236
Epoch 261/300, seasonal_2 Loss: 0.0448 | 0.0259
Epoch 262/300, seasonal_2 Loss: 0.0449 | 0.0254
Epoch 263/300, seasonal_2 Loss: 0.0451 | 0.0273
Epoch 264/300, seasonal_2 Loss: 0.0457 | 0.0263
Epoch 265/300, seasonal_2 Loss: 0.0456 | 0.0269
Epoch 266/300, seasonal_2 Loss: 0.0452 | 0.0272
Epoch 267/300, seasonal_2 Loss: 0.0453 | 0.0272
Epoch 268/300, seasonal_2 Loss: 0.0454 | 0.0265
Epoch 269/300, seasonal_2 Loss: 0.0454 | 0.0247
Epoch 270/300, seasonal_2 Loss: 0.0451 | 0.0255
Epoch 271/300, seasonal_2 Loss: 0.0448 | 0.0255
Epoch 272/300, seasonal_2 Loss: 0.0450 | 0.0259
Epoch 273/300, seasonal_2 Loss: 0.0448 | 0.0259
Epoch 274/300, seasonal_2 Loss: 0.0445 | 0.0252
Epoch 275/300, seasonal_2 Loss: 0.0446 | 0.0285
Epoch 276/300, seasonal_2 Loss: 0.0454 | 0.0256
Epoch 277/300, seasonal_2 Loss: 0.0454 | 0.0254
Epoch 278/300, seasonal_2 Loss: 0.0444 | 0.0267
Epoch 279/300, seasonal_2 Loss: 0.0441 | 0.0294
Epoch 280/300, seasonal_2 Loss: 0.0447 | 0.0326
Epoch 281/300, seasonal_2 Loss: 0.0454 | 0.0306
Epoch 282/300, seasonal_2 Loss: 0.0451 | 0.0262
Epoch 283/300, seasonal_2 Loss: 0.0449 | 0.0291
Epoch 284/300, seasonal_2 Loss: 0.0445 | 0.0269
Epoch 285/300, seasonal_2 Loss: 0.0437 | 0.0270
Epoch 286/300, seasonal_2 Loss: 0.0441 | 0.0280
Epoch 287/300, seasonal_2 Loss: 0.0450 | 0.0262
Epoch 288/300, seasonal_2 Loss: 0.0452 | 0.0264
Epoch 289/300, seasonal_2 Loss: 0.0445 | 0.0294
Epoch 290/300, seasonal_2 Loss: 0.0437 | 0.0291
Epoch 291/300, seasonal_2 Loss: 0.0436 | 0.0283
Epoch 292/300, seasonal_2 Loss: 0.0442 | 0.0313
Epoch 293/300, seasonal_2 Loss: 0.0439 | 0.0326
Epoch 294/300, seasonal_2 Loss: 0.0437 | 0.0289
Epoch 295/300, seasonal_2 Loss: 0.0432 | 0.0282
Epoch 296/300, seasonal_2 Loss: 0.0430 | 0.0283
Epoch 297/300, seasonal_2 Loss: 0.0431 | 0.0287
Epoch 298/300, seasonal_2 Loss: 0.0434 | 0.0293
Epoch 299/300, seasonal_2 Loss: 0.0430 | 0.0283
Epoch 300/300, seasonal_2 Loss: 0.0427 | 0.0308
Training seasonal_3 component with params: {'observation_period_num': 7, 'train_rates': 0.9255758667521732, 'learning_rate': 0.00012144616340285131, 'batch_size': 34, 'step_size': 3, 'gamma': 0.9601827262900791}
Epoch 1/300, seasonal_3 Loss: 0.2662 | 0.1446
Epoch 2/300, seasonal_3 Loss: 0.1299 | 0.1213
Epoch 3/300, seasonal_3 Loss: 0.1139 | 0.1000
Epoch 4/300, seasonal_3 Loss: 0.1066 | 0.0872
Epoch 5/300, seasonal_3 Loss: 0.1031 | 0.0796
Epoch 6/300, seasonal_3 Loss: 0.1003 | 0.0742
Epoch 7/300, seasonal_3 Loss: 0.0979 | 0.0707
Epoch 8/300, seasonal_3 Loss: 0.0959 | 0.0683
Epoch 9/300, seasonal_3 Loss: 0.0935 | 0.0657
Epoch 10/300, seasonal_3 Loss: 0.0911 | 0.0624
Epoch 11/300, seasonal_3 Loss: 0.0884 | 0.0589
Epoch 12/300, seasonal_3 Loss: 0.0857 | 0.0559
Epoch 13/300, seasonal_3 Loss: 0.0836 | 0.0529
Epoch 14/300, seasonal_3 Loss: 0.0819 | 0.0510
Epoch 15/300, seasonal_3 Loss: 0.0806 | 0.0489
Epoch 16/300, seasonal_3 Loss: 0.0795 | 0.0472
Epoch 17/300, seasonal_3 Loss: 0.0786 | 0.0462
Epoch 18/300, seasonal_3 Loss: 0.0776 | 0.0448
Epoch 19/300, seasonal_3 Loss: 0.0765 | 0.0434
Epoch 20/300, seasonal_3 Loss: 0.0754 | 0.0423
Epoch 21/300, seasonal_3 Loss: 0.0744 | 0.0411
Epoch 22/300, seasonal_3 Loss: 0.0733 | 0.0400
Epoch 23/300, seasonal_3 Loss: 0.0725 | 0.0391
Epoch 24/300, seasonal_3 Loss: 0.0716 | 0.0382
Epoch 25/300, seasonal_3 Loss: 0.0709 | 0.0374
Epoch 26/300, seasonal_3 Loss: 0.0704 | 0.0370
Epoch 27/300, seasonal_3 Loss: 0.0698 | 0.0365
Epoch 28/300, seasonal_3 Loss: 0.0693 | 0.0362
Epoch 29/300, seasonal_3 Loss: 0.0689 | 0.0358
Epoch 30/300, seasonal_3 Loss: 0.0684 | 0.0355
Epoch 31/300, seasonal_3 Loss: 0.0680 | 0.0351
Epoch 32/300, seasonal_3 Loss: 0.0677 | 0.0349
Epoch 33/300, seasonal_3 Loss: 0.0673 | 0.0346
Epoch 34/300, seasonal_3 Loss: 0.0669 | 0.0343
Epoch 35/300, seasonal_3 Loss: 0.0666 | 0.0340
Epoch 36/300, seasonal_3 Loss: 0.0663 | 0.0338
Epoch 37/300, seasonal_3 Loss: 0.0660 | 0.0336
Epoch 38/300, seasonal_3 Loss: 0.0657 | 0.0334
Epoch 39/300, seasonal_3 Loss: 0.0654 | 0.0332
Epoch 40/300, seasonal_3 Loss: 0.0651 | 0.0330
Epoch 41/300, seasonal_3 Loss: 0.0649 | 0.0329
Epoch 42/300, seasonal_3 Loss: 0.0647 | 0.0326
Epoch 43/300, seasonal_3 Loss: 0.0644 | 0.0324
Epoch 44/300, seasonal_3 Loss: 0.0642 | 0.0323
Epoch 45/300, seasonal_3 Loss: 0.0640 | 0.0321
Epoch 46/300, seasonal_3 Loss: 0.0638 | 0.0319
Epoch 47/300, seasonal_3 Loss: 0.0637 | 0.0318
Epoch 48/300, seasonal_3 Loss: 0.0635 | 0.0316
Epoch 49/300, seasonal_3 Loss: 0.0633 | 0.0315
Epoch 50/300, seasonal_3 Loss: 0.0631 | 0.0314
Epoch 51/300, seasonal_3 Loss: 0.0630 | 0.0312
Epoch 52/300, seasonal_3 Loss: 0.0628 | 0.0312
Epoch 53/300, seasonal_3 Loss: 0.0627 | 0.0313
Epoch 54/300, seasonal_3 Loss: 0.0628 | 0.0314
Epoch 55/300, seasonal_3 Loss: 0.0630 | 0.0309
Epoch 56/300, seasonal_3 Loss: 0.0627 | 0.0308
Epoch 57/300, seasonal_3 Loss: 0.0624 | 0.0305
Epoch 58/300, seasonal_3 Loss: 0.0622 | 0.0304
Epoch 59/300, seasonal_3 Loss: 0.0620 | 0.0303
Epoch 60/300, seasonal_3 Loss: 0.0618 | 0.0302
Epoch 61/300, seasonal_3 Loss: 0.0616 | 0.0300
Epoch 62/300, seasonal_3 Loss: 0.0615 | 0.0300
Epoch 63/300, seasonal_3 Loss: 0.0614 | 0.0298
Epoch 64/300, seasonal_3 Loss: 0.0613 | 0.0297
Epoch 65/300, seasonal_3 Loss: 0.0613 | 0.0297
Epoch 66/300, seasonal_3 Loss: 0.0612 | 0.0296
Epoch 67/300, seasonal_3 Loss: 0.0611 | 0.0296
Epoch 68/300, seasonal_3 Loss: 0.0610 | 0.0295
Epoch 69/300, seasonal_3 Loss: 0.0609 | 0.0295
Epoch 70/300, seasonal_3 Loss: 0.0608 | 0.0295
Epoch 71/300, seasonal_3 Loss: 0.0606 | 0.0294
Epoch 72/300, seasonal_3 Loss: 0.0604 | 0.0293
Epoch 73/300, seasonal_3 Loss: 0.0603 | 0.0292
Epoch 74/300, seasonal_3 Loss: 0.0602 | 0.0291
Epoch 75/300, seasonal_3 Loss: 0.0601 | 0.0290
Epoch 76/300, seasonal_3 Loss: 0.0600 | 0.0289
Epoch 77/300, seasonal_3 Loss: 0.0599 | 0.0289
Epoch 78/300, seasonal_3 Loss: 0.0598 | 0.0288
Epoch 79/300, seasonal_3 Loss: 0.0597 | 0.0288
Epoch 80/300, seasonal_3 Loss: 0.0596 | 0.0287
Epoch 81/300, seasonal_3 Loss: 0.0595 | 0.0287
Epoch 82/300, seasonal_3 Loss: 0.0595 | 0.0287
Epoch 83/300, seasonal_3 Loss: 0.0594 | 0.0287
Epoch 84/300, seasonal_3 Loss: 0.0593 | 0.0287
Epoch 85/300, seasonal_3 Loss: 0.0593 | 0.0287
Epoch 86/300, seasonal_3 Loss: 0.0592 | 0.0287
Epoch 87/300, seasonal_3 Loss: 0.0591 | 0.0287
Epoch 88/300, seasonal_3 Loss: 0.0591 | 0.0288
Epoch 89/300, seasonal_3 Loss: 0.0590 | 0.0287
Epoch 90/300, seasonal_3 Loss: 0.0589 | 0.0288
Epoch 91/300, seasonal_3 Loss: 0.0589 | 0.0288
Epoch 92/300, seasonal_3 Loss: 0.0588 | 0.0287
Epoch 93/300, seasonal_3 Loss: 0.0588 | 0.0288
Epoch 94/300, seasonal_3 Loss: 0.0587 | 0.0288
Epoch 95/300, seasonal_3 Loss: 0.0587 | 0.0287
Epoch 96/300, seasonal_3 Loss: 0.0586 | 0.0287
Epoch 97/300, seasonal_3 Loss: 0.0586 | 0.0287
Epoch 98/300, seasonal_3 Loss: 0.0585 | 0.0287
Epoch 99/300, seasonal_3 Loss: 0.0585 | 0.0287
Epoch 100/300, seasonal_3 Loss: 0.0584 | 0.0286
Epoch 101/300, seasonal_3 Loss: 0.0584 | 0.0286
Epoch 102/300, seasonal_3 Loss: 0.0584 | 0.0286
Epoch 103/300, seasonal_3 Loss: 0.0583 | 0.0285
Epoch 104/300, seasonal_3 Loss: 0.0583 | 0.0285
Epoch 105/300, seasonal_3 Loss: 0.0582 | 0.0284
Epoch 106/300, seasonal_3 Loss: 0.0582 | 0.0284
Epoch 107/300, seasonal_3 Loss: 0.0582 | 0.0283
Epoch 108/300, seasonal_3 Loss: 0.0581 | 0.0283
Epoch 109/300, seasonal_3 Loss: 0.0581 | 0.0283
Epoch 110/300, seasonal_3 Loss: 0.0581 | 0.0282
Epoch 111/300, seasonal_3 Loss: 0.0580 | 0.0282
Epoch 112/300, seasonal_3 Loss: 0.0580 | 0.0281
Epoch 113/300, seasonal_3 Loss: 0.0579 | 0.0281
Epoch 114/300, seasonal_3 Loss: 0.0579 | 0.0281
Epoch 115/300, seasonal_3 Loss: 0.0579 | 0.0280
Epoch 116/300, seasonal_3 Loss: 0.0578 | 0.0280
Epoch 117/300, seasonal_3 Loss: 0.0578 | 0.0280
Epoch 118/300, seasonal_3 Loss: 0.0578 | 0.0279
Epoch 119/300, seasonal_3 Loss: 0.0577 | 0.0279
Epoch 120/300, seasonal_3 Loss: 0.0577 | 0.0279
Epoch 121/300, seasonal_3 Loss: 0.0577 | 0.0278
Epoch 122/300, seasonal_3 Loss: 0.0577 | 0.0278
Epoch 123/300, seasonal_3 Loss: 0.0576 | 0.0278
Epoch 124/300, seasonal_3 Loss: 0.0576 | 0.0278
Epoch 125/300, seasonal_3 Loss: 0.0576 | 0.0278
Epoch 126/300, seasonal_3 Loss: 0.0576 | 0.0277
Epoch 127/300, seasonal_3 Loss: 0.0575 | 0.0277
Epoch 128/300, seasonal_3 Loss: 0.0575 | 0.0277
Epoch 129/300, seasonal_3 Loss: 0.0575 | 0.0277
Epoch 130/300, seasonal_3 Loss: 0.0575 | 0.0277
Epoch 131/300, seasonal_3 Loss: 0.0575 | 0.0277
Epoch 132/300, seasonal_3 Loss: 0.0574 | 0.0276
Epoch 133/300, seasonal_3 Loss: 0.0574 | 0.0276
Epoch 134/300, seasonal_3 Loss: 0.0574 | 0.0276
Epoch 135/300, seasonal_3 Loss: 0.0574 | 0.0276
Epoch 136/300, seasonal_3 Loss: 0.0573 | 0.0276
Epoch 137/300, seasonal_3 Loss: 0.0573 | 0.0276
Epoch 138/300, seasonal_3 Loss: 0.0573 | 0.0276
Epoch 139/300, seasonal_3 Loss: 0.0573 | 0.0276
Epoch 140/300, seasonal_3 Loss: 0.0573 | 0.0276
Epoch 141/300, seasonal_3 Loss: 0.0572 | 0.0276
Epoch 142/300, seasonal_3 Loss: 0.0572 | 0.0276
Epoch 143/300, seasonal_3 Loss: 0.0572 | 0.0276
Epoch 144/300, seasonal_3 Loss: 0.0572 | 0.0276
Epoch 145/300, seasonal_3 Loss: 0.0572 | 0.0276
Epoch 146/300, seasonal_3 Loss: 0.0572 | 0.0276
Epoch 147/300, seasonal_3 Loss: 0.0571 | 0.0276
Epoch 148/300, seasonal_3 Loss: 0.0571 | 0.0276
Epoch 149/300, seasonal_3 Loss: 0.0571 | 0.0276
Epoch 150/300, seasonal_3 Loss: 0.0571 | 0.0276
Epoch 151/300, seasonal_3 Loss: 0.0571 | 0.0276
Epoch 152/300, seasonal_3 Loss: 0.0571 | 0.0276
Epoch 153/300, seasonal_3 Loss: 0.0571 | 0.0276
Epoch 154/300, seasonal_3 Loss: 0.0571 | 0.0276
Epoch 155/300, seasonal_3 Loss: 0.0571 | 0.0276
Epoch 156/300, seasonal_3 Loss: 0.0571 | 0.0276
Epoch 157/300, seasonal_3 Loss: 0.0571 | 0.0275
Epoch 158/300, seasonal_3 Loss: 0.0571 | 0.0275
Epoch 159/300, seasonal_3 Loss: 0.0570 | 0.0275
Epoch 160/300, seasonal_3 Loss: 0.0570 | 0.0275
Epoch 161/300, seasonal_3 Loss: 0.0570 | 0.0275
Epoch 162/300, seasonal_3 Loss: 0.0570 | 0.0275
Epoch 163/300, seasonal_3 Loss: 0.0570 | 0.0275
Epoch 164/300, seasonal_3 Loss: 0.0570 | 0.0275
Epoch 165/300, seasonal_3 Loss: 0.0570 | 0.0275
Epoch 166/300, seasonal_3 Loss: 0.0569 | 0.0275
Epoch 167/300, seasonal_3 Loss: 0.0569 | 0.0274
Epoch 168/300, seasonal_3 Loss: 0.0569 | 0.0274
Epoch 169/300, seasonal_3 Loss: 0.0569 | 0.0274
Epoch 170/300, seasonal_3 Loss: 0.0569 | 0.0274
Epoch 171/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 172/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 173/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 174/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 175/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 176/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 177/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 178/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 179/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 180/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 181/300, seasonal_3 Loss: 0.0567 | 0.0274
Epoch 182/300, seasonal_3 Loss: 0.0567 | 0.0274
Epoch 183/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 184/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 185/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 186/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 187/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 188/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 189/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 190/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 191/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 192/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 193/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 194/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 195/300, seasonal_3 Loss: 0.0567 | 0.0273
Epoch 196/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 197/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 198/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 199/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 200/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 201/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 202/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 203/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 204/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 205/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 206/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 207/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 208/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 209/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 210/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 211/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 212/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 213/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 214/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 215/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 216/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 217/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 218/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 219/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 220/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 221/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 222/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 223/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 224/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 225/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 226/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 227/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 228/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 229/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 230/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 231/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 232/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 233/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 234/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 235/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 236/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 237/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 238/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 239/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 240/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 241/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 242/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 243/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 244/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 245/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 246/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 247/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 248/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 249/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 250/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 251/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 252/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 253/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 254/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 255/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 256/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 257/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 258/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 259/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 260/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 261/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 262/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 263/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 264/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 265/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 266/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 267/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 268/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 269/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 270/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 271/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 272/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 273/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 274/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 275/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 276/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 277/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 278/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 279/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 280/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 281/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 282/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 283/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 284/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 285/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 286/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 287/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 288/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 289/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 290/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 291/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 292/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 293/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 294/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 295/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 296/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 297/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 298/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 299/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 300/300, seasonal_3 Loss: 0.0565 | 0.0272
Training resid component with params: {'observation_period_num': 18, 'train_rates': 0.9848453067038433, 'learning_rate': 0.0004657100769993278, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9834035982917502}
Epoch 1/300, resid Loss: 0.1724 | 0.1255
Epoch 2/300, resid Loss: 0.1075 | 0.0731
Epoch 3/300, resid Loss: 0.0993 | 0.0791
Epoch 4/300, resid Loss: 0.0928 | 0.0651
Epoch 5/300, resid Loss: 0.0858 | 0.0696
Epoch 6/300, resid Loss: 0.0837 | 0.0637
Epoch 7/300, resid Loss: 0.0814 | 0.0623
Epoch 8/300, resid Loss: 0.0796 | 0.0565
Epoch 9/300, resid Loss: 0.0776 | 0.0554
Epoch 10/300, resid Loss: 0.0762 | 0.0515
Epoch 11/300, resid Loss: 0.0708 | 0.0547
Epoch 12/300, resid Loss: 0.0709 | 0.0547
Epoch 13/300, resid Loss: 0.0695 | 0.0526
Epoch 14/300, resid Loss: 0.0663 | 0.0503
Epoch 15/300, resid Loss: 0.0644 | 0.0505
Epoch 16/300, resid Loss: 0.0714 | 0.0684
Epoch 17/300, resid Loss: 0.0673 | 0.0466
Epoch 18/300, resid Loss: 0.0645 | 0.0400
Epoch 19/300, resid Loss: 0.0610 | 0.0407
Epoch 20/300, resid Loss: 0.0624 | 0.0402
Epoch 21/300, resid Loss: 0.0607 | 0.0357
Epoch 22/300, resid Loss: 0.0571 | 0.0387
Epoch 23/300, resid Loss: 0.0574 | 0.0305
Epoch 24/300, resid Loss: 0.0563 | 0.0298
Epoch 25/300, resid Loss: 0.0548 | 0.0411
Epoch 26/300, resid Loss: 0.0551 | 0.0424
Epoch 27/300, resid Loss: 0.0538 | 0.0370
Epoch 28/300, resid Loss: 0.0538 | 0.0464
Epoch 29/300, resid Loss: 0.0546 | 0.0402
Epoch 30/300, resid Loss: 0.0548 | 0.0574
Epoch 31/300, resid Loss: 0.0510 | 0.0582
Epoch 32/300, resid Loss: 0.0503 | 0.0562
Epoch 33/300, resid Loss: 0.0493 | 0.0753
Epoch 34/300, resid Loss: 0.0489 | 0.0674
Epoch 35/300, resid Loss: 0.0486 | 0.0928
Epoch 36/300, resid Loss: 0.0495 | 0.0733
Epoch 37/300, resid Loss: 0.0494 | 0.0641
Epoch 38/300, resid Loss: 0.0490 | 0.1037
Epoch 39/300, resid Loss: 0.0477 | 0.0390
Epoch 40/300, resid Loss: 0.0464 | 0.0476
Epoch 41/300, resid Loss: 0.0462 | 0.0701
Epoch 42/300, resid Loss: 0.0441 | 0.0501
Epoch 43/300, resid Loss: 0.0471 | 0.0546
Epoch 44/300, resid Loss: 0.0497 | 0.0579
Epoch 45/300, resid Loss: 0.0469 | 0.0358
Epoch 46/300, resid Loss: 0.0467 | 0.0328
Epoch 47/300, resid Loss: 0.0440 | 0.0347
Epoch 48/300, resid Loss: 0.0435 | 0.0458
Epoch 49/300, resid Loss: 0.0419 | 0.0544
Epoch 50/300, resid Loss: 0.0378 | 0.0578
Epoch 51/300, resid Loss: 0.0353 | 0.0650
Epoch 52/300, resid Loss: 0.0348 | 0.0474
Epoch 53/300, resid Loss: 0.0415 | 0.0389
Epoch 54/300, resid Loss: 0.0406 | 0.0493
Epoch 55/300, resid Loss: 0.0396 | 0.0531
Epoch 56/300, resid Loss: 0.0362 | 0.0643
Epoch 57/300, resid Loss: 0.0353 | 0.0663
Epoch 58/300, resid Loss: 0.0429 | 0.0525
Epoch 59/300, resid Loss: 0.0436 | 0.0509
Epoch 60/300, resid Loss: 0.0344 | 0.0722
Epoch 61/300, resid Loss: 0.0323 | 0.0721
Epoch 62/300, resid Loss: 0.0374 | 0.1026
Epoch 63/300, resid Loss: 0.0405 | 0.0556
Epoch 64/300, resid Loss: 0.0411 | 0.0395
Epoch 65/300, resid Loss: 0.0416 | 0.0600
Epoch 66/300, resid Loss: 0.0384 | 0.0368
Epoch 67/300, resid Loss: 0.0340 | 0.0385
Epoch 68/300, resid Loss: 0.0308 | 0.0424
Epoch 69/300, resid Loss: 0.0296 | 0.0551
Epoch 70/300, resid Loss: 0.0308 | 0.0531
Epoch 71/300, resid Loss: 0.0322 | 0.0583
Epoch 72/300, resid Loss: 0.0308 | 0.0722
Epoch 73/300, resid Loss: 0.0286 | 0.0498
Epoch 74/300, resid Loss: 0.0274 | 0.0455
Epoch 75/300, resid Loss: 0.0267 | 0.0434
Epoch 76/300, resid Loss: 0.0264 | 0.0431
Epoch 77/300, resid Loss: 0.0260 | 0.0490
Epoch 78/300, resid Loss: 0.0258 | 0.0444
Epoch 79/300, resid Loss: 0.0261 | 0.0473
Epoch 80/300, resid Loss: 0.0272 | 0.0475
Epoch 81/300, resid Loss: 0.0260 | 0.0427
Epoch 82/300, resid Loss: 0.0258 | 0.0381
Epoch 83/300, resid Loss: 0.0250 | 0.0420
Epoch 84/300, resid Loss: 0.0243 | 0.0443
Epoch 85/300, resid Loss: 0.0241 | 0.0420
Epoch 86/300, resid Loss: 0.0244 | 0.0486
Epoch 87/300, resid Loss: 0.0244 | 0.0502
Epoch 88/300, resid Loss: 0.0246 | 0.0478
Epoch 89/300, resid Loss: 0.0260 | 0.0594
Epoch 90/300, resid Loss: 0.0275 | 0.0496
Epoch 91/300, resid Loss: 0.0264 | 0.0407
Epoch 92/300, resid Loss: 0.0240 | 0.0454
Epoch 93/300, resid Loss: 0.0232 | 0.0550
Epoch 94/300, resid Loss: 0.0235 | 0.0523
Epoch 95/300, resid Loss: 0.0238 | 0.0503
Epoch 96/300, resid Loss: 0.0230 | 0.0608
Epoch 97/300, resid Loss: 0.0225 | 0.0723
Epoch 98/300, resid Loss: 0.0221 | 0.0737
Epoch 99/300, resid Loss: 0.0224 | 0.0613
Epoch 100/300, resid Loss: 0.0228 | 0.0418
Epoch 101/300, resid Loss: 0.0216 | 0.0425
Epoch 102/300, resid Loss: 0.0212 | 0.0406
Epoch 103/300, resid Loss: 0.0209 | 0.0410
Epoch 104/300, resid Loss: 0.0358 | 0.0524
Epoch 105/300, resid Loss: 0.0349 | 0.0689
Epoch 106/300, resid Loss: 0.0492 | 0.0402
Epoch 107/300, resid Loss: 0.0246 | 0.0443
Epoch 108/300, resid Loss: 0.0219 | 0.0466
Epoch 109/300, resid Loss: 0.0208 | 0.0456
Epoch 110/300, resid Loss: 0.0205 | 0.0422
Epoch 111/300, resid Loss: 0.0204 | 0.0432
Epoch 112/300, resid Loss: 0.0200 | 0.0546
Epoch 113/300, resid Loss: 0.0196 | 0.0610
Epoch 114/300, resid Loss: 0.0191 | 0.0630
Epoch 115/300, resid Loss: 0.0186 | 0.0622
Epoch 116/300, resid Loss: 0.0183 | 0.0593
Epoch 117/300, resid Loss: 0.0179 | 0.0544
Epoch 118/300, resid Loss: 0.0177 | 0.0492
Epoch 119/300, resid Loss: 0.0174 | 0.0468
Epoch 120/300, resid Loss: 0.0172 | 0.0436
Epoch 121/300, resid Loss: 0.0170 | 0.0444
Epoch 122/300, resid Loss: 0.0168 | 0.0425
Epoch 123/300, resid Loss: 0.0186 | 0.0420
Epoch 124/300, resid Loss: 0.0233 | 0.0440
Epoch 125/300, resid Loss: 0.0256 | 0.0478
Epoch 126/300, resid Loss: 0.0199 | 0.0463
Epoch 127/300, resid Loss: 0.0192 | 0.0550
Epoch 128/300, resid Loss: 0.0347 | 0.0468
Epoch 129/300, resid Loss: 0.0201 | 0.0477
Epoch 130/300, resid Loss: 0.0175 | 0.0492
Epoch 131/300, resid Loss: 0.0166 | 0.0492
Epoch 132/300, resid Loss: 0.0162 | 0.0503
Epoch 133/300, resid Loss: 0.0159 | 0.0498
Epoch 134/300, resid Loss: 0.0157 | 0.0490
Epoch 135/300, resid Loss: 0.0155 | 0.0481
Epoch 136/300, resid Loss: 0.0153 | 0.0476
Epoch 137/300, resid Loss: 0.0152 | 0.0463
Epoch 138/300, resid Loss: 0.0150 | 0.0455
Epoch 139/300, resid Loss: 0.0149 | 0.0448
Epoch 140/300, resid Loss: 0.0147 | 0.0442
Epoch 141/300, resid Loss: 0.0146 | 0.0445
Epoch 142/300, resid Loss: 0.0145 | 0.0434
Epoch 143/300, resid Loss: 0.0145 | 0.0451
Epoch 144/300, resid Loss: 0.0144 | 0.0443
Epoch 145/300, resid Loss: 0.0145 | 0.0440
Epoch 146/300, resid Loss: 0.0144 | 0.0404
Epoch 147/300, resid Loss: 0.0143 | 0.0465
Epoch 148/300, resid Loss: 0.0144 | 0.0430
Epoch 149/300, resid Loss: 0.0140 | 0.0465
Epoch 150/300, resid Loss: 0.0138 | 0.0444
Epoch 151/300, resid Loss: 0.0138 | 0.0448
Epoch 152/300, resid Loss: 0.0137 | 0.0448
Epoch 153/300, resid Loss: 0.0139 | 0.0502
Epoch 154/300, resid Loss: 0.0138 | 0.0488
Epoch 155/300, resid Loss: 0.0135 | 0.0716
Epoch 156/300, resid Loss: 0.0138 | 0.0698
Epoch 157/300, resid Loss: 0.0137 | 0.0859
Epoch 158/300, resid Loss: 0.0139 | 0.0490
Epoch 159/300, resid Loss: 0.0135 | 0.0465
Epoch 160/300, resid Loss: 0.0133 | 0.0442
Epoch 161/300, resid Loss: 0.0131 | 0.0438
Epoch 162/300, resid Loss: 0.0129 | 0.0434
Epoch 163/300, resid Loss: 0.0126 | 0.0440
Epoch 164/300, resid Loss: 0.0124 | 0.0438
Epoch 165/300, resid Loss: 0.0123 | 0.0440
Epoch 166/300, resid Loss: 0.0124 | 0.0431
Epoch 167/300, resid Loss: 0.0124 | 0.0430
Epoch 168/300, resid Loss: 0.0123 | 0.0430
Epoch 169/300, resid Loss: 0.0120 | 0.0441
Epoch 170/300, resid Loss: 0.0119 | 0.0443
Epoch 171/300, resid Loss: 0.0117 | 0.0445
Epoch 172/300, resid Loss: 0.0116 | 0.0438
Epoch 173/300, resid Loss: 0.0116 | 0.0448
Epoch 174/300, resid Loss: 0.0115 | 0.0452
Epoch 175/300, resid Loss: 0.0116 | 0.0441
Epoch 176/300, resid Loss: 0.0115 | 0.0440
Epoch 177/300, resid Loss: 0.0115 | 0.0473
Epoch 178/300, resid Loss: 0.0112 | 0.0470
Epoch 179/300, resid Loss: 0.0111 | 0.0491
Epoch 180/300, resid Loss: 0.0110 | 0.0554
Epoch 181/300, resid Loss: 0.0110 | 0.0589
Epoch 182/300, resid Loss: 0.0110 | 0.0581
Epoch 183/300, resid Loss: 0.0108 | 0.0542
Epoch 184/300, resid Loss: 0.0107 | 0.0608
Epoch 185/300, resid Loss: 0.0106 | 0.0590
Epoch 186/300, resid Loss: 0.0106 | 0.0572
Epoch 187/300, resid Loss: 0.0107 | 0.0498
Epoch 188/300, resid Loss: 0.0106 | 0.0468
Epoch 189/300, resid Loss: 0.0105 | 0.0431
Epoch 190/300, resid Loss: 0.0105 | 0.0427
Epoch 191/300, resid Loss: 0.0105 | 0.0410
Epoch 192/300, resid Loss: 0.0104 | 0.0406
Epoch 193/300, resid Loss: 0.0102 | 0.0413
Epoch 194/300, resid Loss: 0.0101 | 0.0405
Epoch 195/300, resid Loss: 0.0100 | 0.0416
Epoch 196/300, resid Loss: 0.0100 | 0.0416
Epoch 197/300, resid Loss: 0.0099 | 0.0425
Epoch 198/300, resid Loss: 0.0097 | 0.0423
Epoch 199/300, resid Loss: 0.0096 | 0.0433
Epoch 200/300, resid Loss: 0.0096 | 0.0432
Epoch 201/300, resid Loss: 0.0096 | 0.0443
Epoch 202/300, resid Loss: 0.0097 | 0.0439
Epoch 203/300, resid Loss: 0.0095 | 0.0451
Epoch 204/300, resid Loss: 0.0096 | 0.0435
Epoch 205/300, resid Loss: 0.0094 | 0.0443
Epoch 206/300, resid Loss: 0.0093 | 0.0444
Epoch 207/300, resid Loss: 0.0092 | 0.0443
Epoch 208/300, resid Loss: 0.0091 | 0.0442
Epoch 209/300, resid Loss: 0.0090 | 0.0445
Epoch 210/300, resid Loss: 0.0091 | 0.0440
Epoch 211/300, resid Loss: 0.0091 | 0.0434
Epoch 212/300, resid Loss: 0.0090 | 0.0440
Epoch 213/300, resid Loss: 0.0088 | 0.0422
Epoch 214/300, resid Loss: 0.0088 | 0.0432
Epoch 215/300, resid Loss: 0.0087 | 0.0419
Epoch 216/300, resid Loss: 0.0088 | 0.0409
Epoch 217/300, resid Loss: 0.0088 | 0.0419
Epoch 218/300, resid Loss: 0.0086 | 0.0412
Epoch 219/300, resid Loss: 0.0086 | 0.0404
Epoch 220/300, resid Loss: 0.0086 | 0.0410
Epoch 221/300, resid Loss: 0.0086 | 0.0401
Epoch 222/300, resid Loss: 0.0086 | 0.0404
Epoch 223/300, resid Loss: 0.0085 | 0.0423
Epoch 224/300, resid Loss: 0.0085 | 0.0417
Epoch 225/300, resid Loss: 0.0084 | 0.0422
Epoch 226/300, resid Loss: 0.0084 | 0.0438
Epoch 227/300, resid Loss: 0.0085 | 0.0437
Epoch 228/300, resid Loss: 0.0085 | 0.0432
Epoch 229/300, resid Loss: 0.0084 | 0.0436
Epoch 230/300, resid Loss: 0.0083 | 0.0449
Epoch 231/300, resid Loss: 0.0083 | 0.0501
Epoch 232/300, resid Loss: 0.0083 | 0.0570
Epoch 233/300, resid Loss: 0.0083 | 0.0743
Epoch 234/300, resid Loss: 0.0083 | 0.0871
Epoch 235/300, resid Loss: 0.0083 | 0.0689
Epoch 236/300, resid Loss: 0.0085 | 0.0422
Epoch 237/300, resid Loss: 0.0084 | 0.0416
Epoch 238/300, resid Loss: 0.0080 | 0.0403
Epoch 239/300, resid Loss: 0.0079 | 0.0408
Epoch 240/300, resid Loss: 0.0078 | 0.0399
Epoch 241/300, resid Loss: 0.0077 | 0.0413
Epoch 242/300, resid Loss: 0.0076 | 0.0404
Epoch 243/300, resid Loss: 0.0076 | 0.0415
Epoch 244/300, resid Loss: 0.0076 | 0.0409
Epoch 245/300, resid Loss: 0.0076 | 0.0418
Epoch 246/300, resid Loss: 0.0076 | 0.0412
Epoch 247/300, resid Loss: 0.0077 | 0.0411
Epoch 248/300, resid Loss: 0.0076 | 0.0407
Epoch 249/300, resid Loss: 0.0075 | 0.0412
Epoch 250/300, resid Loss: 0.0074 | 0.0410
Epoch 251/300, resid Loss: 0.0073 | 0.0412
Epoch 252/300, resid Loss: 0.0073 | 0.0414
Epoch 253/300, resid Loss: 0.0072 | 0.0407
Epoch 254/300, resid Loss: 0.0072 | 0.0412
Epoch 255/300, resid Loss: 0.0072 | 0.0412
Epoch 256/300, resid Loss: 0.0071 | 0.0410
Epoch 257/300, resid Loss: 0.0073 | 0.0409
Epoch 258/300, resid Loss: 0.0072 | 0.0401
Epoch 259/300, resid Loss: 0.0072 | 0.0406
Epoch 260/300, resid Loss: 0.0070 | 0.0404
Epoch 261/300, resid Loss: 0.0070 | 0.0408
Epoch 262/300, resid Loss: 0.0070 | 0.0407
Epoch 263/300, resid Loss: 0.0070 | 0.0407
Epoch 264/300, resid Loss: 0.0070 | 0.0411
Epoch 265/300, resid Loss: 0.0070 | 0.0414
Epoch 266/300, resid Loss: 0.0069 | 0.0413
Epoch 267/300, resid Loss: 0.0068 | 0.0419
Epoch 268/300, resid Loss: 0.0069 | 0.0415
Epoch 269/300, resid Loss: 0.0069 | 0.0417
Epoch 270/300, resid Loss: 0.0068 | 0.0421
Epoch 271/300, resid Loss: 0.0067 | 0.0421
Epoch 272/300, resid Loss: 0.0067 | 0.0418
Epoch 273/300, resid Loss: 0.0067 | 0.0418
Epoch 274/300, resid Loss: 0.0067 | 0.0425
Epoch 275/300, resid Loss: 0.0067 | 0.0420
Epoch 276/300, resid Loss: 0.0066 | 0.0422
Epoch 277/300, resid Loss: 0.0066 | 0.0418
Epoch 278/300, resid Loss: 0.0066 | 0.0418
Epoch 279/300, resid Loss: 0.0066 | 0.0418
Epoch 280/300, resid Loss: 0.0065 | 0.0420
Epoch 281/300, resid Loss: 0.0065 | 0.0415
Epoch 282/300, resid Loss: 0.0065 | 0.0417
Epoch 283/300, resid Loss: 0.0065 | 0.0416
Epoch 284/300, resid Loss: 0.0064 | 0.0421
Epoch 285/300, resid Loss: 0.0064 | 0.0420
Epoch 286/300, resid Loss: 0.0064 | 0.0425
Epoch 287/300, resid Loss: 0.0064 | 0.0425
Epoch 288/300, resid Loss: 0.0064 | 0.0433
Epoch 289/300, resid Loss: 0.0063 | 0.0432
Epoch 290/300, resid Loss: 0.0063 | 0.0444
Epoch 291/300, resid Loss: 0.0063 | 0.0448
Epoch 292/300, resid Loss: 0.0063 | 0.0451
Epoch 293/300, resid Loss: 0.0063 | 0.0440
Epoch 294/300, resid Loss: 0.0063 | 0.0439
Epoch 295/300, resid Loss: 0.0063 | 0.0446
Epoch 296/300, resid Loss: 0.0063 | 0.0472
Epoch 297/300, resid Loss: 0.0063 | 0.0479
Epoch 298/300, resid Loss: 0.0063 | 0.0457
Epoch 299/300, resid Loss: 0.0063 | 0.0423
Epoch 300/300, resid Loss: 0.0063 | 0.0420
Runtime (seconds): 3419.96347284317
0.00013662188894747836
[154.71071]
[-1.1566328]
[-5.20173]
[9.843188]
[0.9177271]
[5.7000365]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 33.79260676447302
RMSE: 5.813140869140625
MAE: 5.813140869140625
R-squared: nan
[164.8133]
