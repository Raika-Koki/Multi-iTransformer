[32m[I 2025-02-07 13:11:50,758][0m A new study created in memory with name: no-name-f5d09267-c437-45b2-939e-98149a2bf217[0m
[32m[I 2025-02-07 13:12:40,840][0m Trial 0 finished with value: 0.43407727382987377 and parameters: {'observation_period_num': 210, 'train_rates': 0.9408144164971858, 'learning_rate': 2.2196490953740365e-05, 'batch_size': 119, 'step_size': 5, 'gamma': 0.8279530755251547}. Best is trial 0 with value: 0.43407727382987377.[0m
[32m[I 2025-02-07 13:14:45,419][0m Trial 1 finished with value: 0.2954783992213168 and parameters: {'observation_period_num': 180, 'train_rates': 0.7172801206591747, 'learning_rate': 0.0007523959994266412, 'batch_size': 37, 'step_size': 13, 'gamma': 0.950060135980425}. Best is trial 1 with value: 0.2954783992213168.[0m
[32m[I 2025-02-07 13:15:18,418][0m Trial 2 finished with value: 0.3900234133005142 and parameters: {'observation_period_num': 59, 'train_rates': 0.9252685593644708, 'learning_rate': 3.550228601811018e-05, 'batch_size': 198, 'step_size': 4, 'gamma': 0.7613520681224992}. Best is trial 1 with value: 0.2954783992213168.[0m
[32m[I 2025-02-07 13:15:57,828][0m Trial 3 finished with value: 0.5023745325579546 and parameters: {'observation_period_num': 232, 'train_rates': 0.8062528575709726, 'learning_rate': 6.4371199088831246e-06, 'batch_size': 139, 'step_size': 15, 'gamma': 0.7869429537022927}. Best is trial 1 with value: 0.2954783992213168.[0m
[32m[I 2025-02-07 13:17:14,191][0m Trial 4 finished with value: 0.20670529833770201 and parameters: {'observation_period_num': 76, 'train_rates': 0.77752764347528, 'learning_rate': 0.00013219016888660063, 'batch_size': 68, 'step_size': 10, 'gamma': 0.8571655452598479}. Best is trial 4 with value: 0.20670529833770201.[0m
[32m[I 2025-02-07 13:17:40,196][0m Trial 5 finished with value: 0.6834378200337198 and parameters: {'observation_period_num': 191, 'train_rates': 0.60237328468887, 'learning_rate': 1.024469037959831e-05, 'batch_size': 174, 'step_size': 1, 'gamma': 0.9651724875437612}. Best is trial 4 with value: 0.20670529833770201.[0m
[32m[I 2025-02-07 13:18:05,056][0m Trial 6 finished with value: 0.1896957571037335 and parameters: {'observation_period_num': 38, 'train_rates': 0.7615181924211734, 'learning_rate': 0.0001729000949663964, 'batch_size': 227, 'step_size': 9, 'gamma': 0.7943226474091983}. Best is trial 6 with value: 0.1896957571037335.[0m
[32m[I 2025-02-07 13:18:57,456][0m Trial 7 finished with value: 0.15548760041502524 and parameters: {'observation_period_num': 22, 'train_rates': 0.6737112708819014, 'learning_rate': 5.6299461439476484e-05, 'batch_size': 94, 'step_size': 14, 'gamma': 0.7855879061390587}. Best is trial 7 with value: 0.15548760041502524.[0m
[32m[I 2025-02-07 13:20:03,422][0m Trial 8 finished with value: 0.28007832169532776 and parameters: {'observation_period_num': 96, 'train_rates': 0.9827194737518208, 'learning_rate': 1.963241769267612e-05, 'batch_size': 93, 'step_size': 6, 'gamma': 0.9249425462808387}. Best is trial 7 with value: 0.15548760041502524.[0m
[32m[I 2025-02-07 13:21:59,211][0m Trial 9 finished with value: 0.4915012117391958 and parameters: {'observation_period_num': 227, 'train_rates': 0.7744847918994895, 'learning_rate': 1.6300107570480062e-06, 'batch_size': 42, 'step_size': 5, 'gamma': 0.9209302239900199}. Best is trial 7 with value: 0.15548760041502524.[0m
[32m[I 2025-02-07 13:22:32,346][0m Trial 10 finished with value: 0.16813881578536272 and parameters: {'observation_period_num': 15, 'train_rates': 0.6088304017027165, 'learning_rate': 9.438564818985342e-05, 'batch_size': 150, 'step_size': 13, 'gamma': 0.8783836443676344}. Best is trial 7 with value: 0.15548760041502524.[0m
[32m[I 2025-02-07 13:23:05,359][0m Trial 11 finished with value: 0.15226980591197825 and parameters: {'observation_period_num': 8, 'train_rates': 0.6252353471664284, 'learning_rate': 0.00012679674436013466, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8791429223992338}. Best is trial 11 with value: 0.15226980591197825.[0m
[32m[I 2025-02-07 13:23:54,968][0m Trial 12 finished with value: 0.13105264934111696 and parameters: {'observation_period_num': 12, 'train_rates': 0.6749776653233993, 'learning_rate': 0.0005357935634621323, 'batch_size': 99, 'step_size': 12, 'gamma': 0.873943419253393}. Best is trial 12 with value: 0.13105264934111696.[0m
[32m[I 2025-02-07 13:24:15,437][0m Trial 13 finished with value: 0.3213317136405777 and parameters: {'observation_period_num': 132, 'train_rates': 0.6662018774211098, 'learning_rate': 0.0005698273950893401, 'batch_size': 252, 'step_size': 11, 'gamma': 0.8757393552032265}. Best is trial 12 with value: 0.13105264934111696.[0m
[32m[I 2025-02-07 13:24:45,377][0m Trial 14 finished with value: 0.3437240819084669 and parameters: {'observation_period_num': 118, 'train_rates': 0.6664132844691113, 'learning_rate': 0.0003222095951232391, 'batch_size': 170, 'step_size': 11, 'gamma': 0.904346524239906}. Best is trial 12 with value: 0.13105264934111696.[0m
[32m[I 2025-02-07 13:25:32,513][0m Trial 15 finished with value: 0.14270950018948994 and parameters: {'observation_period_num': 10, 'train_rates': 0.7086351095166443, 'learning_rate': 0.0002771104260608899, 'batch_size': 110, 'step_size': 8, 'gamma': 0.8363439737647047}. Best is trial 12 with value: 0.13105264934111696.[0m
[32m[I 2025-02-07 13:26:27,305][0m Trial 16 finished with value: 0.08511317433845816 and parameters: {'observation_period_num': 52, 'train_rates': 0.8657118101259608, 'learning_rate': 0.0008971874018454238, 'batch_size': 103, 'step_size': 7, 'gamma': 0.8292755560181359}. Best is trial 16 with value: 0.08511317433845816.[0m
[32m[I 2025-02-07 13:27:50,850][0m Trial 17 finished with value: 0.10830878560127837 and parameters: {'observation_period_num': 51, 'train_rates': 0.8493394003870407, 'learning_rate': 0.0005198523306247582, 'batch_size': 66, 'step_size': 8, 'gamma': 0.8257086364264427}. Best is trial 16 with value: 0.08511317433845816.[0m
[32m[I 2025-02-07 13:29:17,507][0m Trial 18 finished with value: 0.13245800487032197 and parameters: {'observation_period_num': 50, 'train_rates': 0.8520612850472932, 'learning_rate': 0.0009471373656888608, 'batch_size': 63, 'step_size': 7, 'gamma': 0.8170397123844215}. Best is trial 16 with value: 0.08511317433845816.[0m
[32m[I 2025-02-07 13:33:56,902][0m Trial 19 finished with value: 0.23798776157867269 and parameters: {'observation_period_num': 148, 'train_rates': 0.8631245528524055, 'learning_rate': 0.0003993505562518191, 'batch_size': 19, 'step_size': 2, 'gamma': 0.842856876329092}. Best is trial 16 with value: 0.08511317433845816.[0m
[32m[I 2025-02-07 13:35:15,097][0m Trial 20 finished with value: 0.7491906084210039 and parameters: {'observation_period_num': 81, 'train_rates': 0.8578439612820294, 'learning_rate': 1.3183057692971493e-06, 'batch_size': 70, 'step_size': 8, 'gamma': 0.7542653817401418}. Best is trial 16 with value: 0.08511317433845816.[0m
[32m[I 2025-02-07 13:36:17,144][0m Trial 21 finished with value: 0.06671759879993876 and parameters: {'observation_period_num': 40, 'train_rates': 0.828376023382619, 'learning_rate': 0.0009681741945126697, 'batch_size': 90, 'step_size': 9, 'gamma': 0.8150271351936389}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:37:39,513][0m Trial 22 finished with value: 0.0719055725187346 and parameters: {'observation_period_num': 43, 'train_rates': 0.8218204270260477, 'learning_rate': 0.00029993635379836625, 'batch_size': 67, 'step_size': 9, 'gamma': 0.8041422939538092}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:38:24,958][0m Trial 23 finished with value: 0.11975727897381114 and parameters: {'observation_period_num': 101, 'train_rates': 0.8155008011197605, 'learning_rate': 0.000931362300372735, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8021194178963811}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:39:34,074][0m Trial 24 finished with value: 0.06790341539985169 and parameters: {'observation_period_num': 71, 'train_rates': 0.901454949937189, 'learning_rate': 0.00020960247545483298, 'batch_size': 84, 'step_size': 7, 'gamma': 0.8092110730141541}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:41:48,066][0m Trial 25 finished with value: 0.1374018868533048 and parameters: {'observation_period_num': 82, 'train_rates': 0.9057941200295618, 'learning_rate': 0.0002283819873731573, 'batch_size': 42, 'step_size': 9, 'gamma': 0.772048903716341}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:42:58,963][0m Trial 26 finished with value: 0.12834154388369762 and parameters: {'observation_period_num': 36, 'train_rates': 0.9016819159920088, 'learning_rate': 6.408597509355807e-05, 'batch_size': 85, 'step_size': 3, 'gamma': 0.8060386126244976}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:47:34,086][0m Trial 27 finished with value: 0.10396483436564839 and parameters: {'observation_period_num': 69, 'train_rates': 0.8270276352835073, 'learning_rate': 0.00023172877956367203, 'batch_size': 19, 'step_size': 6, 'gamma': 0.8512029010037836}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:48:38,535][0m Trial 28 finished with value: 0.18180380842329805 and parameters: {'observation_period_num': 34, 'train_rates': 0.7449737970440681, 'learning_rate': 7.825955104186831e-05, 'batch_size': 80, 'step_size': 9, 'gamma': 0.774008583248713}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:49:28,563][0m Trial 29 finished with value: 0.2970027029514313 and parameters: {'observation_period_num': 103, 'train_rates': 0.9814192372677102, 'learning_rate': 3.726133011320293e-05, 'batch_size': 126, 'step_size': 6, 'gamma': 0.8189797358952099}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:50:17,320][0m Trial 30 finished with value: 0.1500018476106869 and parameters: {'observation_period_num': 146, 'train_rates': 0.8905639253572193, 'learning_rate': 0.0001573579905915384, 'batch_size': 120, 'step_size': 4, 'gamma': 0.8109299551840866}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:51:12,862][0m Trial 31 finished with value: 0.07693386867054199 and parameters: {'observation_period_num': 66, 'train_rates': 0.9484349175732828, 'learning_rate': 0.00040928833665922567, 'batch_size': 107, 'step_size': 7, 'gamma': 0.8328530802881308}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:53:08,582][0m Trial 32 finished with value: 0.11130568574775349 and parameters: {'observation_period_num': 64, 'train_rates': 0.9628242475795228, 'learning_rate': 0.00038520430587878687, 'batch_size': 53, 'step_size': 7, 'gamma': 0.8576450763952685}. Best is trial 21 with value: 0.06671759879993876.[0m
[32m[I 2025-02-07 13:54:05,528][0m Trial 33 finished with value: 0.05666207275383032 and parameters: {'observation_period_num': 32, 'train_rates': 0.9471550339209973, 'learning_rate': 0.0006056266699711347, 'batch_size': 110, 'step_size': 10, 'gamma': 0.8368762919841103}. Best is trial 33 with value: 0.05666207275383032.[0m
[32m[I 2025-02-07 13:55:18,647][0m Trial 34 finished with value: 0.07513747763420854 and parameters: {'observation_period_num': 34, 'train_rates': 0.9291767820992464, 'learning_rate': 0.0006651928782028983, 'batch_size': 83, 'step_size': 10, 'gamma': 0.7849751332717043}. Best is trial 33 with value: 0.05666207275383032.[0m
[32m[I 2025-02-07 13:56:54,768][0m Trial 35 finished with value: 0.13489306814530316 and parameters: {'observation_period_num': 27, 'train_rates': 0.8284138937540462, 'learning_rate': 0.0002323050728514585, 'batch_size': 56, 'step_size': 11, 'gamma': 0.9861926026958967}. Best is trial 33 with value: 0.05666207275383032.[0m
[32m[I 2025-02-07 13:58:05,537][0m Trial 36 finished with value: 0.4798920601606369 and parameters: {'observation_period_num': 45, 'train_rates': 0.8906528239740292, 'learning_rate': 2.87234936978907e-06, 'batch_size': 81, 'step_size': 9, 'gamma': 0.7982960473123286}. Best is trial 33 with value: 0.05666207275383032.[0m
[32m[I 2025-02-07 13:58:50,114][0m Trial 37 finished with value: 0.09150416191649322 and parameters: {'observation_period_num': 91, 'train_rates': 0.9291312087502159, 'learning_rate': 0.0005405017548245947, 'batch_size': 137, 'step_size': 10, 'gamma': 0.7694110978175539}. Best is trial 33 with value: 0.05666207275383032.[0m
[32m[I 2025-02-07 13:59:24,906][0m Trial 38 finished with value: 0.1442642457971052 and parameters: {'observation_period_num': 115, 'train_rates': 0.7938358828593501, 'learning_rate': 0.00011368996613286132, 'batch_size': 159, 'step_size': 8, 'gamma': 0.8456961183899486}. Best is trial 33 with value: 0.05666207275383032.[0m
[32m[I 2025-02-07 13:59:59,109][0m Trial 39 finished with value: 0.2679484188556671 and parameters: {'observation_period_num': 26, 'train_rates': 0.9512221325988127, 'learning_rate': 1.303552422739157e-05, 'batch_size': 190, 'step_size': 12, 'gamma': 0.7838747313323842}. Best is trial 33 with value: 0.05666207275383032.[0m
[32m[I 2025-02-07 14:02:52,441][0m Trial 40 finished with value: 0.1611474034267032 and parameters: {'observation_period_num': 60, 'train_rates': 0.8813259976652987, 'learning_rate': 0.00019888781435624498, 'batch_size': 32, 'step_size': 15, 'gamma': 0.8622924700344085}. Best is trial 33 with value: 0.05666207275383032.[0m
[32m[I 2025-02-07 14:03:59,018][0m Trial 41 finished with value: 0.07008124986744445 and parameters: {'observation_period_num': 38, 'train_rates': 0.922334044150594, 'learning_rate': 0.0007512457124968021, 'batch_size': 91, 'step_size': 10, 'gamma': 0.7869384822910442}. Best is trial 33 with value: 0.05666207275383032.[0m
[32m[I 2025-02-07 14:04:53,819][0m Trial 42 finished with value: 0.060032997913511375 and parameters: {'observation_period_num': 41, 'train_rates': 0.9146109907725283, 'learning_rate': 0.0006701527702873179, 'batch_size': 112, 'step_size': 9, 'gamma': 0.7975402803754715}. Best is trial 33 with value: 0.05666207275383032.[0m
[32m[I 2025-02-07 14:05:46,977][0m Trial 43 finished with value: 0.04560587986047009 and parameters: {'observation_period_num': 23, 'train_rates': 0.91654802254903, 'learning_rate': 0.0006782867445686055, 'batch_size': 117, 'step_size': 11, 'gamma': 0.7902026803168751}. Best is trial 43 with value: 0.04560587986047009.[0m
[32m[I 2025-02-07 14:06:42,606][0m Trial 44 finished with value: 0.04146072641015053 and parameters: {'observation_period_num': 20, 'train_rates': 0.9635340045734434, 'learning_rate': 0.0006780132067661052, 'batch_size': 113, 'step_size': 13, 'gamma': 0.8206132979317424}. Best is trial 44 with value: 0.04146072641015053.[0m
[32m[I 2025-02-07 14:07:37,737][0m Trial 45 finished with value: 0.04649582505226135 and parameters: {'observation_period_num': 21, 'train_rates': 0.967150538758226, 'learning_rate': 0.0006972908248474262, 'batch_size': 114, 'step_size': 13, 'gamma': 0.8226250290768807}. Best is trial 44 with value: 0.04146072641015053.[0m
[32m[I 2025-02-07 14:08:25,128][0m Trial 46 finished with value: 0.0388072207570076 and parameters: {'observation_period_num': 20, 'train_rates': 0.9643523046879802, 'learning_rate': 0.0006771083839611207, 'batch_size': 132, 'step_size': 13, 'gamma': 0.8240794972266416}. Best is trial 46 with value: 0.0388072207570076.[0m
[32m[I 2025-02-07 14:09:14,357][0m Trial 47 finished with value: 0.04017044976353645 and parameters: {'observation_period_num': 19, 'train_rates': 0.968732433472542, 'learning_rate': 0.00044638923664735666, 'batch_size': 130, 'step_size': 14, 'gamma': 0.8888788033174946}. Best is trial 46 with value: 0.0388072207570076.[0m
[32m[I 2025-02-07 14:09:57,576][0m Trial 48 finished with value: 0.07100178301334381 and parameters: {'observation_period_num': 19, 'train_rates': 0.9756578485128554, 'learning_rate': 0.0004191395985485562, 'batch_size': 148, 'step_size': 14, 'gamma': 0.8912592629904681}. Best is trial 46 with value: 0.0388072207570076.[0m
[32m[I 2025-02-07 14:10:44,656][0m Trial 49 finished with value: 0.039149004966020584 and parameters: {'observation_period_num': 6, 'train_rates': 0.9638488499364513, 'learning_rate': 0.00031810451628742594, 'batch_size': 135, 'step_size': 14, 'gamma': 0.9301227265606421}. Best is trial 46 with value: 0.0388072207570076.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.4416 | 0.2562
Epoch 2/300, Loss: 0.2097 | 0.2167
Epoch 3/300, Loss: 0.1957 | 0.2128
Epoch 4/300, Loss: 0.2640 | 0.2462
Epoch 5/300, Loss: 0.2426 | 0.3186
Epoch 6/300, Loss: 0.2769 | 0.7024
Epoch 7/300, Loss: 0.1997 | 0.3481
Epoch 8/300, Loss: 0.1297 | 0.1693
Epoch 9/300, Loss: 0.1233 | 0.1385
Epoch 10/300, Loss: 0.1269 | 0.1794
Epoch 11/300, Loss: 0.1523 | 0.3692
Epoch 12/300, Loss: 0.1271 | 0.1405
Epoch 13/300, Loss: 0.1088 | 0.1248
Epoch 14/300, Loss: 0.1155 | 0.0920
Epoch 15/300, Loss: 0.1270 | 0.0934
Epoch 16/300, Loss: 0.1370 | 0.1641
Epoch 17/300, Loss: 0.1211 | 0.1062
Epoch 18/300, Loss: 0.1162 | 0.0814
Epoch 19/300, Loss: 0.1330 | 0.1126
Epoch 20/300, Loss: 0.1430 | 0.2422
Epoch 21/300, Loss: 0.1131 | 0.1015
Epoch 22/300, Loss: 0.1010 | 0.0898
Epoch 23/300, Loss: 0.0917 | 0.0693
Epoch 24/300, Loss: 0.1015 | 0.0761
Epoch 25/300, Loss: 0.1073 | 0.1439
Epoch 26/300, Loss: 0.0922 | 0.0684
Epoch 27/300, Loss: 0.0942 | 0.0751
Epoch 28/300, Loss: 0.0895 | 0.0669
Epoch 29/300, Loss: 0.0855 | 0.0685
Epoch 30/300, Loss: 0.0817 | 0.0644
Epoch 31/300, Loss: 0.0825 | 0.0652
Epoch 32/300, Loss: 0.0830 | 0.0669
Epoch 33/300, Loss: 0.0801 | 0.0608
Epoch 34/300, Loss: 0.0786 | 0.0601
Epoch 35/300, Loss: 0.0772 | 0.0572
Epoch 36/300, Loss: 0.0756 | 0.0544
Epoch 37/300, Loss: 0.0741 | 0.0527
Epoch 38/300, Loss: 0.0734 | 0.0519
Epoch 39/300, Loss: 0.0731 | 0.0508
Epoch 40/300, Loss: 0.0731 | 0.0494
Epoch 41/300, Loss: 0.0742 | 0.0485
Epoch 42/300, Loss: 0.0759 | 0.0513
Epoch 43/300, Loss: 0.0775 | 0.0543
Epoch 44/300, Loss: 0.0764 | 0.0515
Epoch 45/300, Loss: 0.0733 | 0.0474
Epoch 46/300, Loss: 0.0732 | 0.0581
Epoch 47/300, Loss: 0.0755 | 0.0620
Epoch 48/300, Loss: 0.0740 | 0.0499
Epoch 49/300, Loss: 0.0710 | 0.0486
Epoch 50/300, Loss: 0.0683 | 0.0450
Epoch 51/300, Loss: 0.0671 | 0.0434
Epoch 52/300, Loss: 0.0662 | 0.0425
Epoch 53/300, Loss: 0.0658 | 0.0418
Epoch 54/300, Loss: 0.0654 | 0.0416
Epoch 55/300, Loss: 0.0653 | 0.0413
Epoch 56/300, Loss: 0.0653 | 0.0410
Epoch 57/300, Loss: 0.0655 | 0.0409
Epoch 58/300, Loss: 0.0661 | 0.0408
Epoch 59/300, Loss: 0.0670 | 0.0405
Epoch 60/300, Loss: 0.0680 | 0.0402
Epoch 61/300, Loss: 0.0695 | 0.0416
Epoch 62/300, Loss: 0.0697 | 0.0456
Epoch 63/300, Loss: 0.0682 | 0.0441
Epoch 64/300, Loss: 0.0669 | 0.0428
Epoch 65/300, Loss: 0.0644 | 0.0413
Epoch 66/300, Loss: 0.0642 | 0.0413
Epoch 67/300, Loss: 0.0640 | 0.0397
Epoch 68/300, Loss: 0.0632 | 0.0393
Epoch 69/300, Loss: 0.0626 | 0.0391
Epoch 70/300, Loss: 0.0623 | 0.0390
Epoch 71/300, Loss: 0.0621 | 0.0389
Epoch 72/300, Loss: 0.0619 | 0.0388
Epoch 73/300, Loss: 0.0616 | 0.0386
Epoch 74/300, Loss: 0.0613 | 0.0383
Epoch 75/300, Loss: 0.0612 | 0.0381
Epoch 76/300, Loss: 0.0610 | 0.0379
Epoch 77/300, Loss: 0.0609 | 0.0378
Epoch 78/300, Loss: 0.0607 | 0.0377
Epoch 79/300, Loss: 0.0605 | 0.0375
Epoch 80/300, Loss: 0.0604 | 0.0373
Epoch 81/300, Loss: 0.0602 | 0.0372
Epoch 82/300, Loss: 0.0601 | 0.0372
Epoch 83/300, Loss: 0.0600 | 0.0371
Epoch 84/300, Loss: 0.0598 | 0.0371
Epoch 85/300, Loss: 0.0597 | 0.0370
Epoch 86/300, Loss: 0.0595 | 0.0370
Epoch 87/300, Loss: 0.0594 | 0.0370
Epoch 88/300, Loss: 0.0593 | 0.0369
Epoch 89/300, Loss: 0.0592 | 0.0369
Epoch 90/300, Loss: 0.0592 | 0.0368
Epoch 91/300, Loss: 0.0591 | 0.0367
Epoch 92/300, Loss: 0.0590 | 0.0365
Epoch 93/300, Loss: 0.0588 | 0.0364
Epoch 94/300, Loss: 0.0587 | 0.0364
Epoch 95/300, Loss: 0.0587 | 0.0364
Epoch 96/300, Loss: 0.0586 | 0.0364
Epoch 97/300, Loss: 0.0585 | 0.0364
Epoch 98/300, Loss: 0.0584 | 0.0364
Epoch 99/300, Loss: 0.0583 | 0.0364
Epoch 100/300, Loss: 0.0582 | 0.0364
Epoch 101/300, Loss: 0.0582 | 0.0363
Epoch 102/300, Loss: 0.0581 | 0.0363
Epoch 103/300, Loss: 0.0580 | 0.0363
Epoch 104/300, Loss: 0.0579 | 0.0363
Epoch 105/300, Loss: 0.0579 | 0.0362
Epoch 106/300, Loss: 0.0578 | 0.0362
Epoch 107/300, Loss: 0.0577 | 0.0362
Epoch 108/300, Loss: 0.0577 | 0.0362
Epoch 109/300, Loss: 0.0576 | 0.0362
Epoch 110/300, Loss: 0.0575 | 0.0362
Epoch 111/300, Loss: 0.0575 | 0.0362
Epoch 112/300, Loss: 0.0574 | 0.0362
Epoch 113/300, Loss: 0.0573 | 0.0362
Epoch 114/300, Loss: 0.0573 | 0.0362
Epoch 115/300, Loss: 0.0572 | 0.0362
Epoch 116/300, Loss: 0.0572 | 0.0362
Epoch 117/300, Loss: 0.0571 | 0.0362
Epoch 118/300, Loss: 0.0570 | 0.0362
Epoch 119/300, Loss: 0.0570 | 0.0362
Epoch 120/300, Loss: 0.0569 | 0.0362
Epoch 121/300, Loss: 0.0569 | 0.0361
Epoch 122/300, Loss: 0.0568 | 0.0361
Epoch 123/300, Loss: 0.0568 | 0.0361
Epoch 124/300, Loss: 0.0567 | 0.0361
Epoch 125/300, Loss: 0.0567 | 0.0361
Epoch 126/300, Loss: 0.0566 | 0.0361
Epoch 127/300, Loss: 0.0566 | 0.0360
Epoch 128/300, Loss: 0.0565 | 0.0360
Epoch 129/300, Loss: 0.0565 | 0.0360
Epoch 130/300, Loss: 0.0564 | 0.0359
Epoch 131/300, Loss: 0.0564 | 0.0359
Epoch 132/300, Loss: 0.0563 | 0.0359
Epoch 133/300, Loss: 0.0563 | 0.0358
Epoch 134/300, Loss: 0.0562 | 0.0358
Epoch 135/300, Loss: 0.0562 | 0.0358
Epoch 136/300, Loss: 0.0562 | 0.0357
Epoch 137/300, Loss: 0.0561 | 0.0357
Epoch 138/300, Loss: 0.0561 | 0.0357
Epoch 139/300, Loss: 0.0560 | 0.0356
Epoch 140/300, Loss: 0.0560 | 0.0356
Epoch 141/300, Loss: 0.0560 | 0.0356
Epoch 142/300, Loss: 0.0560 | 0.0355
Epoch 143/300, Loss: 0.0559 | 0.0355
Epoch 144/300, Loss: 0.0559 | 0.0355
Epoch 145/300, Loss: 0.0559 | 0.0355
Epoch 146/300, Loss: 0.0558 | 0.0354
Epoch 147/300, Loss: 0.0558 | 0.0354
Epoch 148/300, Loss: 0.0558 | 0.0354
Epoch 149/300, Loss: 0.0557 | 0.0354
Epoch 150/300, Loss: 0.0557 | 0.0353
Epoch 151/300, Loss: 0.0557 | 0.0353
Epoch 152/300, Loss: 0.0557 | 0.0353
Epoch 153/300, Loss: 0.0556 | 0.0353
Epoch 154/300, Loss: 0.0556 | 0.0353
Epoch 155/300, Loss: 0.0556 | 0.0352
Epoch 156/300, Loss: 0.0556 | 0.0352
Epoch 157/300, Loss: 0.0555 | 0.0352
Epoch 158/300, Loss: 0.0555 | 0.0352
Epoch 159/300, Loss: 0.0555 | 0.0352
Epoch 160/300, Loss: 0.0555 | 0.0352
Epoch 161/300, Loss: 0.0555 | 0.0351
Epoch 162/300, Loss: 0.0554 | 0.0351
Epoch 163/300, Loss: 0.0554 | 0.0351
Epoch 164/300, Loss: 0.0554 | 0.0351
Epoch 165/300, Loss: 0.0554 | 0.0351
Epoch 166/300, Loss: 0.0554 | 0.0351
Epoch 167/300, Loss: 0.0554 | 0.0351
Epoch 168/300, Loss: 0.0553 | 0.0351
Epoch 169/300, Loss: 0.0553 | 0.0350
Epoch 170/300, Loss: 0.0553 | 0.0350
Epoch 171/300, Loss: 0.0553 | 0.0350
Epoch 172/300, Loss: 0.0553 | 0.0350
Epoch 173/300, Loss: 0.0553 | 0.0350
Epoch 174/300, Loss: 0.0553 | 0.0350
Epoch 175/300, Loss: 0.0552 | 0.0350
Epoch 176/300, Loss: 0.0552 | 0.0350
Epoch 177/300, Loss: 0.0552 | 0.0350
Epoch 178/300, Loss: 0.0552 | 0.0350
Epoch 179/300, Loss: 0.0552 | 0.0350
Epoch 180/300, Loss: 0.0552 | 0.0350
Epoch 181/300, Loss: 0.0552 | 0.0349
Epoch 182/300, Loss: 0.0552 | 0.0349
Epoch 183/300, Loss: 0.0551 | 0.0349
Epoch 184/300, Loss: 0.0551 | 0.0349
Epoch 185/300, Loss: 0.0551 | 0.0349
Epoch 186/300, Loss: 0.0551 | 0.0349
Epoch 187/300, Loss: 0.0551 | 0.0349
Epoch 188/300, Loss: 0.0551 | 0.0349
Epoch 189/300, Loss: 0.0551 | 0.0349
Epoch 190/300, Loss: 0.0551 | 0.0349
Epoch 191/300, Loss: 0.0551 | 0.0349
Epoch 192/300, Loss: 0.0551 | 0.0349
Epoch 193/300, Loss: 0.0550 | 0.0349
Epoch 194/300, Loss: 0.0550 | 0.0349
Epoch 195/300, Loss: 0.0550 | 0.0349
Epoch 196/300, Loss: 0.0550 | 0.0349
Epoch 197/300, Loss: 0.0550 | 0.0349
Epoch 198/300, Loss: 0.0550 | 0.0349
Epoch 199/300, Loss: 0.0550 | 0.0349
Epoch 200/300, Loss: 0.0550 | 0.0348
Epoch 201/300, Loss: 0.0550 | 0.0348
Epoch 202/300, Loss: 0.0550 | 0.0348
Epoch 203/300, Loss: 0.0550 | 0.0348
Epoch 204/300, Loss: 0.0550 | 0.0348
Epoch 205/300, Loss: 0.0550 | 0.0348
Epoch 206/300, Loss: 0.0550 | 0.0348
Epoch 207/300, Loss: 0.0550 | 0.0348
Epoch 208/300, Loss: 0.0549 | 0.0348
Epoch 209/300, Loss: 0.0549 | 0.0348
Epoch 210/300, Loss: 0.0549 | 0.0348
Epoch 211/300, Loss: 0.0549 | 0.0348
Epoch 212/300, Loss: 0.0549 | 0.0348
Epoch 213/300, Loss: 0.0549 | 0.0348
Epoch 214/300, Loss: 0.0549 | 0.0348
Epoch 215/300, Loss: 0.0549 | 0.0348
Epoch 216/300, Loss: 0.0549 | 0.0348
Epoch 217/300, Loss: 0.0549 | 0.0348
Epoch 218/300, Loss: 0.0549 | 0.0348
Epoch 219/300, Loss: 0.0549 | 0.0348
Epoch 220/300, Loss: 0.0549 | 0.0348
Epoch 221/300, Loss: 0.0549 | 0.0348
Epoch 222/300, Loss: 0.0549 | 0.0348
Epoch 223/300, Loss: 0.0549 | 0.0348
Epoch 224/300, Loss: 0.0549 | 0.0348
Epoch 225/300, Loss: 0.0549 | 0.0348
Epoch 226/300, Loss: 0.0549 | 0.0348
Epoch 227/300, Loss: 0.0549 | 0.0348
Epoch 228/300, Loss: 0.0549 | 0.0348
Epoch 229/300, Loss: 0.0549 | 0.0348
Epoch 230/300, Loss: 0.0549 | 0.0348
Epoch 231/300, Loss: 0.0549 | 0.0348
Epoch 232/300, Loss: 0.0548 | 0.0348
Epoch 233/300, Loss: 0.0548 | 0.0348
Epoch 234/300, Loss: 0.0548 | 0.0348
Epoch 235/300, Loss: 0.0548 | 0.0348
Epoch 236/300, Loss: 0.0548 | 0.0348
Epoch 237/300, Loss: 0.0548 | 0.0348
Epoch 238/300, Loss: 0.0548 | 0.0348
Epoch 239/300, Loss: 0.0548 | 0.0347
Epoch 240/300, Loss: 0.0548 | 0.0347
Epoch 241/300, Loss: 0.0548 | 0.0347
Epoch 242/300, Loss: 0.0548 | 0.0347
Epoch 243/300, Loss: 0.0548 | 0.0347
Epoch 244/300, Loss: 0.0548 | 0.0347
Epoch 245/300, Loss: 0.0548 | 0.0347
Epoch 246/300, Loss: 0.0548 | 0.0347
Epoch 247/300, Loss: 0.0548 | 0.0347
Epoch 248/300, Loss: 0.0548 | 0.0347
Epoch 249/300, Loss: 0.0548 | 0.0347
Epoch 250/300, Loss: 0.0548 | 0.0347
Epoch 251/300, Loss: 0.0548 | 0.0347
Epoch 252/300, Loss: 0.0548 | 0.0347
Epoch 253/300, Loss: 0.0548 | 0.0347
Epoch 254/300, Loss: 0.0548 | 0.0347
Epoch 255/300, Loss: 0.0548 | 0.0347
Epoch 256/300, Loss: 0.0548 | 0.0347
Epoch 257/300, Loss: 0.0548 | 0.0347
Epoch 258/300, Loss: 0.0548 | 0.0347
Epoch 259/300, Loss: 0.0548 | 0.0347
Epoch 260/300, Loss: 0.0548 | 0.0347
Epoch 261/300, Loss: 0.0548 | 0.0347
Epoch 262/300, Loss: 0.0548 | 0.0347
Epoch 263/300, Loss: 0.0548 | 0.0347
Epoch 264/300, Loss: 0.0548 | 0.0347
Epoch 265/300, Loss: 0.0548 | 0.0347
Epoch 266/300, Loss: 0.0548 | 0.0347
Epoch 267/300, Loss: 0.0548 | 0.0347
Epoch 268/300, Loss: 0.0548 | 0.0347
Epoch 269/300, Loss: 0.0548 | 0.0347
Epoch 270/300, Loss: 0.0548 | 0.0347
Epoch 271/300, Loss: 0.0548 | 0.0347
Epoch 272/300, Loss: 0.0548 | 0.0347
Epoch 273/300, Loss: 0.0548 | 0.0347
Epoch 274/300, Loss: 0.0548 | 0.0347
Epoch 275/300, Loss: 0.0548 | 0.0347
Epoch 276/300, Loss: 0.0548 | 0.0347
Epoch 277/300, Loss: 0.0548 | 0.0347
Epoch 278/300, Loss: 0.0548 | 0.0347
Epoch 279/300, Loss: 0.0548 | 0.0347
Epoch 280/300, Loss: 0.0548 | 0.0347
Epoch 281/300, Loss: 0.0548 | 0.0347
Epoch 282/300, Loss: 0.0548 | 0.0347
Epoch 283/300, Loss: 0.0548 | 0.0347
Epoch 284/300, Loss: 0.0548 | 0.0347
Epoch 285/300, Loss: 0.0548 | 0.0347
Epoch 286/300, Loss: 0.0548 | 0.0347
Epoch 287/300, Loss: 0.0548 | 0.0347
Epoch 288/300, Loss: 0.0548 | 0.0347
Epoch 289/300, Loss: 0.0548 | 0.0347
Epoch 290/300, Loss: 0.0548 | 0.0347
Epoch 291/300, Loss: 0.0548 | 0.0347
Epoch 292/300, Loss: 0.0548 | 0.0347
Epoch 293/300, Loss: 0.0548 | 0.0347
Epoch 294/300, Loss: 0.0548 | 0.0347
Epoch 295/300, Loss: 0.0548 | 0.0347
Epoch 296/300, Loss: 0.0548 | 0.0347
Epoch 297/300, Loss: 0.0548 | 0.0347
Epoch 298/300, Loss: 0.0548 | 0.0347
Epoch 299/300, Loss: 0.0548 | 0.0347
Epoch 300/300, Loss: 0.0548 | 0.0347
Runtime (seconds): 141.5038058757782
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 83.32662173197605
RMSE: 9.128341674804688
MAE: 9.128341674804688
R-squared: nan
[202.43834]
